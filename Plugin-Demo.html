<!DOCTYPE html>
<html lang="en-AU" prefix="og: http://ogp.me/ns#">
  <!-- head -->
  <head>
    <!-- meta -->
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1"
    />
    <meta name="description" content="Just another WordPress site" />
    <link
      rel="alternate"
      hreflang="en-au"
      href="https://empathiccomputing.org/team/mark-billinghurst/"
    />
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-87941464-1"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-87941464-1");
    </script>
    <link
      rel="shortcut icon"
      href="http://empathiccomputing.org/wp-content/uploads/2018/05/favicon.png"
    />

    <!-- wp_head() -->
    <title>Mark Billinghurst - Empathic Computing Lab</title>
    <!-- script | dynamic -->
    <script id="mfn-dnmc-config-js">
      //<![CDATA[
      window.mfn = {
        mobile_init: 1240,
        nicescroll: 40,
        parallax: "translate3d",
        responsive: 1,
        retina_js: 0,
      };
      window.mfn_lightbox = {
        disable: false,
        disableMobile: false,
        title: false,
      };
      window.mfn_sliders = {
        blog: 0,
        clients: 0,
        offer: 0,
        portfolio: 0,
        shop: 0,
        slider: 0,
        testimonials: 0,
      };
      //]]>
    </script>

    <!-- This site is optimized with the Yoast SEO plugin v7.4.2 - https://yoast.com/wordpress/plugins/seo/ -->
    <link
      rel="canonical"
      href="https://empathiccomputing.org/team/mark-billinghurst/"
    />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta
      property="og:title"
      content="Mark Billinghurst - Empathic Computing Lab"
    />
    <meta
      property="og:description"
      content="Prof. Mark Billinghurst has a wealth of knowledge and expertise in human-computer interface technology, particularly in the area of Augmented Reality (the overlay of three-dimensional images on the real world). In 2002, the former HIT Lab US Research Associate completed […]"
    />
    <meta
      property="og:url"
      content="https://empathiccomputing.org/team/mark-billinghurst/"
    />
    <meta property="og:site_name" content="Empathic Computing Lab" />
    <meta
      property="og:image"
      content="https://empathiccomputing.org/wp-content/uploads/2018/05/Mark1-570x5701.png"
    />
    <meta
      property="og:image:secure_url"
      content="https://empathiccomputing.org/wp-content/uploads/2018/05/Mark1-570x5701.png"
    />
    <meta property="og:image:width" content="570" />
    <meta property="og:image:height" content="570" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta
      name="twitter:description"
      content="Prof. Mark Billinghurst has a wealth of knowledge and expertise in human-computer interface technology, particularly in the area of Augmented Reality (the overlay of three-dimensional images on the real world). In 2002, the former HIT Lab US Research Associate completed […]"
    />
    <meta
      name="twitter:title"
      content="Mark Billinghurst - Empathic Computing Lab"
    />
    <meta
      name="twitter:image"
      content="https://empathiccomputing.org/wp-content/uploads/2018/05/Mark1-570x5701.png"
    />
    <!-- / Yoast SEO plugin. -->

    <link rel="dns-prefetch" href="//fonts.googleapis.com" />
    <link rel="dns-prefetch" href="//s.w.org" />
    <link
      rel="alternate"
      type="application/rss+xml"
      title="Empathic Computing Lab &raquo; Feed"
      href="https://empathiccomputing.org/feed/"
    />
    <link
      rel="alternate"
      type="application/rss+xml"
      title="Empathic Computing Lab &raquo; Comments Feed"
      href="https://empathiccomputing.org/comments/feed/"
    />
    <script type="text/javascript">
      window._wpemojiSettings = {
        baseUrl: "https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/",
        ext: ".png",
        svgUrl: "https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/",
        svgExt: ".svg",
        source: {
          concatemoji:
            "https:\/\/empathiccomputing.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.9.26",
        },
      };
      !(function (e, a, t) {
        var n,
          r,
          o,
          i = a.createElement("canvas"),
          p = i.getContext && i.getContext("2d");
        function s(e, t) {
          var a = String.fromCharCode;
          p.clearRect(0, 0, i.width, i.height),
            p.fillText(a.apply(this, e), 0, 0);
          e = i.toDataURL();
          return (
            p.clearRect(0, 0, i.width, i.height),
            p.fillText(a.apply(this, t), 0, 0),
            e === i.toDataURL()
          );
        }
        function c(e) {
          var t = a.createElement("script");
          (t.src = e),
            (t.defer = t.type = "text/javascript"),
            a.getElementsByTagName("head")[0].appendChild(t);
        }
        for (
          o = Array("flag", "emoji"),
            t.supports = { everything: !0, everythingExceptFlag: !0 },
            r = 0;
          r < o.length;
          r++
        )
          (t.supports[o[r]] = (function (e) {
            if (!p || !p.fillText) return !1;
            switch (
              ((p.textBaseline = "top"), (p.font = "600 32px Arial"), e)
            ) {
              case "flag":
                return s(
                  [55356, 56826, 55356, 56819],
                  [55356, 56826, 8203, 55356, 56819]
                )
                  ? !1
                  : !s(
                      [
                        55356, 57332, 56128, 56423, 56128, 56418, 56128, 56421,
                        56128, 56430, 56128, 56423, 56128, 56447,
                      ],
                      [
                        55356, 57332, 8203, 56128, 56423, 8203, 56128, 56418,
                        8203, 56128, 56421, 8203, 56128, 56430, 8203, 56128,
                        56423, 8203, 56128, 56447,
                      ]
                    );
              case "emoji":
                return !s(
                  [55358, 56760, 9792, 65039],
                  [55358, 56760, 8203, 9792, 65039]
                );
            }
            return !1;
          })(o[r])),
            (t.supports.everything = t.supports.everything && t.supports[o[r]]),
            "flag" !== o[r] &&
              (t.supports.everythingExceptFlag =
                t.supports.everythingExceptFlag && t.supports[o[r]]);
        (t.supports.everythingExceptFlag =
          t.supports.everythingExceptFlag && !t.supports.flag),
          (t.DOMReady = !1),
          (t.readyCallback = function () {
            t.DOMReady = !0;
          }),
          t.supports.everything ||
            ((n = function () {
              t.readyCallback();
            }),
            a.addEventListener
              ? (a.addEventListener("DOMContentLoaded", n, !1),
                e.addEventListener("load", n, !1))
              : (e.attachEvent("onload", n),
                a.attachEvent("onreadystatechange", function () {
                  "complete" === a.readyState && t.readyCallback();
                })),
            (n = t.source || {}).concatemoji
              ? c(n.concatemoji)
              : n.wpemoji && n.twemoji && (c(n.twemoji), c(n.wpemoji)));
      })(window, document, window._wpemojiSettings);
    </script>
    <style type="text/css">
      img.wp-smiley,
      img.emoji {
        display: inline !important;
        border: none !important;
        box-shadow: none !important;
        height: 1em !important;
        width: 1em !important;
        margin: 0 0.07em !important;
        vertical-align: -0.1em !important;
        background: none !important;
        padding: 0 !important;
      }
    </style>
    <link
      rel="stylesheet"
      id="contact-form-7-css"
      href="https://empathiccomputing.org/wp-content/plugins/contact-form-7/includes/css/styles.css?ver=5.0.1"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="rs-plugin-settings-css"
      href="https://empathiccomputing.org/wp-content/plugins/revslider/public/assets/css/settings.css?ver=5.4.7.3"
      type="text/css"
      media="all"
    />
    <style id="rs-plugin-settings-inline-css" type="text/css">
      #rs-demo-id {
      }
    </style>
    <link
      rel="stylesheet"
      id="style-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/style.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-base-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/css/base.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-layout-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/css/layout.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-shortcodes-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/css/shortcodes.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-animations-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/assets/animations/animations.min.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-jquery-ui-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/assets/ui/jquery.ui.all.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-jplayer-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/assets/jplayer/css/jplayer.blue.monday.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="mfn-responsive-css"
      href="https://empathiccomputing.org/wp-content/themes/empathic/css/responsive.css?ver=20.8.9.0"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="Raleway-css"
      href="https://fonts.googleapis.com/css?family=Raleway%3A1%2C100%2C300%2C400%2C500%2C700%2C900&#038;ver=4.9.26"
      type="text/css"
      media="all"
    />
    <link
      rel="stylesheet"
      id="Source+Sans+Pro-css"
      href="https://fonts.googleapis.com/css?family=Source+Sans+Pro%3A1%2C100%2C300%2C400%2C500%2C700%2C900&#038;ver=4.9.26"
      type="text/css"
      media="all"
    />
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/jquery.js?ver=1.12.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/plugins/revslider/public/assets/js/jquery.themepunch.tools.min.js?ver=5.4.7.3"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/plugins/revslider/public/assets/js/jquery.themepunch.revolution.min.js?ver=5.4.7.3"
    ></script>
    <link
      rel="https://api.w.org/"
      href="https://empathiccomputing.org/wp-json/"
    />
    <link
      rel="EditURI"
      type="application/rsd+xml"
      title="RSD"
      href="https://empathiccomputing.org/xmlrpc.php?rsd"
    />
    <link
      rel="wlwmanifest"
      type="application/wlwmanifest+xml"
      href="https://empathiccomputing.org/wp-includes/wlwmanifest.xml"
    />
    <meta name="generator" content="WordPress 4.9.26" />
    <link rel="shortlink" href="https://empathiccomputing.org/?p=125" />
    <link
      rel="alternate"
      type="application/json+oembed"
      href="https://empathiccomputing.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fempathiccomputing.org%2Fteam%2Fmark-billinghurst%2F"
    />
    <link
      rel="alternate"
      type="text/xml+oembed"
      href="https://empathiccomputing.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fempathiccomputing.org%2Fteam%2Fmark-billinghurst%2F&#038;format=xml"
    />
    <!-- style | dynamic -->
    <style id="mfn-dnmc-style-css">
      @media only screen and (min-width: 1240px) {
        body:not(.header-simple) #Top_bar #menu {
          display: block !important;
        }
        .tr-menu #Top_bar #menu {
          background: none !important;
        }
        #Top_bar .menu > li > ul.mfn-megamenu {
          width: 984px;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li {
          float: left;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li.mfn-megamenu-cols-1 {
          width: 100%;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li.mfn-megamenu-cols-2 {
          width: 50%;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li.mfn-megamenu-cols-3 {
          width: 33.33%;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li.mfn-megamenu-cols-4 {
          width: 25%;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li.mfn-megamenu-cols-5 {
          width: 20%;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li.mfn-megamenu-cols-6 {
          width: 16.66%;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li > ul {
          display: block !important;
          position: inherit;
          left: auto;
          top: auto;
          border-width: 0 1px 0 0;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li:last-child > ul {
          border: 0;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li > ul li {
          width: auto;
        }
        #Top_bar .menu > li > ul.mfn-megamenu a.mfn-megamenu-title {
          text-transform: uppercase;
          font-weight: 400;
          background: none;
        }
        #Top_bar .menu > li > ul.mfn-megamenu a .menu-arrow {
          display: none;
        }
        .menuo-right #Top_bar .menu > li > ul.mfn-megamenu {
          left: auto;
          right: 0;
        }
        .menuo-right #Top_bar .menu > li > ul.mfn-megamenu-bg {
          box-sizing: border-box;
        }
        #Top_bar .menu > li > ul.mfn-megamenu-bg {
          padding: 20px 166px 20px 20px;
          background-repeat: no-repeat;
          background-position: right bottom;
        }
        .rtl #Top_bar .menu > li > ul.mfn-megamenu-bg {
          padding-left: 166px;
          padding-right: 20px;
          background-position: left bottom;
        }
        #Top_bar .menu > li > ul.mfn-megamenu-bg > li {
          background: none;
        }
        #Top_bar .menu > li > ul.mfn-megamenu-bg > li a {
          border: none;
        }
        #Top_bar .menu > li > ul.mfn-megamenu-bg > li > ul {
          background: none !important;
          -webkit-box-shadow: 0 0 0 0;
          -moz-box-shadow: 0 0 0 0;
          box-shadow: 0 0 0 0;
        }
        .mm-vertical #Top_bar .container {
          position: relative;
        }
        .mm-vertical #Top_bar .top_bar_left {
          position: static;
        }
        .mm-vertical #Top_bar .menu > li ul {
          box-shadow: 0 0 0 0 transparent !important;
          background-image: none;
        }
        .mm-vertical #Top_bar .menu > li > ul.mfn-megamenu {
          width: 98% !important;
          margin: 0 1%;
          padding: 20px 0;
        }
        .mm-vertical.header-plain #Top_bar .menu > li > ul.mfn-megamenu {
          width: 100% !important;
          margin: 0;
        }
        .mm-vertical #Top_bar .menu > li > ul.mfn-megamenu > li {
          display: table-cell;
          float: none !important;
          width: 10%;
          padding: 0 15px;
          border-right: 1px solid rgba(0, 0, 0, 0.05);
        }
        .mm-vertical #Top_bar .menu > li > ul.mfn-megamenu > li:last-child {
          border-right-width: 0;
        }
        .mm-vertical #Top_bar .menu > li > ul.mfn-megamenu > li.hide-border {
          border-right-width: 0;
        }
        .mm-vertical #Top_bar .menu > li > ul.mfn-megamenu > li a {
          border-bottom-width: 0;
          padding: 9px 15px;
          line-height: 120%;
        }
        .mm-vertical
          #Top_bar
          .menu
          > li
          > ul.mfn-megamenu
          a.mfn-megamenu-title {
          font-weight: 700;
        }
        .rtl
          .mm-vertical
          #Top_bar
          .menu
          > li
          > ul.mfn-megamenu
          > li:first-child {
          border-right-width: 0;
        }
        .rtl
          .mm-vertical
          #Top_bar
          .menu
          > li
          > ul.mfn-megamenu
          > li:last-child {
          border-right-width: 1px;
        }
        #Header_creative #Top_bar .menu > li > ul.mfn-megamenu {
          width: 980px !important;
          margin: 0;
        }
        .header-plain:not(.menuo-right) #Header .top_bar_left {
          width: auto !important;
        }
        .header-stack.header-center #Top_bar #menu {
          display: inline-block !important;
        }
        .header-simple #Top_bar #menu {
          display: none;
          height: auto;
          width: 300px;
          bottom: auto;
          top: 100%;
          right: 1px;
          position: absolute;
          margin: 0;
        }
        .header-simple #Header a.responsive-menu-toggle {
          display: block;
          right: 10px;
        }
        .header-simple #Top_bar #menu > ul {
          width: 100%;
          float: left;
        }
        .header-simple #Top_bar #menu ul li {
          width: 100%;
          padding-bottom: 0;
          border-right: 0;
          position: relative;
        }
        .header-simple #Top_bar #menu ul li a {
          padding: 0 20px;
          margin: 0;
          display: block;
          height: auto;
          line-height: normal;
          border: none;
        }
        .header-simple #Top_bar #menu ul li a:after {
          display: none;
        }
        .header-simple #Top_bar #menu ul li a span {
          border: none;
          line-height: 44px;
          display: inline;
          padding: 0;
        }
        .header-simple #Top_bar #menu ul li.submenu .menu-toggle {
          display: block;
          position: absolute;
          right: 0;
          top: 0;
          width: 44px;
          height: 44px;
          line-height: 44px;
          font-size: 30px;
          font-weight: 300;
          text-align: center;
          cursor: pointer;
          color: #444;
          opacity: 0.33;
        }
        .header-simple #Top_bar #menu ul li.submenu .menu-toggle:after {
          content: "+";
        }
        .header-simple #Top_bar #menu ul li.hover > .menu-toggle:after {
          content: "-";
        }
        .header-simple #Top_bar #menu ul li.hover a {
          border-bottom: 0;
        }
        .header-simple #Top_bar #menu ul.mfn-megamenu li .menu-toggle {
          display: none;
        }
        .header-simple #Top_bar #menu ul li ul {
          position: relative !important;
          left: 0 !important;
          top: 0;
          padding: 0;
          margin: 0 !important;
          width: auto !important;
          background-image: none;
        }
        .header-simple #Top_bar #menu ul li ul li {
          width: 100% !important;
          display: block;
          padding: 0;
        }
        .header-simple #Top_bar #menu ul li ul li a {
          padding: 0 20px 0 30px;
        }
        .header-simple #Top_bar #menu ul li ul li a .menu-arrow {
          display: none;
        }
        .header-simple #Top_bar #menu ul li ul li a span {
          padding: 0;
        }
        .header-simple #Top_bar #menu ul li ul li a span:after {
          display: none !important;
        }
        .header-simple
          #Top_bar
          .menu
          > li
          > ul.mfn-megamenu
          a.mfn-megamenu-title {
          text-transform: uppercase;
          font-weight: 400;
        }
        .header-simple #Top_bar .menu > li > ul.mfn-megamenu > li > ul {
          display: block !important;
          position: inherit;
          left: auto;
          top: auto;
        }
        .header-simple #Top_bar #menu ul li ul li ul {
          border-left: 0 !important;
          padding: 0;
          top: 0;
        }
        .header-simple #Top_bar #menu ul li ul li ul li a {
          padding: 0 20px 0 40px;
        }
        .rtl.header-simple #Top_bar #menu {
          left: 1px;
          right: auto;
        }
        .rtl.header-simple #Top_bar a.responsive-menu-toggle {
          left: 10px;
          right: auto;
        }
        .rtl.header-simple #Top_bar #menu ul li.submenu .menu-toggle {
          left: 0;
          right: auto;
        }
        .rtl.header-simple #Top_bar #menu ul li ul {
          left: auto !important;
          right: 0 !important;
        }
        .rtl.header-simple #Top_bar #menu ul li ul li a {
          padding: 0 30px 0 20px;
        }
        .rtl.header-simple #Top_bar #menu ul li ul li ul li a {
          padding: 0 40px 0 20px;
        }
        .menu-highlight #Top_bar .menu > li {
          margin: 0 2px;
        }
        .menu-highlight:not(.header-creative) #Top_bar .menu > li > a {
          margin: 20px 0;
          padding: 0;
          -webkit-border-radius: 5px;
          border-radius: 5px;
        }
        .menu-highlight #Top_bar .menu > li > a:after {
          display: none;
        }
        .menu-highlight #Top_bar .menu > li > a span:not(.description) {
          line-height: 50px;
        }
        .menu-highlight #Top_bar .menu > li > a span.description {
          display: none;
        }
        .menu-highlight.header-stack #Top_bar .menu > li > a {
          margin: 10px 0 !important;
        }
        .menu-highlight.header-stack
          #Top_bar
          .menu
          > li
          > a
          span:not(.description) {
          line-height: 40px;
        }
        .menu-highlight.header-transparent #Top_bar .menu > li > a {
          margin: 5px 0;
        }
        .menu-highlight.header-simple #Top_bar #menu ul li,
        .menu-highlight.header-creative #Top_bar #menu ul li {
          margin: 0;
        }
        .menu-highlight.header-simple #Top_bar #menu ul li > a,
        .menu-highlight.header-creative #Top_bar #menu ul li > a {
          -webkit-border-radius: 0;
          border-radius: 0;
        }
        .menu-highlight:not(.header-fixed):not(.header-simple)
          #Top_bar.is-sticky
          .menu
          > li
          > a {
          margin: 10px 0 !important;
          padding: 5px 0 !important;
        }
        .menu-highlight:not(.header-fixed):not(.header-simple)
          #Top_bar.is-sticky
          .menu
          > li
          > a
          span {
          line-height: 30px !important;
        }
        .header-modern.menu-highlight.menuo-right .menu_wrapper {
          margin-right: 20px;
        }
        .menu-line-below #Top_bar .menu > li > a:after {
          top: auto;
          bottom: -4px;
        }
        .menu-line-below #Top_bar.is-sticky .menu > li > a:after {
          top: auto;
          bottom: -4px;
        }
        .menu-line-below-80 #Top_bar:not(.is-sticky) .menu > li > a:after {
          height: 4px;
          left: 10%;
          top: 50%;
          margin-top: 20px;
          width: 80%;
        }
        .menu-line-below-80-1 #Top_bar:not(.is-sticky) .menu > li > a:after {
          height: 1px;
          left: 10%;
          top: 50%;
          margin-top: 20px;
          width: 80%;
        }
        .menu-link-color #Top_bar .menu > li > a:after {
          display: none !important;
        }
        .menu-arrow-top #Top_bar .menu > li > a:after {
          background: none repeat scroll 0 0 rgba(0, 0, 0, 0) !important;
          border-color: #ccc transparent transparent;
          border-style: solid;
          border-width: 7px 7px 0;
          display: block;
          height: 0;
          left: 50%;
          margin-left: -7px;
          top: 0 !important;
          width: 0;
        }
        .menu-arrow-top.header-transparent #Top_bar .menu > li > a:after,
        .menu-arrow-top.header-plain #Top_bar .menu > li > a:after {
          display: none;
        }
        .menu-arrow-top #Top_bar.is-sticky .menu > li > a:after {
          top: 0 !important;
        }
        .menu-arrow-bottom #Top_bar .menu > li > a:after {
          background: none !important;
          border-color: transparent transparent #ccc;
          border-style: solid;
          border-width: 0 7px 7px;
          display: block;
          height: 0;
          left: 50%;
          margin-left: -7px;
          top: auto;
          bottom: 0;
          width: 0;
        }
        .menu-arrow-bottom.header-transparent #Top_bar .menu > li > a:after,
        .menu-arrow-bottom.header-plain #Top_bar .menu > li > a:after {
          display: none;
        }
        .menu-arrow-bottom #Top_bar.is-sticky .menu > li > a:after {
          top: auto;
          bottom: 0;
        }
        .menuo-no-borders #Top_bar .menu > li > a span:not(.description) {
          border-right-width: 0;
        }
        .menuo-no-borders #Header_creative #Top_bar .menu > li > a span {
          border-bottom-width: 0;
        }
        .menuo-right #Top_bar .menu_wrapper {
          float: right;
        }
        .menuo-right.header-stack:not(.header-center) #Top_bar .menu_wrapper {
          margin-right: 150px;
        }
        body.header-creative {
          padding-left: 50px;
        }
        body.header-creative.header-open {
          padding-left: 250px;
        }
        body.error404,
        body.under-construction,
        body.template-blank {
          padding-left: 0 !important;
        }
        .header-creative.footer-fixed #Footer,
        .header-creative.footer-sliding #Footer,
        .header-creative.footer-stick #Footer.is-sticky {
          box-sizing: border-box;
          padding-left: 50px;
        }
        .header-open.footer-fixed #Footer,
        .header-open.footer-sliding #Footer,
        .header-creative.footer-stick #Footer.is-sticky {
          padding-left: 250px;
        }
        .header-rtl.header-creative.footer-fixed #Footer,
        .header-rtl.header-creative.footer-sliding #Footer,
        .header-rtl.header-creative.footer-stick #Footer.is-sticky {
          padding-left: 0;
          padding-right: 50px;
        }
        .header-rtl.header-open.footer-fixed #Footer,
        .header-rtl.header-open.footer-sliding #Footer,
        .header-rtl.header-creative.footer-stick #Footer.is-sticky {
          padding-right: 250px;
        }
        #Header_creative {
          background: #fff;
          position: fixed;
          width: 250px;
          height: 100%;
          left: -200px;
          top: 0;
          z-index: 9002;
          -webkit-box-shadow: 2px 0 4px 2px rgba(0, 0, 0, 0.15);
          box-shadow: 2px 0 4px 2px rgba(0, 0, 0, 0.15);
        }
        #Header_creative .container {
          width: 100%;
        }
        #Header_creative .creative-wrapper {
          opacity: 0;
          margin-right: 50px;
        }
        #Header_creative a.creative-menu-toggle {
          display: block;
          width: 34px;
          height: 34px;
          line-height: 34px;
          font-size: 22px;
          text-align: center;
          position: absolute;
          top: 10px;
          right: 8px;
          border-radius: 3px;
        }
        .admin-bar #Header_creative a.creative-menu-toggle {
          top: 42px;
        }
        #Header_creative #Top_bar {
          position: static;
          width: 100%;
        }
        #Header_creative #Top_bar .top_bar_left {
          width: 100% !important;
          float: none;
        }
        #Header_creative #Top_bar .top_bar_right {
          width: 100% !important;
          float: none;
          height: auto;
          margin-bottom: 35px;
          text-align: center;
          padding: 0 20px;
          top: 0;
          -webkit-box-sizing: border-box;
          -moz-box-sizing: border-box;
          box-sizing: border-box;
        }
        #Header_creative #Top_bar .top_bar_right:before {
          display: none;
        }
        #Header_creative #Top_bar .top_bar_right_wrapper {
          top: 0;
        }
        #Header_creative #Top_bar .logo {
          float: none;
          text-align: center;
          margin: 15px 0;
        }
        #Header_creative #Top_bar .menu_wrapper {
          float: none;
          margin: 0 0 30px;
        }
        #Header_creative #Top_bar .menu > li {
          width: 100%;
          float: none;
          position: relative;
        }
        #Header_creative #Top_bar .menu > li > a {
          padding: 0;
          text-align: center;
        }
        #Header_creative #Top_bar .menu > li > a:after {
          display: none;
        }
        #Header_creative #Top_bar .menu > li > a span {
          border-right: 0;
          border-bottom-width: 1px;
          line-height: 38px;
        }
        #Header_creative #Top_bar .menu li ul {
          left: 100%;
          right: auto;
          top: 0;
          box-shadow: 2px 2px 2px 0 rgba(0, 0, 0, 0.03);
          -webkit-box-shadow: 2px 2px 2px 0 rgba(0, 0, 0, 0.03);
        }
        #Header_creative #Top_bar .menu > li > ul.mfn-megamenu {
          width: 700px !important;
        }
        #Header_creative #Top_bar .menu > li > ul.mfn-megamenu > li > ul {
          left: 0;
        }
        #Header_creative #Top_bar .menu li ul li a {
          padding-top: 9px;
          padding-bottom: 8px;
        }
        #Header_creative #Top_bar .menu li ul li ul {
          top: 0;
        }
        #Header_creative #Top_bar .menu > li > a span.description {
          display: block;
          font-size: 13px;
          line-height: 28px !important;
          clear: both;
        }
        #Header_creative #Top_bar .search_wrapper {
          left: 100%;
          top: auto;
          bottom: 0;
        }
        #Header_creative #Top_bar a#header_cart {
          display: inline-block;
          float: none;
          top: 3px;
        }
        #Header_creative #Top_bar a#search_button {
          display: inline-block;
          float: none;
          top: 3px;
        }
        #Header_creative #Top_bar .wpml-languages {
          display: inline-block;
          float: none;
          top: 0;
        }
        #Header_creative #Top_bar .wpml-languages.enabled:hover a.active {
          padding-bottom: 9px;
        }
        #Header_creative #Top_bar a.button.action_button {
          display: inline-block;
          float: none;
          top: 16px;
          margin: 0;
        }
        #Header_creative #Top_bar .banner_wrapper {
          display: block;
          text-align: center;
        }
        #Header_creative #Top_bar .banner_wrapper img {
          max-width: 100%;
          height: auto;
          display: inline-block;
        }
        #Header_creative #Action_bar {
          display: none;
          position: absolute;
          bottom: 0;
          top: auto;
          clear: both;
          padding: 0 20px;
          box-sizing: border-box;
        }
        #Header_creative #Action_bar .social {
          float: none;
          text-align: center;
          padding: 5px 0 15px;
        }
        #Header_creative #Action_bar .social li {
          margin-bottom: 2px;
        }
        #Header_creative .social li a {
          color: rgba(0, 0, 0, 0.5);
        }
        #Header_creative .social li a:hover {
          color: #000;
        }
        #Header_creative .creative-social {
          position: absolute;
          bottom: 10px;
          right: 0;
          width: 50px;
        }
        #Header_creative .creative-social li {
          display: block;
          float: none;
          width: 100%;
          text-align: center;
          margin-bottom: 5px;
        }
        .header-creative .fixed-nav.fixed-nav-prev {
          margin-left: 50px;
        }
        .header-creative.header-open .fixed-nav.fixed-nav-prev {
          margin-left: 250px;
        }
        .menuo-last #Header_creative #Top_bar .menu li.last ul {
          top: auto;
          bottom: 0;
        }
        .header-open #Header_creative {
          left: 0;
        }
        .header-open #Header_creative .creative-wrapper {
          opacity: 1;
          margin: 0 !important;
        }
        .header-open #Header_creative .creative-menu-toggle,
        .header-open #Header_creative .creative-social {
          display: none;
        }
        .header-open #Header_creative #Action_bar {
          display: block;
        }
        body.header-rtl.header-creative {
          padding-left: 0;
          padding-right: 50px;
        }
        .header-rtl #Header_creative {
          left: auto;
          right: -200px;
        }
        .header-rtl.nice-scroll #Header_creative {
          margin-right: 10px;
        }
        .header-rtl #Header_creative .creative-wrapper {
          margin-left: 50px;
          margin-right: 0;
        }
        .header-rtl #Header_creative a.creative-menu-toggle {
          left: 8px;
          right: auto;
        }
        .header-rtl #Header_creative .creative-social {
          left: 0;
          right: auto;
        }
        .header-rtl #Footer #back_to_top.sticky {
          right: 125px;
        }
        .header-rtl #popup_contact {
          right: 70px;
        }
        .header-rtl #Header_creative #Top_bar .menu li ul {
          left: auto;
          right: 100%;
        }
        .header-rtl #Header_creative #Top_bar .search_wrapper {
          left: auto;
          right: 100%;
        }
        .header-rtl .fixed-nav.fixed-nav-prev {
          margin-left: 0 !important;
        }
        .header-rtl .fixed-nav.fixed-nav-next {
          margin-right: 50px;
        }
        body.header-rtl.header-creative.header-open {
          padding-left: 0;
          padding-right: 250px !important;
        }
        .header-rtl.header-open #Header_creative {
          left: auto;
          right: 0;
        }
        .header-rtl.header-open #Footer #back_to_top.sticky {
          right: 325px;
        }
        .header-rtl.header-open #popup_contact {
          right: 270px;
        }
        .header-rtl.header-open .fixed-nav.fixed-nav-next {
          margin-right: 250px;
        }
        #Header_creative.active {
          left: -1px;
        }
        .header-rtl #Header_creative.active {
          left: auto;
          right: -1px;
        }
        #Header_creative.active .creative-wrapper {
          opacity: 1;
          margin: 0;
        }
        .header-creative .vc_row[data-vc-full-width] {
          padding-left: 50px;
        }
        .header-creative.header-open .vc_row[data-vc-full-width] {
          padding-left: 250px;
        }
        .header-open .vc_parallax .vc_parallax-inner {
          left: auto;
          width: calc(100% - 250px);
        }
        .header-open.header-rtl .vc_parallax .vc_parallax-inner {
          left: 0;
          right: auto;
        }
        #Header_creative.scroll {
          height: 100%;
          overflow-y: auto;
        }
        #Header_creative.scroll:not(.dropdown) .menu li ul {
          display: none !important;
        }
        #Header_creative.scroll #Action_bar {
          position: static;
        }
        #Header_creative.dropdown {
          outline: none;
        }
        #Header_creative.dropdown #Top_bar .menu_wrapper {
          float: left;
        }
        #Header_creative.dropdown #Top_bar #menu ul li {
          position: relative;
          float: left;
        }
        #Header_creative.dropdown #Top_bar #menu ul li a:after {
          display: none;
        }
        #Header_creative.dropdown #Top_bar #menu ul li a span {
          line-height: 38px;
          padding: 0;
        }
        #Header_creative.dropdown #Top_bar #menu ul li.submenu .menu-toggle {
          display: block;
          position: absolute;
          right: 0;
          top: 0;
          width: 38px;
          height: 38px;
          line-height: 38px;
          font-size: 26px;
          font-weight: 300;
          text-align: center;
          cursor: pointer;
          color: #444;
          opacity: 0.33;
        }
        #Header_creative.dropdown
          #Top_bar
          #menu
          ul
          li.submenu
          .menu-toggle:after {
          content: "+";
        }
        #Header_creative.dropdown
          #Top_bar
          #menu
          ul
          li.hover
          > .menu-toggle:after {
          content: "-";
        }
        #Header_creative.dropdown #Top_bar #menu ul li.hover a {
          border-bottom: 0;
        }
        #Header_creative.dropdown
          #Top_bar
          #menu
          ul.mfn-megamenu
          li
          .menu-toggle {
          display: none;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul {
          position: relative !important;
          left: 0 !important;
          top: 0;
          padding: 0;
          margin-left: 0 !important;
          width: auto !important;
          background-image: none;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul li {
          width: 100% !important;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul li a {
          padding: 0 10px;
          text-align: center;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul li a .menu-arrow {
          display: none;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul li a span {
          padding: 0;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul li a span:after {
          display: none !important;
        }
        #Header_creative.dropdown
          #Top_bar
          .menu
          > li
          > ul.mfn-megamenu
          a.mfn-megamenu-title {
          text-transform: uppercase;
          font-weight: 400;
        }
        #Header_creative.dropdown
          #Top_bar
          .menu
          > li
          > ul.mfn-megamenu
          > li
          > ul {
          display: block !important;
          position: inherit;
          left: auto;
          top: auto;
        }
        #Header_creative.dropdown #Top_bar #menu ul li ul li ul {
          border-left: 0 !important;
          padding: 0;
          top: 0;
        }
        #Header_creative {
          transition: left 0.5s ease-in-out, right 0.5s ease-in-out;
        }
        #Header_creative .creative-wrapper {
          transition: opacity 0.5s ease-in-out, margin 0s ease-in-out 0.5s;
        }
        #Header_creative.active .creative-wrapper {
          transition: opacity 0.5s ease-in-out, margin 0s ease-in-out;
        }
      }
      @media only screen and (min-width: 1240px) {
        #Top_bar.is-sticky {
          position: fixed !important;
          width: 100%;
          left: 0;
          top: -60px;
          height: 60px;
          z-index: 701;
          background: #fff;
          opacity: 0.97;
          filter: alpha(opacity = 97);
          -webkit-box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.1);
          -moz-box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.1);
          box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.1);
        }
        .layout-boxed.header-boxed #Top_bar.is-sticky {
          max-width: 1240px;
          left: 50%;
          -webkit-transform: translateX(-50%);
          transform: translateX(-50%);
        }
        .layout-boxed.header-boxed.nice-scroll #Top_bar.is-sticky {
          margin-left: -5px;
        }
        #Top_bar.is-sticky .top_bar_left,
        #Top_bar.is-sticky .top_bar_right,
        #Top_bar.is-sticky .top_bar_right:before {
          background: none;
        }
        #Top_bar.is-sticky .top_bar_right {
          top: -4px;
          height: auto;
        }
        #Top_bar.is-sticky .top_bar_right_wrapper {
          top: 15px;
        }
        .header-plain #Top_bar.is-sticky .top_bar_right_wrapper {
          top: 0;
        }
        #Top_bar.is-sticky .logo {
          width: auto;
          margin: 0 30px 0 20px;
          padding: 0;
        }
        #Top_bar.is-sticky #logo {
          padding: 5px 0 !important;
          height: 50px !important;
          line-height: 50px !important;
        }
        .logo-no-sticky-padding #Top_bar.is-sticky #logo {
          height: 60px !important;
          line-height: 60px !important;
        }
        #Top_bar.is-sticky #logo img.logo-main {
          display: none;
        }
        #Top_bar.is-sticky #logo img.logo-sticky {
          display: inline;
          max-height: 35px;
        }
        #Top_bar.is-sticky .menu_wrapper {
          clear: none;
        }
        #Top_bar.is-sticky .menu_wrapper .menu > li > a {
          padding: 15px 0;
        }
        #Top_bar.is-sticky .menu > li > a,
        #Top_bar.is-sticky .menu > li > a span {
          line-height: 30px;
        }
        #Top_bar.is-sticky .menu > li > a:after {
          top: auto;
          bottom: -4px;
        }
        #Top_bar.is-sticky .menu > li > a span.description {
          display: none;
        }
        #Top_bar.is-sticky .secondary_menu_wrapper,
        #Top_bar.is-sticky .banner_wrapper {
          display: none;
        }
        .header-overlay #Top_bar.is-sticky {
          display: none;
        }
        .sticky-dark #Top_bar.is-sticky {
          background: rgba(0, 0, 0, 0.8);
        }
        .sticky-dark #Top_bar.is-sticky #menu {
          background: rgba(0, 0, 0, 0.8);
        }
        .sticky-dark #Top_bar.is-sticky .menu > li > a {
          color: #fff;
        }
        .sticky-dark #Top_bar.is-sticky .top_bar_right a {
          color: rgba(255, 255, 255, 0.5);
        }
        .sticky-dark #Top_bar.is-sticky .wpml-languages a.active,
        .sticky-dark #Top_bar.is-sticky .wpml-languages ul.wpml-lang-dropdown {
          background: rgba(0, 0, 0, 0.3);
          border-color: rgba(0, 0, 0, 0.1);
        }
      }
      @media only screen and (min-width: 768px) and (max-width: 1240px) {
        .header_placeholder {
          height: 0 !important;
        }
      }
      @media only screen and (max-width: 1239px) {
        #Top_bar #menu {
          display: none;
          height: auto;
          width: 300px;
          bottom: auto;
          top: 100%;
          right: 1px;
          position: absolute;
          margin: 0;
        }
        #Top_bar a.responsive-menu-toggle {
          display: block;
        }
        #Top_bar #menu > ul {
          width: 100%;
          float: left;
        }
        #Top_bar #menu ul li {
          width: 100%;
          padding-bottom: 0;
          border-right: 0;
          position: relative;
        }
        #Top_bar #menu ul li a {
          padding: 0 25px;
          margin: 0;
          display: block;
          height: auto;
          line-height: normal;
          border: none;
        }
        #Top_bar #menu ul li a:after {
          display: none;
        }
        #Top_bar #menu ul li a span {
          border: none;
          line-height: 44px;
          display: inline;
          padding: 0;
        }
        #Top_bar #menu ul li a span.description {
          margin: 0 0 0 5px;
        }
        #Top_bar #menu ul li.submenu .menu-toggle {
          display: block;
          position: absolute;
          right: 15px;
          top: 0;
          width: 44px;
          height: 44px;
          line-height: 44px;
          font-size: 30px;
          font-weight: 300;
          text-align: center;
          cursor: pointer;
          color: #444;
          opacity: 0.33;
        }
        #Top_bar #menu ul li.submenu .menu-toggle:after {
          content: "+";
        }
        #Top_bar #menu ul li.hover > .menu-toggle:after {
          content: "-";
        }
        #Top_bar #menu ul li.hover a {
          border-bottom: 0;
        }
        #Top_bar #menu ul li a span:after {
          display: none !important;
        }
        #Top_bar #menu ul.mfn-megamenu li .menu-toggle {
          display: none;
        }
        #Top_bar #menu ul li ul {
          position: relative !important;
          left: 0 !important;
          top: 0;
          padding: 0;
          margin-left: 0 !important;
          width: auto !important;
          background-image: none !important;
          box-shadow: 0 0 0 0 transparent !important;
          -webkit-box-shadow: 0 0 0 0 transparent !important;
        }
        #Top_bar #menu ul li ul li {
          width: 100% !important;
        }
        #Top_bar #menu ul li ul li a {
          padding: 0 20px 0 35px;
        }
        #Top_bar #menu ul li ul li a .menu-arrow {
          display: none;
        }
        #Top_bar #menu ul li ul li a span {
          padding: 0;
        }
        #Top_bar #menu ul li ul li a span:after {
          display: none !important;
        }
        #Top_bar .menu > li > ul.mfn-megamenu a.mfn-megamenu-title {
          text-transform: uppercase;
          font-weight: 400;
        }
        #Top_bar .menu > li > ul.mfn-megamenu > li > ul {
          display: block !important;
          position: inherit;
          left: auto;
          top: auto;
        }
        #Top_bar #menu ul li ul li ul {
          border-left: 0 !important;
          padding: 0;
          top: 0;
        }
        #Top_bar #menu ul li ul li ul li a {
          padding: 0 20px 0 45px;
        }
        .rtl #Top_bar #menu {
          left: 1px;
          right: auto;
        }
        .rtl #Top_bar a.responsive-menu-toggle {
          left: 20px;
          right: auto;
        }
        .rtl #Top_bar #menu ul li.submenu .menu-toggle {
          left: 15px;
          right: auto;
          border-left: none;
          border-right: 1px solid #eee;
        }
        .rtl #Top_bar #menu ul li ul {
          left: auto !important;
          right: 0 !important;
        }
        .rtl #Top_bar #menu ul li ul li a {
          padding: 0 30px 0 20px;
        }
        .rtl #Top_bar #menu ul li ul li ul li a {
          padding: 0 40px 0 20px;
        }
        .header-stack .menu_wrapper a.responsive-menu-toggle {
          position: static !important;
          margin: 11px 0 !important;
        }
        .header-stack .menu_wrapper #menu {
          left: 0;
          right: auto;
        }
        .rtl.header-stack #Top_bar #menu {
          left: auto;
          right: 0;
        }
        .admin-bar #Header_creative {
          top: 32px;
        }
        .header-creative.layout-boxed {
          padding-top: 85px;
        }
        .header-creative.layout-full-width #Wrapper {
          padding-top: 60px;
        }
        #Header_creative {
          position: fixed;
          width: 100%;
          left: 0 !important;
          top: 0;
          z-index: 1001;
        }
        #Header_creative .creative-wrapper {
          display: block !important;
          opacity: 1 !important;
        }
        #Header_creative .creative-menu-toggle,
        #Header_creative .creative-social {
          display: none !important;
          opacity: 1 !important;
          filter: alpha(opacity=100) !important;
        }
        #Header_creative #Top_bar {
          position: static;
          width: 100%;
        }
        #Header_creative #Top_bar #logo {
          height: 50px;
          line-height: 50px;
          padding: 5px 0;
        }
        #Header_creative #Top_bar #logo img.logo-sticky {
          max-height: 40px !important;
        }
        #Header_creative #logo img.logo-main {
          display: none;
        }
        #Header_creative #logo img.logo-sticky {
          display: inline-block;
        }
        .logo-no-sticky-padding #Header_creative #Top_bar #logo {
          height: 60px;
          line-height: 60px;
          padding: 0;
        }
        .logo-no-sticky-padding
          #Header_creative
          #Top_bar
          #logo
          img.logo-sticky {
          max-height: 60px !important;
        }
        #Header_creative #Top_bar #header_cart {
          top: 21px;
        }
        #Header_creative #Top_bar #search_button {
          top: 20px;
        }
        #Header_creative #Top_bar .wpml-languages {
          top: 11px;
        }
        #Header_creative #Top_bar .action_button {
          top: 9px;
        }
        #Header_creative #Top_bar .top_bar_right {
          height: 60px;
          top: 0;
        }
        #Header_creative #Top_bar .top_bar_right:before {
          display: none;
        }
        #Header_creative #Top_bar .top_bar_right_wrapper {
          top: 0;
        }
        #Header_creative #Action_bar {
          display: none;
        }
        #Header_creative.scroll {
          overflow: visible !important;
        }
      }
      #Header_wrapper,
      #Intro {
        background-color: #fdfdfd;
      }
      #Subheader {
        background-color: rgba(12, 44, 84, 0.75);
      }
      .header-classic #Action_bar,
      .header-fixed #Action_bar,
      .header-plain #Action_bar,
      .header-split #Action_bar,
      .header-stack #Action_bar {
        background-color: #2c2c2c;
      }
      #Sliding-top {
        background-color: #545454;
      }
      #Sliding-top a.sliding-top-control {
        border-right-color: #545454;
      }
      #Sliding-top.st-center a.sliding-top-control,
      #Sliding-top.st-left a.sliding-top-control {
        border-top-color: #545454;
      }
      #Footer {
        background-color: #1a1a1a;
      }
      body,
      ul.timeline_items,
      .icon_box a .desc,
      .icon_box a:hover .desc,
      .feature_list ul li a,
      .list_item a,
      .list_item a:hover,
      .widget_recent_entries ul li a,
      .flat_box a,
      .flat_box a:hover,
      .story_box .desc,
      .content_slider.carouselul li a .title,
      .content_slider.flat.description ul li .desc,
      .content_slider.flat.description ul li a .desc,
      .post-nav.minimal a i {
        color: #4e4e4f;
      }
      .post-nav.minimal a svg {
        fill: #4e4e4f;
      }
      .themecolor,
      .opening_hours .opening_hours_wrapper li span,
      .fancy_heading_icon .icon_top,
      .fancy_heading_arrows .icon-right-dir,
      .fancy_heading_arrows .icon-left-dir,
      .fancy_heading_line .title,
      .button-love a.mfn-love,
      .format-link .post-title .icon-link,
      .pager-single > span,
      .pager-single a:hover,
      .widget_meta ul,
      .widget_pages ul,
      .widget_rss ul,
      .widget_mfn_recent_comments ul li:after,
      .widget_archive ul,
      .widget_recent_comments ul li:after,
      .widget_nav_menu ul,
      .woocommerce ul.products li.product .price,
      .shop_slider .shop_slider_ul li .item_wrapper .price,
      .woocommerce-page ul.products li.product .price,
      .widget_price_filter .price_label .from,
      .widget_price_filter .price_label .to,
      .woocommerce ul.product_list_widget li .quantity .amount,
      .woocommerce .product div.entry-summary .price,
      .woocommerce .star-rating span,
      #Error_404 .error_pic i,
      .style-simple #Filters .filters_wrapper ul li a:hover,
      .style-simple #Filters .filters_wrapper ul li.current-cat a,
      .style-simple .quick_fact .title {
        color: #2991d6;
      }
      .themebg,
      #comments .commentlist > li .reply a.comment-reply-link,
      #Filters .filters_wrapper ul li a:hover,
      #Filters .filters_wrapper ul li.current-cat a,
      .fixed-nav .arrow,
      .offer_thumb .slider_pagination a:before,
      .offer_thumb .slider_pagination a.selected:after,
      .pager .pages a:hover,
      .pager .pages a.active,
      .pager .pages span.page-numbers.current,
      .pager-single span:after,
      .portfolio_group.exposure .portfolio-item .desc-inner .line,
      .Recent_posts ul li .desc:after,
      .Recent_posts ul li .photo .c,
      .slider_pagination a.selected,
      .slider_pagination .slick-active a,
      .slider_pagination a.selected:after,
      .slider_pagination .slick-active a:after,
      .testimonials_slider .slider_images,
      .testimonials_slider .slider_images a:after,
      .testimonials_slider .slider_images:before,
      #Top_bar a#header_cart span,
      .widget_categories ul,
      .widget_mfn_menu ul li a:hover,
      .widget_mfn_menu ul li.current-menu-item:not(.current-menu-ancestor) > a,
      .widget_mfn_menu ul li.current_page_item:not(.current_page_ancestor) > a,
      .widget_product_categories ul,
      .widget_recent_entries ul li:after,
      .woocommerce-account table.my_account_orders .order-number a,
      .woocommerce-MyAccount-navigation ul li.is-active a,
      .style-simple .accordion .question:after,
      .style-simple .faq .question:after,
      .style-simple .icon_box .desc_wrapper .title:before,
      .style-simple #Filters .filters_wrapper ul li a:after,
      .style-simple .article_box .desc_wrapper p:after,
      .style-simple .sliding_box .desc_wrapper:after,
      .style-simple .trailer_box:hover .desc,
      .tp-bullets.simplebullets.round .bullet.selected,
      .tp-bullets.simplebullets.round .bullet.selected:after,
      .tparrows.default,
      .tp-bullets.tp-thumbs .bullet.selected:after {
        background-color: #2991d6;
      }
      .Latest_news ul li .photo,
      .Recent_posts.blog_news ul li .photo,
      .style-simple .opening_hours .opening_hours_wrapper li label,
      .style-simple .timeline_items li:hover h3,
      .style-simple .timeline_items li:nth-child(even):hover h3,
      .style-simple .timeline_items li:hover .desc,
      .style-simple .timeline_items li:nth-child(even):hover,
      .style-simple .offer_thumb .slider_pagination a.selected {
        border-color: #2991d6;
      }
      a {
        color: #2991d6;
      }
      a:hover {
        color: #2275ac;
      }
      *::-moz-selection {
        background-color: #2991d6;
      }
      *::selection {
        background-color: #2991d6;
      }
      .blockquote p.author span,
      .counter .desc_wrapper .title,
      .article_box .desc_wrapper p,
      .team .desc_wrapper p.subtitle,
      .pricing-box .plan-header p.subtitle,
      .pricing-box .plan-header .price sup.period,
      .chart_box p,
      .fancy_heading .inside,
      .fancy_heading_line .slogan,
      .post-meta,
      .post-meta a,
      .post-footer,
      .post-footer a span.label,
      .pager .pages a,
      .button-love a .label,
      .pager-single a,
      #comments .commentlist > li .comment-author .says,
      .fixed-nav .desc .date,
      .filters_buttons li.label,
      .Recent_posts ul li a .desc .date,
      .widget_recent_entries ul li .post-date,
      .tp_recent_tweets .twitter_time,
      .widget_price_filter .price_label,
      .shop-filters .woocommerce-result-count,
      .woocommerce ul.product_list_widget li .quantity,
      .widget_shopping_cart ul.product_list_widget li dl,
      .product_meta .posted_in,
      .woocommerce .shop_table .product-name .variation > dd,
      .shipping-calculator-button:after,
      .shop_slider .shop_slider_ul li .item_wrapper .price del,
      .testimonials_slider .testimonials_slider_ul li .author span,
      .testimonials_slider .testimonials_slider_ul li .author span a,
      .Latest_news ul li .desc_footer,
      .share-simple-wrapper .icons a {
        color: #a8a8a8;
      }
      h1,
      h1 a,
      h1 a:hover,
      .text-logo #logo {
        color: #353638;
      }
      h2,
      h2 a,
      h2 a:hover {
        color: #353638;
      }
      h3,
      h3 a,
      h3 a:hover {
        color: #2b393f;
      }
      h4,
      h4 a,
      h4 a:hover,
      .style-simple .sliding_box .desc_wrapper h4 {
        color: #2d4b66;
      }
      h5,
      h5 a,
      h5 a:hover {
        color: #353638;
      }
      h6,
      h6 a,
      h6 a:hover,
      a.content_link .title {
        color: #353638;
      }
      .dropcap,
      .highlight:not(.highlight_image) {
        background-color: #2991d6;
      }
      a.button,
      a.tp-button {
        background-color: #f7f7f7;
        color: #747474;
      }
      .button-stroke a.button,
      .button-stroke a.button.action_button,
      .button-stroke a.button .button_icon i,
      .button-stroke a.tp-button {
        border-color: #f7f7f7;
        color: #747474;
      }
      .button-stroke a:hover.button,
      .button-stroke a:hover.tp-button {
        background-color: #f7f7f7 !important;
        color: #fff;
      }
      a.button_theme,
      a.tp-button.button_theme,
      button,
      input[type="submit"],
      input[type="reset"],
      input[type="button"] {
        background-color: #2991d6;
        color: #ffffff;
      }
      .button-stroke a.button.button_theme:not(.action_button),
      .button-stroke a.button.button_theme .button_icon i,
      .button-stroke a.tp-button.button_theme,
      .button-stroke button,
      .button-stroke input[type="submit"],
      .button-stroke input[type="reset"],
      .button-stroke input[type="button"] {
        border-color: #2991d6;
        color: #2991d6 !important;
      }
      .button-stroke a.button.button_theme:hover,
      .button-stroke a.tp-button.button_theme:hover,
      .button-stroke button:hover,
      .button-stroke input[type="submit"]:hover,
      .button-stroke input[type="reset"]:hover,
      .button-stroke input[type="button"]:hover {
        background-color: #2991d6 !important;
        color: #ffffff !important;
      }
      a.mfn-link {
        color: #656b6f;
      }
      a.mfn-link-2 span,
      a:hover.mfn-link-2 span:before,
      a.hover.mfn-link-2 span:before,
      a.mfn-link-5 span,
      a.mfn-link-8:after,
      a.mfn-link-8:before {
        background: #2195de;
      }
      a:hover.mfn-link {
        color: #2991d6;
      }
      a.mfn-link-2 span:before,
      a:hover.mfn-link-4:before,
      a:hover.mfn-link-4:after,
      a.hover.mfn-link-4:before,
      a.hover.mfn-link-4:after,
      a.mfn-link-5:before,
      a.mfn-link-7:after,
      a.mfn-link-7:before {
        background: #2275ac;
      }
      a.mfn-link-6:before {
        border-bottom-color: #2275ac;
      }
      .woocommerce #respond input#submit,
      .woocommerce a.button,
      .woocommerce button.button,
      .woocommerce input.button,
      .woocommerce #respond input#submit:hover,
      .woocommerce a.button:hover,
      .woocommerce button.button:hover,
      .woocommerce input.button:hover {
        background-color: #2991d6;
        color: #fff;
      }
      .woocommerce #respond input#submit.alt,
      .woocommerce a.button.alt,
      .woocommerce button.button.alt,
      .woocommerce input.button.alt,
      .woocommerce #respond input#submit.alt:hover,
      .woocommerce a.button.alt:hover,
      .woocommerce button.button.alt:hover,
      .woocommerce input.button.alt:hover {
        background-color: #2991d6;
        color: #fff;
      }
      .woocommerce #respond input#submit.disabled,
      .woocommerce #respond input#submit:disabled,
      .woocommerce #respond input#submit[disabled]:disabled,
      .woocommerce a.button.disabled,
      .woocommerce a.button:disabled,
      .woocommerce a.button[disabled]:disabled,
      .woocommerce button.button.disabled,
      .woocommerce button.button:disabled,
      .woocommerce button.button[disabled]:disabled,
      .woocommerce input.button.disabled,
      .woocommerce input.button:disabled,
      .woocommerce input.button[disabled]:disabled {
        background-color: #2991d6;
        color: #fff;
      }
      .woocommerce #respond input#submit.disabled:hover,
      .woocommerce #respond input#submit:disabled:hover,
      .woocommerce #respond input#submit[disabled]:disabled:hover,
      .woocommerce a.button.disabled:hover,
      .woocommerce a.button:disabled:hover,
      .woocommerce a.button[disabled]:disabled:hover,
      .woocommerce button.button.disabled:hover,
      .woocommerce button.button:disabled:hover,
      .woocommerce button.button[disabled]:disabled:hover,
      .woocommerce input.button.disabled:hover,
      .woocommerce input.button:disabled:hover,
      .woocommerce input.button[disabled]:disabled:hover {
        background-color: #2991d6;
        color: #fff;
      }
      .button-stroke.woocommerce-page #respond input#submit,
      .button-stroke.woocommerce-page a.button:not(.action_button),
      .button-stroke.woocommerce-page button.button,
      .button-stroke.woocommerce-page input.button {
        border: 2px solid #2991d6 !important;
        color: #2991d6 !important;
      }
      .button-stroke.woocommerce-page #respond input#submit:hover,
      .button-stroke.woocommerce-page a.button:not(.action_button):hover,
      .button-stroke.woocommerce-page button.button:hover,
      .button-stroke.woocommerce-page input.button:hover {
        background-color: #2991d6 !important;
        color: #fff !important;
      }
      .column_column ul,
      .column_column ol,
      .the_content_wrapper ul,
      .the_content_wrapper ol {
        color: #737e86;
      }
      .hr_color,
      .hr_color hr,
      .hr_dots span {
        color: #2991d6;
        background: #2991d6;
      }
      .hr_zigzag i {
        color: #2991d6;
      }
      .highlight-left:after,
      .highlight-right:after {
        background: #2991d6;
      }
      @media only screen and (max-width: 767px) {
        .highlight-left .wrap:first-child,
        .highlight-right .wrap:last-child {
          background: #2991d6;
        }
      }
      #Header .top_bar_left,
      .header-classic #Top_bar,
      .header-plain #Top_bar,
      .header-stack #Top_bar,
      .header-split #Top_bar,
      .header-fixed #Top_bar,
      .header-below #Top_bar,
      #Header_creative,
      #Top_bar #menu,
      .sticky-tb-color #Top_bar.is-sticky {
        background-color: #fdfdfd;
      }
      #Top_bar .wpml-languages a.active,
      #Top_bar .wpml-languages ul.wpml-lang-dropdown {
        background-color: #fdfdfd;
      }
      #Top_bar .top_bar_right:before {
        background-color: #e3e3e3;
      }
      #Header .top_bar_right {
        background-color: #f5f5f5;
      }
      #Top_bar .top_bar_right a:not(.action_button) {
        color: #444444;
      }
      #Top_bar .menu > li > a,
      #Top_bar #menu ul li.submenu .menu-toggle {
        color: #44525e;
      }
      #Top_bar .menu > li.current-menu-item > a,
      #Top_bar .menu > li.current_page_item > a,
      #Top_bar .menu > li.current-menu-parent > a,
      #Top_bar .menu > li.current-page-parent > a,
      #Top_bar .menu > li.current-menu-ancestor > a,
      #Top_bar .menu > li.current-page-ancestor > a,
      #Top_bar .menu > li.current_page_ancestor > a,
      #Top_bar .menu > li.hover > a {
        color: #9e5b9c;
      }
      #Top_bar .menu > li a:after {
        background: #9e5b9c;
      }
      .menuo-arrows
        #Top_bar
        .menu
        > li.submenu
        > a
        > span:not(.description)::after {
        border-top-color: #44525e;
      }
      #Top_bar
        .menu
        > li.current-menu-item.submenu
        > a
        > span:not(.description)::after,
      #Top_bar
        .menu
        > li.current_page_item.submenu
        > a
        > span:not(.description)::after,
      #Top_bar
        .menu
        > li.current-menu-parent.submenu
        > a
        > span:not(.description)::after,
      #Top_bar
        .menu
        > li.current-page-parent.submenu
        > a
        > span:not(.description)::after,
      #Top_bar
        .menu
        > li.current-menu-ancestor.submenu
        > a
        > span:not(.description)::after,
      #Top_bar
        .menu
        > li.current-page-ancestor.submenu
        > a
        > span:not(.description)::after,
      #Top_bar
        .menu
        > li.current_page_ancestor.submenu
        > a
        > span:not(.description)::after,
      #Top_bar .menu > li.hover.submenu > a > span:not(.description)::after {
        border-top-color: #9e5b9c;
      }
      .menu-highlight #Top_bar #menu > ul > li.current-menu-item > a,
      .menu-highlight #Top_bar #menu > ul > li.current_page_item > a,
      .menu-highlight #Top_bar #menu > ul > li.current-menu-parent > a,
      .menu-highlight #Top_bar #menu > ul > li.current-page-parent > a,
      .menu-highlight #Top_bar #menu > ul > li.current-menu-ancestor > a,
      .menu-highlight #Top_bar #menu > ul > li.current-page-ancestor > a,
      .menu-highlight #Top_bar #menu > ul > li.current_page_ancestor > a,
      .menu-highlight #Top_bar #menu > ul > li.hover > a {
        background: #f2f2f2;
      }
      .menu-arrow-bottom #Top_bar .menu > li > a:after {
        border-bottom-color: #9e5b9c;
      }
      .menu-arrow-top #Top_bar .menu > li > a:after {
        border-top-color: #9e5b9c;
      }
      .header-plain #Top_bar .menu > li.current-menu-item > a,
      .header-plain #Top_bar .menu > li.current_page_item > a,
      .header-plain #Top_bar .menu > li.current-menu-parent > a,
      .header-plain #Top_bar .menu > li.current-page-parent > a,
      .header-plain #Top_bar .menu > li.current-menu-ancestor > a,
      .header-plain #Top_bar .menu > li.current-page-ancestor > a,
      .header-plain #Top_bar .menu > li.current_page_ancestor > a,
      .header-plain #Top_bar .menu > li.hover > a,
      .header-plain #Top_bar a:hover#header_cart,
      .header-plain #Top_bar a:hover#search_button,
      .header-plain #Top_bar .wpml-languages:hover,
      .header-plain #Top_bar .wpml-languages ul.wpml-lang-dropdown {
        background: #f2f2f2;
        color: #9e5b9c;
      }
      .header-plain #Top_bar,
      .header-plain #Top_bar .menu > li > a span:not(.description),
      .header-plain #Top_bar a#header_cart,
      .header-plain #Top_bar a#search_button,
      .header-plain #Top_bar .wpml-languages,
      .header-plain #Top_bar a.button.action_button {
        border-color: #f2f2f2;
      }
      #Top_bar .menu > li ul {
        background-color: #f2f2f2;
      }
      #Top_bar .menu > li ul li a {
        color: #5f5f5f;
      }
      #Top_bar .menu > li ul li a:hover,
      #Top_bar .menu > li ul li.hover > a {
        color: #2e2e2e;
      }
      #Top_bar .search_wrapper {
        background: #2991d6;
      }
      .overlay-menu-toggle {
        color: #5e5e5e !important;
        background: transparent;
      }
      #Overlay {
        background: rgba(41, 145, 214, 0.95);
      }
      #overlay-menu ul li a,
      .header-overlay .overlay-menu-toggle.focus {
        color: #ffffff;
      }
      #overlay-menu ul li.current-menu-item > a,
      #overlay-menu ul li.current_page_item > a,
      #overlay-menu ul li.current-menu-parent > a,
      #overlay-menu ul li.current-page-parent > a,
      #overlay-menu ul li.current-menu-ancestor > a,
      #overlay-menu ul li.current-page-ancestor > a,
      #overlay-menu ul li.current_page_ancestor > a {
        color: #b1dcfb;
      }
      #Top_bar .responsive-menu-toggle,
      #Header_creative .creative-menu-toggle,
      #Header_creative .responsive-menu-toggle {
        color: #5e5e5e;
        background: transparent;
      }
      #Side_slide {
        background-color: #191919;
        border-color: #191919;
      }
      #Side_slide,
      #Side_slide .search-wrapper input.field,
      #Side_slide a:not(.button),
      #Side_slide #menu ul li.submenu .menu-toggle {
        color: #a6a6a6;
      }
      #Side_slide a:not(.button):hover,
      #Side_slide a.active,
      #Side_slide #menu ul li.hover > .menu-toggle {
        color: #ffffff;
      }
      #Side_slide #menu ul li.current-menu-item > a,
      #Side_slide #menu ul li.current_page_item > a,
      #Side_slide #menu ul li.current-menu-parent > a,
      #Side_slide #menu ul li.current-page-parent > a,
      #Side_slide #menu ul li.current-menu-ancestor > a,
      #Side_slide #menu ul li.current-page-ancestor > a,
      #Side_slide #menu ul li.current_page_ancestor > a,
      #Side_slide #menu ul li.hover > a,
      #Side_slide #menu ul li:hover > a {
        color: #ffffff;
      }
      #Action_bar .contact_details {
        color: #bbbbbb;
      }
      #Action_bar .contact_details a {
        color: #0095eb;
      }
      #Action_bar .contact_details a:hover {
        color: #007cc3;
      }
      #Action_bar .social li a,
      #Header_creative .social li a,
      #Action_bar .social-menu a {
        color: #bbbbbb;
      }
      #Action_bar .social li a:hover,
      #Header_creative .social li a:hover,
      #Action_bar .social-menu a:hover {
        color: #ffffff;
      }
      #Subheader .title {
        color: #ffffff;
      }
      #Subheader ul.breadcrumbs li,
      #Subheader ul.breadcrumbs li a {
        color: rgba(255, 255, 255, 0.6);
      }
      #Footer,
      #Footer .widget_recent_entries ul li a {
        color: #cecece;
      }
      #Footer a {
        color: #f2f2f2;
      }
      #Footer a:hover {
        color: #2275ac;
      }
      #Footer h1,
      #Footer h1 a,
      #Footer h1 a:hover,
      #Footer h2,
      #Footer h2 a,
      #Footer h2 a:hover,
      #Footer h3,
      #Footer h3 a,
      #Footer h3 a:hover,
      #Footer h4,
      #Footer h4 a,
      #Footer h4 a:hover,
      #Footer h5,
      #Footer h5 a,
      #Footer h5 a:hover,
      #Footer h6,
      #Footer h6 a,
      #Footer h6 a:hover {
        color: #2991d6;
      }
      #Footer .themecolor,
      #Footer .widget_meta ul,
      #Footer .widget_pages ul,
      #Footer .widget_rss ul,
      #Footer .widget_mfn_recent_comments ul li:after,
      #Footer .widget_archive ul,
      #Footer .widget_recent_comments ul li:after,
      #Footer .widget_nav_menu ul,
      #Footer .widget_price_filter .price_label .from,
      #Footer .widget_price_filter .price_label .to,
      #Footer .star-rating span {
        color: #2991d6;
      }
      #Footer .themebg,
      #Footer .widget_categories ul,
      #Footer .Recent_posts ul li .desc:after,
      #Footer .Recent_posts ul li .photo .c,
      #Footer .widget_recent_entries ul li:after,
      #Footer .widget_mfn_menu ul li a:hover,
      #Footer .widget_product_categories ul {
        background-color: #2991d6;
      }
      #Footer .Recent_posts ul li a .desc .date,
      #Footer .widget_recent_entries ul li .post-date,
      #Footer .tp_recent_tweets .twitter_time,
      #Footer .widget_price_filter .price_label,
      #Footer .shop-filters .woocommerce-result-count,
      #Footer ul.product_list_widget li .quantity,
      #Footer .widget_shopping_cart ul.product_list_widget li dl {
        color: #a8a8a8;
      }
      #Footer .footer_copy .social li a,
      #Footer .footer_copy .social-menu a {
        color: #65666c;
      }
      #Footer .footer_copy .social li a:hover,
      #Footer .footer_copy .social-menu a:hover {
        color: #ffffff;
      }
      a#back_to_top.button.button_js,
      #popup_contact > a.button {
        color: #65666c;
        background: transparent;
        -webkit-box-shadow: none;
        box-shadow: none;
      }
      a#back_to_top.button.button_js:after,
      #popup_contact > a.button:after {
        display: none;
      }
      #Sliding-top,
      #Sliding-top .widget_recent_entries ul li a {
        color: #cccccc;
      }
      #Sliding-top a {
        color: #2991d6;
      }
      #Sliding-top a:hover {
        color: #2275ac;
      }
      #Sliding-top h1,
      #Sliding-top h1 a,
      #Sliding-top h1 a:hover,
      #Sliding-top h2,
      #Sliding-top h2 a,
      #Sliding-top h2 a:hover,
      #Sliding-top h3,
      #Sliding-top h3 a,
      #Sliding-top h3 a:hover,
      #Sliding-top h4,
      #Sliding-top h4 a,
      #Sliding-top h4 a:hover,
      #Sliding-top h5,
      #Sliding-top h5 a,
      #Sliding-top h5 a:hover,
      #Sliding-top h6,
      #Sliding-top h6 a,
      #Sliding-top h6 a:hover {
        color: #ffffff;
      }
      #Sliding-top .themecolor,
      #Sliding-top .widget_meta ul,
      #Sliding-top .widget_pages ul,
      #Sliding-top .widget_rss ul,
      #Sliding-top .widget_mfn_recent_comments ul li:after,
      #Sliding-top .widget_archive ul,
      #Sliding-top .widget_recent_comments ul li:after,
      #Sliding-top .widget_nav_menu ul,
      #Sliding-top .widget_price_filter .price_label .from,
      #Sliding-top .widget_price_filter .price_label .to,
      #Sliding-top .star-rating span {
        color: #2991d6;
      }
      #Sliding-top .themebg,
      #Sliding-top .widget_categories ul,
      #Sliding-top .Recent_posts ul li .desc:after,
      #Sliding-top .Recent_posts ul li .photo .c,
      #Sliding-top .widget_recent_entries ul li:after,
      #Sliding-top .widget_mfn_menu ul li a:hover,
      #Sliding-top .widget_product_categories ul {
        background-color: #2991d6;
      }
      #Sliding-top .Recent_posts ul li a .desc .date,
      #Sliding-top .widget_recent_entries ul li .post-date,
      #Sliding-top .tp_recent_tweets .twitter_time,
      #Sliding-top .widget_price_filter .price_label,
      #Sliding-top .shop-filters .woocommerce-result-count,
      #Sliding-top ul.product_list_widget li .quantity,
      #Sliding-top .widget_shopping_cart ul.product_list_widget li dl {
        color: #a8a8a8;
      }
      blockquote,
      blockquote a,
      blockquote a:hover {
        color: #444444;
      }
      .image_frame .image_wrapper .image_links,
      .portfolio_group.masonry-hover
        .portfolio-item
        .masonry-hover-wrapper
        .hover-desc {
        background: rgba(41, 145, 214, 0.8);
      }
      .masonry.tiles .post-item .post-desc-wrapper .post-desc .post-title:after,
      .masonry.tiles .post-item.no-img,
      .masonry.tiles .post-item.format-quote,
      .blog-teaser li .desc-wrapper .desc .post-title:after,
      .blog-teaser li.no-img,
      .blog-teaser li.format-quote {
        background: #2991d6;
      }
      .image_frame .image_wrapper .image_links a {
        color: #ffffff;
      }
      .image_frame .image_wrapper .image_links a:hover {
        background: #ffffff;
        color: #2991d6;
      }
      .image_frame {
        border-color: #f8f8f8;
      }
      .image_frame .image_wrapper .mask::after {
        background: rgba(255, 255, 255, 0.4);
      }
      .sliding_box .desc_wrapper {
        background: #2991d6;
      }
      .sliding_box .desc_wrapper:after {
        border-bottom-color: #2991d6;
      }
      .counter .icon_wrapper i {
        color: #2991d6;
      }
      .quick_fact .number-wrapper {
        color: #2991d6;
      }
      .progress_bars .bars_list li .bar .progress {
        background-color: #2991d6;
      }
      a:hover.icon_bar {
        color: #2991d6 !important;
      }
      a.content_link,
      a:hover.content_link {
        color: #2991d6;
      }
      a.content_link:before {
        border-bottom-color: #2991d6;
      }
      a.content_link:after {
        border-color: #2991d6;
      }
      .get_in_touch,
      .infobox {
        background-color: #2991d6;
      }
      .google-map-contact-wrapper .get_in_touch:after {
        border-top-color: #2991d6;
      }
      .timeline_items li h3:before,
      .timeline_items:after,
      .timeline .post-item:before {
        border-color: #2991d6;
      }
      .how_it_works .image .number {
        background: #2991d6;
      }
      .trailer_box .desc .subtitle,
      .trailer_box.plain .desc .line {
        background-color: #2991d6;
      }
      .trailer_box.plain .desc .subtitle {
        color: #2991d6;
      }
      .icon_box .icon_wrapper,
      .icon_box a .icon_wrapper,
      .style-simple .icon_box:hover .icon_wrapper {
        color: #2991d6;
      }
      .icon_box:hover .icon_wrapper:before,
      .icon_box a:hover .icon_wrapper:before {
        background-color: #2991d6;
      }
      ul.clients.clients_tiles li .client_wrapper:hover:before {
        background: #2991d6;
      }
      ul.clients.clients_tiles li .client_wrapper:after {
        border-bottom-color: #2991d6;
      }
      .list_item.lists_1 .list_left {
        background-color: #2991d6;
      }
      .list_item .list_left {
        color: #2991d6;
      }
      .feature_list ul li .icon i {
        color: #2991d6;
      }
      .feature_list ul li:hover,
      .feature_list ul li:hover a {
        background: #2991d6;
      }
      .ui-tabs .ui-tabs-nav li.ui-state-active a,
      .accordion .question.active .title > .acc-icon-plus,
      .accordion .question.active .title > .acc-icon-minus,
      .faq .question.active .title > .acc-icon-plus,
      .faq .question.active .title,
      .accordion .question.active .title {
        color: #2991d6;
      }
      .ui-tabs .ui-tabs-nav li.ui-state-active a:after {
        background: #2991d6;
      }
      body.table-hover:not(.woocommerce-page) table tr:hover td {
        background: #2991d6;
      }
      .pricing-box .plan-header .price sup.currency,
      .pricing-box .plan-header .price > span {
        color: #2991d6;
      }
      .pricing-box .plan-inside ul li .yes {
        background: #2991d6;
      }
      .pricing-box-box.pricing-box-featured {
        background: #2991d6;
      }
      input[type="date"],
      input[type="email"],
      input[type="number"],
      input[type="password"],
      input[type="search"],
      input[type="tel"],
      input[type="text"],
      input[type="url"],
      select,
      textarea,
      .woocommerce .quantity input.qty,
      .dark input[type="email"],
      .dark input[type="password"],
      .dark input[type="tel"],
      .dark input[type="text"],
      .dark select,
      .dark textarea {
        color: #626262;
        background-color: rgba(255, 255, 255, 1);
        border-color: #ebebeb;
      }
      ::-webkit-input-placeholder {
        color: #929292;
      }
      ::-moz-placeholder {
        color: #929292;
      }
      :-ms-input-placeholder {
        color: #929292;
      }
      input[type="date"]:focus,
      input[type="email"]:focus,
      input[type="number"]:focus,
      input[type="password"]:focus,
      input[type="search"]:focus,
      input[type="tel"]:focus,
      input[type="text"]:focus,
      input[type="url"]:focus,
      select:focus,
      textarea:focus {
        color: #1982c2;
        background-color: rgba(233, 245, 252, 1) !important;
        border-color: #d5e5ee;
      }
      :focus::-webkit-input-placeholder {
        color: #929292;
      }
      :focus::-moz-placeholder {
        color: #929292;
      }
      .woocommerce span.onsale,
      .shop_slider .shop_slider_ul li .item_wrapper span.onsale {
        border-top-color: #2991d6 !important;
      }
      .woocommerce .widget_price_filter .ui-slider .ui-slider-handle {
        border-color: #2991d6 !important;
      }
      @media only screen and (min-width: 768px) {
        .header-semi #Top_bar:not(.is-sticky) {
          background-color: rgba(253, 253, 253, 0.8);
        }
      }
      @media only screen and (max-width: 767px) {
        #Top_bar {
          background: #fdfdfd !important;
        }
        #Action_bar {
          background: #ffffff !important;
        }
        #Action_bar .contact_details {
          color: #222222;
        }
        #Action_bar .contact_details a {
          color: #0095eb;
        }
        #Action_bar .contact_details a:hover {
          color: #007cc3;
        }
        #Action_bar .social li a,
        #Action_bar .social-menu a {
          color: #bbbbbb;
        }
        #Action_bar .social li a:hover,
        #Action_bar .social-menu a:hover {
          color: #777777;
        }
      }
      html {
        background-color: #fcfcfc;
      }
      #Wrapper,
      #Content {
        background-color: #f9f9f9;
      }
      body,
      button,
      span.date_label,
      .timeline_items li h3 span,
      input[type="submit"],
      input[type="reset"],
      input[type="button"],
      input[type="text"],
      input[type="password"],
      input[type="tel"],
      input[type="email"],
      textarea,
      select,
      .offer_li .title h3 {
        font-family: "Raleway", Arial, Tahoma, sans-serif;
      }
      #menu > ul > li > a,
      .action_button,
      #overlay-menu ul li a {
        font-family: "Source Sans Pro", Arial, Tahoma, sans-serif;
      }
      #Subheader .title {
        font-family: "Source Sans Pro", Arial, Tahoma, sans-serif;
      }
      h1,
      h2,
      h3,
      h4,
      .text-logo #logo {
        font-family: "Source Sans Pro", Arial, Tahoma, sans-serif;
      }
      h5,
      h6 {
        font-family: "Source Sans Pro", Arial, Tahoma, sans-serif;
      }
      blockquote {
        font-family: "Source Sans Pro", Arial, Tahoma, sans-serif;
      }
      .chart_box .chart .num,
      .counter .desc_wrapper .number-wrapper,
      .how_it_works .image .number,
      .pricing-box .plan-header .price,
      .quick_fact .number-wrapper,
      .woocommerce .product div.entry-summary .price {
        font-family: "Source Sans Pro", Arial, Tahoma, sans-serif;
      }
      body {
        font-size: 16px;
        line-height: 24px;
        font-weight: 400;
        letter-spacing: 0px;
      }
      big,
      .big {
        font-size: 18px;
        line-height: 28px;
        font-weight: 400;
        letter-spacing: 0px;
      }
      #menu > ul > li > a,
      a.button.action_button,
      #overlay-menu ul li a {
        font-size: 20px;
        font-weight: 400;
        letter-spacing: 0px;
      }
      #overlay-menu ul li a {
        line-height: 30px;
      }
      #Subheader .title {
        font-size: 25px;
        line-height: 25px;
        font-weight: 400;
        letter-spacing: 0px;
      }
      h1,
      .text-logo #logo {
        font-size: 48px;
        line-height: 48px;
        font-weight: 100;
        letter-spacing: 2px;
      }
      h2 {
        font-size: 40px;
        line-height: 40px;
        font-weight: 100;
        letter-spacing: 2px;
      }
      h3 {
        font-size: 32px;
        line-height: 34px;
        font-weight: 100;
        letter-spacing: 1px;
      }
      h4 {
        font-size: 24px;
        line-height: 28px;
        font-weight: 700;
        letter-spacing: 1px;
      }
      h5 {
        font-size: 20px;
        line-height: 24px;
        font-weight: 500;
        letter-spacing: 2px;
      }
      h6 {
        font-size: 14px;
        line-height: 19px;
        font-weight: 400;
        letter-spacing: 0px;
      }
      #Intro .intro-title {
        font-size: 70px;
        line-height: 70px;
        font-weight: 400;
        letter-spacing: 0px;
      }
      @media only screen and (min-width: 768px) and (max-width: 959px) {
        body {
          font-size: 14px;
          line-height: 20px;
        }
        big,
        .big {
          font-size: 15px;
          line-height: 24px;
        }
        #menu > ul > li > a,
        a.button.action_button,
        #overlay-menu ul li a {
          font-size: 17px;
        }
        #overlay-menu ul li a {
          line-height: 25.5px;
        }
        #Subheader .title {
          font-size: 21px;
          line-height: 21px;
        }
        h1,
        .text-logo #logo {
          font-size: 41px;
          line-height: 41px;
        }
        h2 {
          font-size: 34px;
          line-height: 34px;
        }
        h3 {
          font-size: 27px;
          line-height: 29px;
        }
        h4 {
          font-size: 20px;
          line-height: 24px;
        }
        h5 {
          font-size: 17px;
          line-height: 20px;
        }
        h6 {
          font-size: 13px;
          line-height: 19px;
        }
        #Intro .intro-title {
          font-size: 60px;
          line-height: 60px;
        }
        blockquote {
          font-size: 15px;
        }
        .chart_box .chart .num {
          font-size: 45px;
          line-height: 45px;
        }
        .counter .desc_wrapper .number-wrapper {
          font-size: 45px;
          line-height: 45px;
        }
        .counter .desc_wrapper .title {
          font-size: 14px;
          line-height: 18px;
        }
        .faq .question .title {
          font-size: 14px;
        }
        .fancy_heading .title {
          font-size: 38px;
          line-height: 38px;
        }
        .offer .offer_li .desc_wrapper .title h3 {
          font-size: 32px;
          line-height: 32px;
        }
        .offer_thumb_ul li.offer_thumb_li .desc_wrapper .title h3 {
          font-size: 32px;
          line-height: 32px;
        }
        .pricing-box .plan-header h2 {
          font-size: 27px;
          line-height: 27px;
        }
        .pricing-box .plan-header .price > span {
          font-size: 40px;
          line-height: 40px;
        }
        .pricing-box .plan-header .price sup.currency {
          font-size: 18px;
          line-height: 18px;
        }
        .pricing-box .plan-header .price sup.period {
          font-size: 14px;
          line-height: 14px;
        }
        .quick_fact .number {
          font-size: 80px;
          line-height: 80px;
        }
        .trailer_box .desc h2 {
          font-size: 27px;
          line-height: 27px;
        }
        .widget > h3 {
          font-size: 17px;
          line-height: 20px;
        }
      }
      @media only screen and (min-width: 480px) and (max-width: 767px) {
        body {
          font-size: 13px;
          line-height: 19px;
        }
        big,
        .big {
          font-size: 14px;
          line-height: 21px;
        }
        #menu > ul > li > a,
        a.button.action_button,
        #overlay-menu ul li a {
          font-size: 15px;
        }
        #overlay-menu ul li a {
          line-height: 22.5px;
        }
        #Subheader .title {
          font-size: 19px;
          line-height: 19px;
        }
        h1,
        .text-logo #logo {
          font-size: 36px;
          line-height: 36px;
        }
        h2 {
          font-size: 30px;
          line-height: 30px;
        }
        h3 {
          font-size: 24px;
          line-height: 26px;
        }
        h4 {
          font-size: 18px;
          line-height: 21px;
        }
        h5 {
          font-size: 15px;
          line-height: 19px;
        }
        h6 {
          font-size: 13px;
          line-height: 19px;
        }
        #Intro .intro-title {
          font-size: 53px;
          line-height: 53px;
        }
        blockquote {
          font-size: 14px;
        }
        .chart_box .chart .num {
          font-size: 40px;
          line-height: 40px;
        }
        .counter .desc_wrapper .number-wrapper {
          font-size: 40px;
          line-height: 40px;
        }
        .counter .desc_wrapper .title {
          font-size: 13px;
          line-height: 16px;
        }
        .faq .question .title {
          font-size: 13px;
        }
        .fancy_heading .title {
          font-size: 34px;
          line-height: 34px;
        }
        .offer .offer_li .desc_wrapper .title h3 {
          font-size: 28px;
          line-height: 28px;
        }
        .offer_thumb_ul li.offer_thumb_li .desc_wrapper .title h3 {
          font-size: 28px;
          line-height: 28px;
        }
        .pricing-box .plan-header h2 {
          font-size: 24px;
          line-height: 24px;
        }
        .pricing-box .plan-header .price > span {
          font-size: 34px;
          line-height: 34px;
        }
        .pricing-box .plan-header .price sup.currency {
          font-size: 16px;
          line-height: 16px;
        }
        .pricing-box .plan-header .price sup.period {
          font-size: 13px;
          line-height: 13px;
        }
        .quick_fact .number {
          font-size: 70px;
          line-height: 70px;
        }
        .trailer_box .desc h2 {
          font-size: 24px;
          line-height: 24px;
        }
        .widget > h3 {
          font-size: 16px;
          line-height: 19px;
        }
      }
      @media only screen and (max-width: 479px) {
        body {
          font-size: 13px;
          line-height: 19px;
        }
        big,
        .big {
          font-size: 13px;
          line-height: 19px;
        }
        #menu > ul > li > a,
        a.button.action_button,
        #overlay-menu ul li a {
          font-size: 13px;
        }
        #overlay-menu ul li a {
          line-height: 19.5px;
        }
        #Subheader .title {
          font-size: 15px;
          line-height: 19px;
        }
        h1,
        .text-logo #logo {
          font-size: 29px;
          line-height: 29px;
        }
        h2 {
          font-size: 24px;
          line-height: 24px;
        }
        h3 {
          font-size: 19px;
          line-height: 20px;
        }
        h4 {
          font-size: 14px;
          line-height: 19px;
        }
        h5 {
          font-size: 13px;
          line-height: 19px;
        }
        h6 {
          font-size: 13px;
          line-height: 19px;
        }
        #Intro .intro-title {
          font-size: 42px;
          line-height: 42px;
        }
        blockquote {
          font-size: 13px;
        }
        .chart_box .chart .num {
          font-size: 35px;
          line-height: 35px;
        }
        .counter .desc_wrapper .number-wrapper {
          font-size: 35px;
          line-height: 35px;
        }
        .counter .desc_wrapper .title {
          font-size: 13px;
          line-height: 26px;
        }
        .faq .question .title {
          font-size: 13px;
        }
        .fancy_heading .title {
          font-size: 30px;
          line-height: 30px;
        }
        .offer .offer_li .desc_wrapper .title h3 {
          font-size: 26px;
          line-height: 26px;
        }
        .offer_thumb_ul li.offer_thumb_li .desc_wrapper .title h3 {
          font-size: 26px;
          line-height: 26px;
        }
        .pricing-box .plan-header h2 {
          font-size: 21px;
          line-height: 21px;
        }
        .pricing-box .plan-header .price > span {
          font-size: 32px;
          line-height: 32px;
        }
        .pricing-box .plan-header .price sup.currency {
          font-size: 14px;
          line-height: 14px;
        }
        .pricing-box .plan-header .price sup.period {
          font-size: 13px;
          line-height: 13px;
        }
        .quick_fact .number {
          font-size: 60px;
          line-height: 60px;
        }
        .trailer_box .desc h2 {
          font-size: 21px;
          line-height: 21px;
        }
        .widget > h3 {
          font-size: 15px;
          line-height: 18px;
        }
      }
      .with_aside .sidebar.columns {
        width: 23%;
      }
      .with_aside .sections_group {
        width: 77%;
      }
      .aside_both .sidebar.columns {
        width: 18%;
      }
      .aside_both .sidebar.sidebar-1 {
        margin-left: -82%;
      }
      .aside_both .sections_group {
        width: 64%;
        margin-left: 18%;
      }
      @media only screen and (min-width: 1240px) {
        #Wrapper,
        .with_aside .content_wrapper {
          max-width: 1240px;
        }
        .section_wrapper,
        .container {
          max-width: 1220px;
        }
        .layout-boxed.header-boxed #Top_bar.is-sticky {
          max-width: 1240px;
        }
      }
      @media only screen and (max-width: 767px) {
        .section_wrapper,
        .container,
        .four.columns .widget-area {
          max-width: 700px !important;
        }
      }
      #Top_bar #logo,
      .header-fixed #Top_bar #logo,
      .header-plain #Top_bar #logo,
      .header-transparent #Top_bar #logo {
        height: 85px;
        line-height: 85px;
        padding: 5px 0;
      }
      .logo-overflow #Top_bar:not(.is-sticky) .logo {
        height: 95px;
      }
      #Top_bar .menu > li > a {
        padding: 17.5px 0;
      }
      .menu-highlight:not(.header-creative) #Top_bar .menu > li > a {
        margin: 22.5px 0;
      }
      .header-plain:not(.menu-highlight)
        #Top_bar
        .menu
        > li
        > a
        span:not(.description) {
        line-height: 95px;
      }
      .header-fixed #Top_bar .menu > li > a {
        padding: 32.5px 0;
      }
      #Top_bar .top_bar_right,
      .header-plain #Top_bar .top_bar_right {
        height: 95px;
      }
      #Top_bar .top_bar_right_wrapper {
        top: 27.5px;
      }
      .header-plain #Top_bar a#header_cart,
      .header-plain #Top_bar a#search_button,
      .header-plain #Top_bar .wpml-languages,
      .header-plain #Top_bar a.button.action_button {
        line-height: 95px;
      }
      .header-plain #Top_bar .wpml-languages,
      .header-plain #Top_bar a.button.action_button {
        height: 95px;
      }
      @media only screen and (max-width: 767px) {
        #Top_bar a.responsive-menu-toggle {
          top: 52.5px;
        }
        .mobile-header-mini #Top_bar #logo {
          height: 50px !important;
          line-height: 50px !important;
          margin: 5px 0;
        }
      }
      .twentytwenty-before-label::before {
        content: "Before";
      }
      .twentytwenty-after-label::before {
        content: "After";
      }
      #Side_slide {
        right: -250px;
        width: 250px;
      }
      .blog-teaser li .desc-wrapper .desc {
        background-position-y: -1px;
      }
    </style>
    <!-- style | custom css | theme options -->
    <style id="mfn-dnmc-theme-css">
      /* List */
      .list_item .list_right h4 {
        font-size: 22px;
        color: #353638;
        font-weight: 400;
        letter-spacing: 1px;
      }

      /* Infobox */
      .infobox {
        background-position: right top;
        max-width: 350px;
        margin: 0 auto;
      }
      .infobox h3 {
        font-size: 24px;
        font-weight: 700;
        letter-spacing: 1px;
        line-height: 25px;
      }

      /* Hover box */
      .hover_box .hover_box_wrapper {
        padding-bottom: 2px;
      }

      /* Footer */
      #Footer .widget h4 {
        font-weight: 300;
      }

      .image_frame .image_wrapper {
        max-height: 200px;
      }

      .single-team .image_frame .image_wrapper {
        max-height: none !important;
      }
    </style>
    <meta
      name="generator"
      content="Powered by Slider Revolution 5.4.7.3 - responsive, Mobile-Friendly Slider Plugin for WordPress with comfortable drag and drop interface."
    />
    <script type="text/javascript">
      function setREVStartSize(e) {
        try {
          e.c = jQuery(e.c);
          var i = jQuery(window).width(),
            t = 9999,
            r = 0,
            n = 0,
            l = 0,
            f = 0,
            s = 0,
            h = 0;
          if (
            (e.responsiveLevels &&
              (jQuery.each(e.responsiveLevels, function (e, f) {
                f > i && ((t = r = f), (l = e)),
                  i > f && f > r && ((r = f), (n = e));
              }),
              t > r && (l = n)),
            (f = e.gridheight[l] || e.gridheight[0] || e.gridheight),
            (s = e.gridwidth[l] || e.gridwidth[0] || e.gridwidth),
            (h = i / s),
            (h = h > 1 ? 1 : h),
            (f = Math.round(h * f)),
            "fullscreen" == e.sliderLayout)
          ) {
            var u = (e.c.width(), jQuery(window).height());
            if (void 0 != e.fullScreenOffsetContainer) {
              var c = e.fullScreenOffsetContainer.split(",");
              if (c)
                jQuery.each(c, function (e, i) {
                  u = jQuery(i).length > 0 ? u - jQuery(i).outerHeight(!0) : u;
                }),
                  e.fullScreenOffset.split("%").length > 1 &&
                  void 0 != e.fullScreenOffset &&
                  e.fullScreenOffset.length > 0
                    ? (u -=
                        (jQuery(window).height() *
                          parseInt(e.fullScreenOffset, 0)) /
                        100)
                    : void 0 != e.fullScreenOffset &&
                      e.fullScreenOffset.length > 0 &&
                      (u -= parseInt(e.fullScreenOffset, 0));
            }
            f = u;
          } else void 0 != e.minHeight && f < e.minHeight && (f = e.minHeight);
          e.c.closest(".rev_slider_wrapper").css({ height: f });
        } catch (d) {
          console.log("Failure at Presize of Slider:" + d);
        }
      }
    </script>
  </head>

  <!-- body -->
  <body
    class="team-template-default single single-team postid-125 color-custom style-default button-flat layout-full-width nice-scroll-on hide-love header-classic sticky-header sticky-white ab-hide subheader-title-left menuo-right menuo-no-borders footer-copy-center mobile-tb-hide mobile-mini-mr-ll be-reg-20890"
    data-rsssl="1"
  >
    <!-- mfn_hook_top --><!-- mfn_hook_top -->

    <!-- #Wrapper -->
    <div id="Wrapper">
      <!-- #Header_bg -->
      <div id="Header_wrapper">
        <!-- #Header -->
        <header id="Header">
          <!-- .header_placeholder 4sticky  -->
          <div class="header_placeholder"></div>

          <div id="Top_bar" class="loading">
            <div class="container">
              <div class="column one">
                <div class="top_bar_left clearfix">
                  <!-- Logo -->
                  <div class="logo">
                    <a
                      id="logo"
                      href="https://empathiccomputing.org"
                      title="Empathic Computing Lab"
                      data-height="85"
                      data-padding="5"
                      ><img
                        class="logo-main scale-with-grid"
                        src="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-retina="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-height=""
                        alt="" /><img
                        class="logo-sticky scale-with-grid"
                        src="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-retina="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-height=""
                        alt="" /><img
                        class="logo-mobile scale-with-grid"
                        src="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-retina="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-height=""
                        alt="" /><img
                        class="logo-mobile-sticky scale-with-grid"
                        src="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-retina="http://empathiccomputing.org/wp-content/uploads/2018/05/logo.png"
                        data-height=""
                        alt=""
                    /></a>
                  </div>
                  <div class="menu_wrapper">
                    <nav id="menu">
                      <ul id="menu-main-menu" class="menu menu-main">
                        <li
                          id="menu-item-111"
                          class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home"
                        >
                          <a href="https://empathiccomputing.org/"
                            ><span>Home</span></a
                          >
                        </li>
                        <li
                          id="menu-item-110"
                          class="menu-item menu-item-type-post_type menu-item-object-page"
                        >
                          <a href="https://empathiccomputing.org/about-us/"
                            ><span>About us</span></a
                          >
                        </li>
                        <li
                          id="menu-item-112"
                          class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children"
                        >
                          <a href="#"><span>Research</span></a>
                          <ul class="sub-menu">
                            <li
                              id="menu-item-108"
                              class="menu-item menu-item-type-post_type menu-item-object-page"
                            >
                              <a
                                href="https://empathiccomputing.org/publications/"
                                ><span>Publications</span></a
                              >
                            </li>
                            <li
                              id="menu-item-109"
                              class="menu-item menu-item-type-post_type menu-item-object-page"
                            >
                              <a href="https://empathiccomputing.org/projects/"
                                ><span>Projects</span></a
                              >
                            </li>
                            <li
                              id="menu-item-107"
                              class="menu-item menu-item-type-post_type menu-item-object-page"
                            >
                              <a href="https://empathiccomputing.org/videos/"
                                ><span>Videos</span></a
                              >
                            </li>
                            <li
                              id="menu-item-106"
                              class="menu-item menu-item-type-post_type menu-item-object-page"
                            >
                              <a
                                href="https://empathiccomputing.org/image-gallery/"
                                ><span>Image gallery</span></a
                              >
                            </li>
                          </ul>
                        </li>
                        <li
                          id="menu-item-105"
                          class="menu-item menu-item-type-post_type menu-item-object-page"
                        >
                          <a href="https://empathiccomputing.org/our-team/"
                            ><span>Our Team</span></a
                          >
                        </li>
                        <li
                          id="menu-item-104"
                          class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children"
                        >
                          <a href="https://empathiccomputing.org/news/"
                            ><span>News &#038; Events</span></a
                          >
                          <ul class="sub-menu">
                            <li
                              id="menu-item-1429"
                              class="menu-item menu-item-type-post_type menu-item-object-page"
                            >
                              <a
                                href="https://empathiccomputing.org/ecl-speaker-series/"
                                ><span>ECL Speaker Series</span></a
                              >
                            </li>
                            <li
                              id="menu-item-1542"
                              class="menu-item menu-item-type-post_type menu-item-object-page"
                            >
                              <a
                                href="https://empathiccomputing.org/ecl-workshops/"
                                ><span>ECL Workshops</span></a
                              >
                            </li>
                          </ul>
                        </li>
                        <li
                          id="menu-item-344"
                          class="menu-item menu-item-type-post_type menu-item-object-page"
                        >
                          <a href="https://empathiccomputing.org/blog/"
                            ><span>Blog</span></a
                          >
                        </li>
                        <li
                          id="menu-item-102"
                          class="menu-item menu-item-type-post_type menu-item-object-page"
                        >
                          <a href="https://empathiccomputing.org/join-us/"
                            ><span>Join us</span></a
                          >
                        </li>
                        <li
                          id="menu-item-103"
                          class="menu-item menu-item-type-post_type menu-item-object-page"
                        >
                          <a href="https://empathiccomputing.org/contact-us/"
                            ><span>Contact us</span></a
                          >
                        </li>
                      </ul>
                    </nav>
                    <a class="responsive-menu-toggle" href="#"
                      ><i class="icon-menu-fine"></i
                    ></a>
                  </div>

                  <div class="secondary_menu_wrapper">
                    <!-- #secondary-menu -->
                  </div>

                  <div class="banner_wrapper"></div>

                  <div class="search_wrapper">
                    <!-- #searchform -->

                    <form
                      method="get"
                      id="searchform"
                      action="https://empathiccomputing.org/"
                    >
                      <i class="icon_search icon-search-fine"></i>
                      <a href="#" class="icon_close"
                        ><i class="icon-cancel-fine"></i
                      ></a>

                      <input
                        type="text"
                        class="field"
                        name="s"
                        placeholder="Enter your search"
                      />
                      <input
                        type="submit"
                        class="submit"
                        value=""
                        style="display: none"
                      />
                    </form>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </header>

        <div id="Subheader" style="">
          <div class="container">
            <div class="column one">
              <h1 class="title">Mark Billinghurst</h1>
            </div>
          </div>
        </div>
      </div>

      <!-- mfn_hook_content_before --><!-- mfn_hook_content_before -->
      <!-- #Content -->
      <div id="Content" class="">
        <div class="content_wrapper clearfix">
          <!-- .sections_group -->
          <div class="sections_group">
            <div
              id="portfolio-item-125"
              class="no-title no-share post-125 team type-team status-publish has-post-thumbnail hentry"
            >
              <div class="section section-post-header">
                <div class="section_wrapper clearfix">
                  <div class="column one-third team-photo">
                    <div class="image_frame scale-with-grid">
                      <div class="image_wrapper">
                        <a
                          href="https://empathiccomputing.org/wp-content/uploads/2018/05/Mark1-570x5701.png"
                          rel="prettyphoto"
                          ><div class="mask"></div>
                          <img
                            width="570"
                            height="480"
                            src="https://empathiccomputing.org/wp-content/uploads/2018/05/Mark1-570x5701-570x480.png"
                            class="scale-with-grid wp-post-image"
                            alt=""
                        /></a>
                        <div class="image_links">
                          <a
                            href="https://empathiccomputing.org/wp-content/uploads/2018/05/Mark1-570x5701.png"
                            class="zoom"
                            rel="prettyphoto"
                            ><i class="icon-search"></i
                          ></a>
                        </div>
                      </div>
                    </div>

                    <div class="social-menu">
                      <a
                        target="_blank"
                        href="mailto:mark.billinghurst@unisa.edu.au"
                        class="icon-mail"
                      ></a>
                      <a
                        target="_blank"
                        href="https://www.facebook.com/mark.billinghurst"
                        class="icon-facebook-circled"
                      ></a>
                      <a
                        target="_blank"
                        href="https://twitter.com/marknb00"
                        class="icon-twitter-circled"
                      ></a>
                      <a
                        target="_blank"
                        href="https://www.linkedin.com/in/mark-billinghurst-4a636/"
                        class="icon-linkedin-circled"
                      ></a>
                    </div>
                  </div>
                  <div class="column two-third">
                    <h2>Mark Billinghurst</h2>
                    <h5>Director</h5>
                    <p>
                      Prof. Mark Billinghurst has a wealth of knowledge and
                      expertise in human-computer interface technology,
                      particularly in the area of Augmented Reality (the overlay
                      of three-dimensional images on the real world).
                    </p>
                    <p>
                      In 2002, the former HIT Lab US Research Associate
                      completed his PhD in Electrical Engineering, at the
                      University of Washington, under the supervision of
                      Professor Thomas Furness III and Professor Linda Shapiro.
                      As part of the research for his thesis titled Shared
                      Space: Exploration in Collaborative Augmented Reality, Dr
                      Billinghurst invented the Magic Book &#8211; an animated
                      children&#8217;s book that comes to life when viewed
                      through the lightweight head-mounted display (HMD).
                    </p>
                    <p>
                      Not surprisingly, Dr Billinghurst has achieved several
                      accolades in recent years for his contribution to Human
                      Interface Technology research. He was awarded a Discover
                      Magazine Award in 2001, for Entertainment for creating the
                      Magic Book technology. He was selected as one of eight
                      leading New Zealand innovators and entrepreneurs to be
                      showcased at the Carter Holt Harvey New Zealand Innovation
                      Pavilion at the America&#8217;s Cup Village from November
                      2002 until March 2003. In 2004 he was nominated for a
                      prestigious World Technology Network (WTN) World
                      Technology Award in the education category and in 2005 he
                      was appointed to the New Zealand Government&#8217;s Growth
                      and Innovation Advisory Board.
                    </p>
                    <p>
                      Originally educated in New Zealand, Dr Billinghurst is a
                      two-time graduate of Waikato University where he completed
                      a BCMS (Bachelor of Computing and Mathematical
                      Science)(first class honours) in 1990 and a Master of
                      Philosophy (Applied Mathematics &amp; Physics) in 1992.
                    </p>
                    <p>
                      Research interests: Dr. Billinghurst&#8217;s research
                      focuses primarily on advanced 3D user interfaces such as:
                    </p>
                    <ul>
                      <li>
                        Wearable Computing &#8211; Spatial and collaborative
                        interfaces for small wearable computers. These
                        interfaces address the idea of what is possible when you
                        merge ubiquitous computing and communications on the
                        body.
                      </li>
                      <li>
                        Shared Space &#8211; An interface that demonstrates how
                        augmented reality, the overlaying of virtual objects on
                        the real world, can radically enhance face-face and
                        remote collaboration.
                      </li>
                      <li>
                        Multimodal Input &#8211; Combining natural language and
                        artificial intelligence techniques to allow
                        human-computer interaction with an intuitive mix of
                        voice, gesture, speech, gaze and body motion.
                      </li>
                    </ul>
                  </div>
                </div>
                <div class="section_wrapper clearfix">
                    <h2>Publications</h2>
                        <!-- Container for publications -->
    <div id="reposync"></div>

    <!-- Add the script with full path -->
    <script src="X:/home/gshukla/Project/Project/dist/reposync.min.js"></script>
    
    <script>
        // Add error handling for script loading
        window.addEventListener('load', function() {
            if (typeof RepoSync === 'undefined') {
                console.error('RepoSync script failed to load');
                document.getElementById('reposync').innerHTML = 
                    'Error: RepoSync plugin failed to load. Please check the console for details.';
                return;
            }

            try {
                new RepoSync({
                    scholarId: "S-J_ItYAAAAJ",
                    title: "Dr. Mark Billinghurst Publications"
                });
            } catch (error) {
                console.error('Error initializing RepoSync:', error);
                document.getElementById('reposync').innerHTML = 
                    'Error: Failed to initialize RepoSync. Please check the console for details.';
            }
        });
    </script>
                  <div class="column one">
                    <h3 class="portfolio_title">Projects</h3>
                    <ul class="team_group lm_wrapper '. $team_classes .'">
                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/sharedsphere/"
                            >
                              <img
                                width="300"
                                height="169"
                                src="https://empathiccomputing.org/wp-content/uploads/2018/06/cap14.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14.jpg          1216w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14-300x169.jpg   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14-768x432.jpg   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14-1024x576.jpg 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14-260x146.jpg   260w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14-50x28.jpg      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/06/cap14-133x75.jpg    133w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/sharedsphere/"
                            >
                              SharedSphere</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            SharedSphere is a Mixed Reality based remote
                            collaboration system which not only allows sharing a
                            live captured immersive 360 panorama, but also
                            supports enriched two-way communication and
                            collaboration through sharing non-verbal
                            communication cues, such as view awareness cues,
                            drawn annotation, and hand gestures.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/augmented-mirrors/"
                            >
                              <img
                                width="300"
                                height="185"
                                src="https://empathiccomputing.org/wp-content/uploads/2018/07/mirror3.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/mirror3.jpg         727w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/mirror3-300x185.jpg 300w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/mirror3-237x146.jpg 237w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/mirror3-50x31.jpg    50w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/mirror3-122x75.jpg  122w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/augmented-mirrors/"
                            >
                              Augmented Mirrors</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            Mirrors are physical displays that show our real
                            world in reflection. While physical mirrors simply
                            show what is in the real world scene, with help of
                            digital technology, we can also alter the reality
                            reflected in the mirror. The Augmented Mirrors
                            project aims at exploring visualisation interaction
                            techniques for exploiting mirrors as Augmented
                            Reality (AR) displays. The project especially
                            focuses on using user interface agents for guiding
                            user interaction with Augmented Mirrors.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/mini-me/"
                            >
                              <img
                                width="300"
                                height="202"
                                src="https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars.jpg          3024w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars-300x202.jpg   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars-768x517.jpg   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars-1024x689.jpg 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars-217x146.jpg   217w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars-50x34.jpg      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/BothAvatars-112x75.jpg    112w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/mini-me/"
                            >
                              Mini-Me</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            Mini-Me is an adaptive avatar for enhancing Mixed
                            Reality (MR) remote collaboration between a local
                            Augmented Reality (AR) user and a remote Virtual
                            Reality (VR) user. The Mini-Me avatar represents the
                            VR user’s gaze direction and body gestures while it
                            transforms in size and orientation to stay within
                            the AR user’s field of view. We tested Mini-Me in
                            two collaborative scenarios: an asymmetric remote
                            expert in VR assisting a local worker in AR, and a
                            symmetric collaboration in urban planning. We found
                            that the presence of the Mini-Me significantly
                            improved Social Presence and the overall experience
                            of MR collaboration.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/pinpointing/"
                            >
                              <img
                                width="300"
                                height="209"
                                src="https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout.png          1387w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout-300x209.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout-768x534.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout-1024x712.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout-210x146.png   210w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout-50x35.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/TargetLayout-108x75.png    108w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/pinpointing/"
                            >
                              Pinpointing</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            Head and eye movement can be leveraged to improve
                            the user’s interaction repertoire for wearable
                            displays. Head movements are deliberate and
                            accurate, and provide the current state-of-the-art
                            pointing technique. Eye gaze can potentially be
                            faster and more ergonomic, but suffers from low
                            accuracy due to calibration errors and drift of
                            wearable eye-tracking sensors. This work
                            investigates precise, multimodal selection
                            techniques using head motion and eye gaze. A
                            comparison of speed and pointing accuracy reveals
                            the relative merits of each method, including the
                            achievable target size for robust selection. We
                            demonstrate and discuss example applications for
                            augmented reality, including compact menus with deep
                            structure, and a proof-of-concept method for on-line
                            correction of calibration drift.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/empathy-glasses/"
                            >
                              <img
                                width="300"
                                height="188"
                                src="https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2.jpg         594w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-300x188.jpg 300w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-233x146.jpg 233w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-50x31.jpg    50w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-120x75.jpg  120w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/empathy-glasses/"
                            >
                              Empathy Glasses</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            We have been developing a remote collaboration
                            system with Empathy Glasses, a head worn display
                            designed to create a stronger feeling of empathy
                            between remote collaborators. To do this, we
                            combined a head- mounted see-through display with a
                            facial expression recognition system, a heart rate
                            sensor, and an eye tracker. The goal is to enable a
                            remote person to see and hear from another person's
                            perspective and to understand how they are feeling.
                            In this way, the system shares non-verbal cues that
                            could help increase empathy between remote
                            collaborators.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/empathy-in-virtual-reality/"
                            >
                              <img
                                width="300"
                                height="144"
                                src="https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate.jpg          1898w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate-300x144.jpg   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate-768x368.jpg   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate-1024x490.jpg 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate-260x125.jpg   260w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate-50x24.jpg      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2018/07/HeartRate-150x72.jpg    150w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/empathy-in-virtual-reality/"
                            >
                              Empathy in Virtual Reality</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            Virtual reality (VR) interfaces is an influential
                            medium to trigger emotional changes in humans.
                            However, there is little research on making users of
                            VR interfaces aware of their own and in
                            collaborative interfaces, one another's emotional
                            state. In this project, through a series of system
                            development and user evaluations, we are
                            investigating how physiological data such as heart
                            rate, galvanic skin response, pupil dilation, and
                            EEG can be used as a medium to communicate emotional
                            states either to self (single user interfaces) or
                            the collaborator (collaborative interfaces). The
                            overarching goal is to make VR environments more
                            empathetic and collaborators more aware of each
                            other's emotional state.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/sharing-gesture-and-gaze-cues-for-enhancing-ar-collaboration/"
                            >
                              <img
                                width="300"
                                height="300"
                                src="https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-300x300.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-300x300.png 300w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-150x150.png 150w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-768x768.png 768w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-146x146.png 146w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-50x50.png    50w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-75x75.png    75w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-85x85.png    85w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1-80x80.png    80w,
                                  https://empathiccomputing.org/wp-content/uploads/2021/10/study2Concept-1.png         800w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/sharing-gesture-and-gaze-cues-for-enhancing-ar-collaboration/"
                            >
                              Sharing Gesture and Gaze Cues for Enhancing MR
                              Collaboration</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This research focuses on visualizing shared gaze
                            cues, designing interfaces for collaborative
                            experience, and incorporating multimodal interaction
                            techniques and physiological cues to support
                            empathic Mixed Reality (MR) remote collaboration
                            using HoloLens 2, Vive Pro Eye, Meta Pro, HP
                            Omnicept, Theta V 360 camera, Windows Speech
                            Recognition, Leap motion hand tracking, and
                            Zephyr/Shimmer Sensing technologies
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/using-a-mobile-phone-in-vr/"
                            >
                              <img
                                width="300"
                                height="225"
                                src="https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2.jpg          1630w,
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2-300x225.jpg   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2-768x576.jpg   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2-1024x768.jpg 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2-195x146.jpg   195w,
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2-50x37.jpg      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2022/08/Phone2-100x75.jpg    100w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/using-a-mobile-phone-in-vr/"
                            >
                              Using a Mobile Phone in VR</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            Virtual Reality (VR) Head-Mounted Display (HMD)
                            technology immerses a user in a computer generated
                            virtual environment. However, a VR HMD also blocks
                            the users’ view of their physical surroundings, and
                            so prevents them from using their mobile phones in a
                            natural manner. In this project, we present a novel
                            Augmented Virtuality (AV) interface that enables
                            people to naturally interact with a mobile phone in
                            real time in a virtual environment. The system
                            allows the user to wear a VR HMD while seeing
                            his/her 3D hands captured by a depth sensor and
                            rendered in different styles, and enables the user
                            to operate a virtual mobile phone aligned with their
                            real phone.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/show-me-around/"
                            >
                              <img
                                width="300"
                                height="193"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1.jpg          1246w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1-300x193.jpg   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1-768x494.jpg   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1-1024x659.jpg 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1-227x146.jpg   227w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1-50x32.jpg      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Show-Me-Around-1-117x75.jpg    117w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/show-me-around/"
                            >
                              Show Me Around</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This project introduces an immersive way to
                            experience a conference call - by using a 360°
                            camera to live stream a person’s surroundings to
                            remote viewers. Viewers have the ability to freely
                            look around the host video and get a better
                            understanding of the sender’s surroundings. Viewers
                            can also observe where the other participants are
                            looking, allowing them to understand better the
                            conversation and what people are paying attention
                            to. In a user study of the system, people found it
                            much more immersive than a traditional video
                            conferencing call and reported that they felt that
                            they were transported to a remote location. Possible
                            applications of this system include virtual tourism,
                            education, industrial monitoring, entertainment, and
                            more.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/tbi-cafe/"
                            >
                              <img
                                width="300"
                                height="169"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/04/ss10.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10.png          2560w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10-300x169.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10-768x432.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10-1024x576.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10-260x146.png   260w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10-50x28.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/ss10-133x75.png    133w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/tbi-cafe/"
                            >
                              TBI Cafe</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            Over 36,000 Kiwis experience Traumatic Brain Injury
                            (TBI) per year. TBI patients often experience severe
                            cognitive fatigue, which impairs their ability to
                            cope well in public/social settings. Rehabilitation
                            can involve taking people into social settings with
                            a therapist so that they can relearn how to interact
                            in these environments. However, this is a
                            time-consuming, expensive and difficult process. To
                            address this, we've created the TBI Cafe, a VR tool
                            designed to help TBI patients cope with their injury
                            and practice interacting in a cafe. In this
                            application, people in VR practice ordering food and
                            drink while interacting with virtual characters.
                            Different types of distractions are introduced, such
                            as a crying baby and loud conversations, which are
                            designed to make the experience more stressful, and
                            let the user practice managing stressful situations.
                            Clinical trials with the software are currently
                            underway.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/haptic-hongi/"
                            >
                              <img
                                width="206"
                                height="300"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi.png          722w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi-206x300.png  206w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi-701x1024.png 701w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi-100x146.png  100w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi-34x50.png     34w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/Haptic-Hongi-51x75.png     51w
                                "
                                sizes="(max-width: 206px) 100vw, 206px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/haptic-hongi/"
                            >
                              Haptic Hongi</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This project explores if XR technologies help
                            overcome intercultural discomfort by using Augmented
                            Reality (AR) and haptic feedback to present a
                            traditional Māori greeting. Using a Hololens2 AR
                            headset, guests see a pre-recorded volumetric
                            virtual video of Tania, a Māori woman, who greets
                            them in a re-imagined, contemporary first encounter
                            between indigenous Māori and newcomers. The
                            visitors, manuhiri, consider their response in the
                            absence of usual social pressures. After a brief
                            introduction, the virtual Tania slowly leans
                            forward, inviting the visitor to ‘hongi’, a pressing
                            together of noses and foreheads in a gesture
                            symbolising “ ...peace and oneness of thought,
                            purpose, desire, and hope”. This is felt as a haptic
                            response delivered via a custom-made actuator built
                            into the visitors' AR headset.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/radarhand/"
                            >
                              <img
                                width="300"
                                height="161"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5.png         901w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5-300x161.png 300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5-768x412.png 768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5-260x139.png 260w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5-50x27.png    50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/radar-hand-5-140x75.png  140w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/radarhand/"
                            >
                              RadarHand</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            RadarHand is a wrist-worn wearable system that uses
                            radar sensing to detect on-skin proprioceptive hand
                            gestures, making it easy to interact with simple
                            finger motions. Radar has the advantage of being
                            robust, private, small, penetrating materials and
                            requiring low computation costs. In this project, we
                            first evaluated the proprioceptive nature of the
                            back of the hand and found that the thumb is the
                            most proprioceptive of all the finger joints,
                            followed by the index finger, middle finger, ring
                            finger and pinky finger. This helped determine the
                            types of gestures most suitable for the system.
                            Next, we trained deep-learning models for gesture
                            classification. Out of 27 gesture group
                            possibilities, we achieved 92% accuracy for a
                            generic set of seven gestures and 93% accuracy for
                            the proprioceptive set of eight gestures. We also
                            evaluated RadarHand's performance in real-time and
                            achieved an accuracy of between 74% and 91%
                            depending if the system or user initiates the
                            gesture first. This research could contribute to a
                            new generation of radar-based interfaces that allow
                            people to interact with computers in a more natural
                            way.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/asymmetric-interaction-for-vr-sketching/"
                            >
                              <img
                                width="300"
                                height="197"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching.png         756w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-300x197.png 300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-223x146.png 223w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-50x33.png    50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-114x75.png  114w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/asymmetric-interaction-for-vr-sketching/"
                            >
                              Asymmetric Interaction for VR sketching</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This project explores how tool-based asymmetric VR
                            interfaces can be used by artists to create
                            immersive artwork more effectively. Most VR
                            interfaces use two input methods of the same type,
                            such as two handheld controllers or two bare-hand
                            gestures. However, it is common for artists to use
                            different tools in each hand, such as a pencil and
                            sketch pad. The research involves developed
                            interaction methods that use different input methods
                            in the edge hand, such as a stylus and gesture.
                            Using this interface, artists can rapidly sketch
                            their designs in VR. User studies are being
                            conducted to compare asymmetric and symmetric
                            interfaces to see which provides the best
                            performance and which the users prefer more.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/detecting-of-the-onset-of-cybersickness-using-physiological-cues/"
                            >
                              <img
                                width="300"
                                height="133"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang.png          1271w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang-300x133.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang-768x340.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang-1024x454.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang-260x115.png   260w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang-50x22.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/banner-chang-150x66.png    150w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/detecting-of-the-onset-of-cybersickness-using-physiological-cues/"
                            >
                              Detecting of the Onset of Cybersickness using
                              Physiological Cues</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            In this project we explore if the onset of
                            cybersickness can be detected by considering
                            multiple physiological signals simultaneously from
                            users in VR. We are particularly interested in
                            physiological cues that can be collected from the
                            current generation of VR HMDs, such as eye-gaze, and
                            heart rate. We are also interested in exploring
                            other physiological cues that could be available in
                            the near future in VR HMDs, such as GSR and EEG.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/kiwirescuer-a-new-interactive-exhibition-using-an-asymmetric-interaction/"
                            >
                              <img
                                width="300"
                                height="106"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1.png          2667w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1-300x106.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1-768x271.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1-1024x361.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1-260x92.png    260w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1-50x18.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/teaser-1-150x53.png    150w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/kiwirescuer-a-new-interactive-exhibition-using-an-asymmetric-interaction/"
                            >
                              KiwiRescuer: A new interactive exhibition using an
                              asymmetric interaction</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This research demo aims to address the problem of
                            passive and dull museum exhibition experiences that
                            many audiences still encounter. The current
                            approaches to exhibitions are typically less
                            interactive and mostly provide single sensory
                            information (e.g., visual, auditory, or haptic) in a
                            one-to-one experience.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/tangible-augmented-reality-for-learning-programming-learning/"
                            >
                              <img
                                width="300"
                                height="225"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1.png          1440w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1-300x225.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1-768x576.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1-1024x768.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1-195x146.png   195w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1-50x38.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/dmtry1-100x75.png    100w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/tangible-augmented-reality-for-learning-programming-learning/"
                            >
                              Tangible Augmented Reality for Learning
                              Programming Learning</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This project explores how tangible Augmented Reality
                            (AR) can be used to teach computer programming. We
                            have developed TARPLE, A Tangible Augmented Reality
                            Programming Learning Environment, and are studying
                            its efficacy for teaching text-based programming
                            languages to novice learners. TARPLE uses physical
                            blocks to represent programming functions and
                            overlays virtual imagery on the blocks to show the
                            programming code. Use can arrange the blocks by
                            moving them with their hands, and see the AR content
                            either through the Microsoft Hololens2 AR display,
                            or a handheld tablet. This research project expands
                            upon the broader question of educational AR as well
                            as on the questions of tangible programming
                            languages and tangible learning mediums. When
                            supported by the embodied learning and natural
                            interaction affordances of AR, physical objects may
                            hold the key to developing fundamental knowledge of
                            abstract, complex subjects for younger learners in
                            particular. It may also serve as a powerful future
                            tool in advancing early computational thinking
                            skills in novices. Evaluation of such learning
                            environments addresses the hypothesis that hybrid
                            tangible AR mediums are able to support an extended
                            learning taxonomy both within the classroom and
                            without.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/using-3d-spaces-and-360-video-content-for-collaboration/"
                            >
                              <img
                                width="300"
                                height="182"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/07/theo.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo.png          1600w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo-300x182.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo-768x466.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo-1024x621.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo-241x146.png   241w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo-50x30.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/theo-124x75.png    124w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/using-3d-spaces-and-360-video-content-for-collaboration/"
                            >
                              Using 3D Spaces and 360 Video Content for
                              Collaboration</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This project explores techniques to enhance
                            collaborative experience in Mixed Reality
                            environments using 3D reconstructions, 360 videos
                            and 2D images. Previous research has shown that 360
                            video can provide a high resolution immersive visual
                            space for collaboration, but little spatial
                            information. Conversely, 3D scanned environments can
                            provide high quality spatial cues, but with poor
                            visual resolution. This project combines both
                            approaches, enabling users to switch between a 3D
                            view or 360 video of a collaborative space. In this
                            hybrid interface, users can pick the representation
                            of space best suited to the needs of the
                            collaborative task. The project seeks to provide
                            design guidelines for collaboration systems to
                            enable empathic collaboration by sharing visual cues
                            and environments across time and space.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/mpconnect-a-mixed-presence-mixed-reality-system/"
                            >
                              <img
                                width="300"
                                height="208"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4.jpg"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4.jpg          1600w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4-300x208.jpg   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4-768x533.jpg   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4-1024x711.jpg 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4-210x146.jpg   210w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4-50x35.jpg      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/mitch4-108x75.jpg    108w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/mpconnect-a-mixed-presence-mixed-reality-system/"
                            >
                              MPConnect: A Mixed Presence Mixed Reality
                              System</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            This project explores how a Mixed Presence Mixed
                            Reality System can enhance remote collaboration.
                            Collaborative Mixed Reality (MR) is a popular area
                            of research, but most work has focused on one-to-one
                            systems where either both collaborators are
                            co-located or the collaborators are remote from one
                            another. For example, remote users might collaborate
                            in a shared Virtual Reality (VR) system, or a local
                            worker might use an Augmented Reality (AR) display
                            to connect with a remote expert to help them
                            complete a task.
                          </p>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <a
                              href="https://empathiccomputing.org/project/ar-based-spatiotemporal-interface-and-visualization-for-the-physical-task/"
                            >
                              <img
                                width="300"
                                height="171"
                                src="https://empathiccomputing.org/wp-content/uploads/2023/07/hw.png"
                                class="attachment-300x300 size-300x300 wp-post-image"
                                alt=""
                                itemprop="image"
                                srcset="
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw.png          1031w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw-300x171.png   300w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw-768x437.png   768w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw-1024x583.png 1024w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw-256x146.png   256w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw-50x28.png      50w,
                                  https://empathiccomputing.org/wp-content/uploads/2023/07/hw-132x75.png    132w
                                "
                                sizes="(max-width: 300px) 100vw, 300px"
                              />
                            </a>
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            <a
                              href="https://empathiccomputing.org/project/ar-based-spatiotemporal-interface-and-visualization-for-the-physical-task/"
                            >
                              AR-based spatiotemporal interface and
                              visualization for the physical task</a
                            >
                          </h5>
                          <p style="font-style: italic; font-size: 12px">
                            The proposed study aims to assist in solving
                            physical tasks such as mechanical assembly or
                            collaborative design efficiently by using augmented
                            reality-based space-time visualization techniques.
                            In particular, when disassembling/reassembling is
                            required, 3D recording of past actions and playback
                            visualization are used to help memorize the exact
                            assembly order and position of objects in the task.
                            This study proposes a novel method that employs
                            3D-based spatial information recording and augmented
                            reality-based playback to effectively support these
                            types of physical tasks.
                          </p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </div>
                <div class="section_wrapper clearfix">
                  <div class="column one">
                    <h3 class="portfolio_title">Publications</h3>
                    <ul class="team_group lm_wrapper '. $team_classes .'">
                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="225"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web.jpg         800w,
                                https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web-300x225.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web-768x576.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web-195x146.jpg 195w,
                                https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web-50x38.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/06/shared-sphere-web-100x75.jpg  100w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Mixed Reality Collaboration through Sharing a Live
                            Panorama
                          </h5>
                          <small
                            >Gun A. Lee, Theophilus Teo, Seungwon Kim, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gun A. Lee, Theophilus Teo, Seungwon Kim, and Mark
                            Billinghurst. 2017. Mixed reality collaboration
                            through sharing a live panorama. In SIGGRAPH Asia
                            2017 Mobile Graphics & Interactive Applications (SA
                            '17). ACM, New York, NY, USA, Article 14, 4 pages.
                            http://doi.acm.org/10.1145/3132787.3139203
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Lee:2017:MRC:3132787.3139203,<br />
                            author = {Lee, Gun A. and Teo, Theophilus and Kim,
                            Seungwon and Billinghurst, Mark},<br />
                            title = {Mixed Reality Collaboration Through Sharing
                            a Live Panorama},<br />
                            booktitle = {SIGGRAPH Asia 2017 Mobile Graphics \&
                            Interactive Applications},<br />
                            series = {SA '17},<br />
                            year = {2017},<br />
                            isbn = {978-1-4503-5410-3},<br />
                            location = {Bangkok, Thailand},<br />
                            pages = {14:1--14:4},<br />
                            articleno = {14},<br />
                            numpages = {4},<br />
                            url =
                            {http://doi.acm.org/10.1145/3132787.3139203},<br />
                            doi = {10.1145/3132787.3139203},<br />
                            acmid = {3139203},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {panorama, remote collaboration, shared
                            experience},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3139203"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3139203</a
                            ><br />Video:
                            <a
                              href="https://www.youtube.com/watch?v=q_giuLot76k"
                              target="_blank"
                              >https://www.youtube.com/watch?v=q_giuLot76k</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            One of the popular features on modern social
                            networking platforms is sharing live 360 panorama
                            video. This research investigates on how to further
                            improve shared live panorama based collaborative
                            experiences by applying Mixed Reality (MR)
                            technology. SharedSphere is a wearable MR remote
                            collaboration system. In addition to sharing a live
                            captured immersive panorama, SharedSphere enriches
                            the collaboration through overlaying MR
                            visualisation of non-verbal communication cues
                            (e.g., view awareness and gestures cues). User
                            feedback collected through a preliminary user study
                            indicated that sharing of live 360 panorama video
                            was beneficial by providing a more immersive
                            experience and supporting view independence. Users
                            also felt that the view awareness cues were helpful
                            for understanding the remote collaborator’s focus.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-300x300.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-300x300.jpg    300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-150x150.jpg    150w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-768x768.jpg    768w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-1024x1024.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-146x146.jpg    146w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-50x50.jpg       50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-75x75.jpg       75w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-85x85.jpg       85w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/MiniMe-80x80.jpg       80w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Mini-Me: An Adaptive Avatar for Mixed Reality Remote
                            Collaboration
                          </h5>
                          <small
                            >Thammathip Piumsomboon, Gun A Lee, Jonathon D Hart,
                            Barrett Ens, Robert W Lindeman, Bruce H Thomas, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Thammathip Piumsomboon, Gun A. Lee, Jonathon D.
                            Hart, Barrett Ens, Robert W. Lindeman, Bruce H.
                            Thomas, and Mark Billinghurst. 2018. Mini-Me: An
                            Adaptive Avatar for Mixed Reality Remote
                            Collaboration. In Proceedings of the 2018 CHI
                            Conference on Human Factors in Computing Systems
                            (CHI '18). ACM, New York, NY, USA, Paper 46, 13
                            pages. DOI: https://doi.org/10.1145/3173574.3173620
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Piumsomboon:2018:MAA:3173574.3173620,<br />
                            author = {Piumsomboon, Thammathip and Lee, Gun A.
                            and Hart, Jonathon D. and Ens, Barrett and Lindeman,
                            Robert W. and Thomas, Bruce H. and Billinghurst,
                            Mark},<br />
                            title = {Mini-Me: An Adaptive Avatar for Mixed
                            Reality Remote Collaboration},<br />
                            booktitle = {Proceedings of the 2018 CHI Conference
                            on Human Factors in Computing Systems},<br />
                            series = {CHI '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5620-6},<br />
                            location = {Montreal QC, Canada},<br />
                            pages = {46:1--46:13},<br />
                            articleno = {46},<br />
                            numpages = {13},<br />
                            url =
                            {http://doi.acm.org/10.1145/3173574.3173620},<br />
                            doi = {10.1145/3173574.3173620},<br />
                            acmid = {3173620},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {augmented reality, avatar, awareness,
                            gaze, gesture, mixed reality, redirected, remote
                            collaboration, remote embodiment, virtual
                            reality},<br />
                            } <br />
                            [download]
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3173620"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3173620</a
                            ><br />Video:
                            <a
                              href="https://www.youtube.com/watch?v=6aRYSsDJmEk"
                              target="_blank"
                              >https://www.youtube.com/watch?v=6aRYSsDJmEk</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present Mini-Me, an adaptive avatar for enhancing
                            Mixed Reality (MR) remote collaboration between a
                            local Augmented Reality (AR) user and a remote
                            Virtual Reality (VR) user. The Mini-Me avatar
                            represents the VR user's gaze direction and body
                            gestures while it transforms in size and orientation
                            to stay within the AR user's field of view. A user
                            study was conducted to evaluate Mini-Me in two
                            collaborative scenarios: an asymmetric remote expert
                            in VR assisting a local worker in AR, and a
                            symmetric collaboration in urban planning. We found
                            that the presence of the Mini-Me significantly
                            improved Social Presence and the overall experience
                            of MR collaboration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="192"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/Pinpointing.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Pinpointing.png         684w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Pinpointing-300x192.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Pinpointing-228x146.png 228w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Pinpointing-50x32.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Pinpointing-117x75.png  117w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Pinpointing: Precise Head-and Eye-Based Target
                            Selection for Augmented Reality
                          </h5>
                          <small
                            >Mikko Kytö, Barrett Ens, Thammathip Piumsomboon,
                            Gun A Lee, Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Mikko Kytö, Barrett Ens, Thammathip Piumsomboon, Gun
                            A. Lee, and Mark Billinghurst. 2018. Pinpointing:
                            Precise Head- and Eye-Based Target Selection for
                            Augmented Reality. In Proceedings of the 2018 CHI
                            Conference on Human Factors in Computing Systems
                            (CHI '18). ACM, New York, NY, USA, Paper 81, 14
                            pages. DOI: https://doi.org/10.1145/3173574.3173655
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Kyto:2018:PPH:3173574.3173655,<br />
                            author = {Kyt\"{o}, Mikko and Ens, Barrett and
                            Piumsomboon, Thammathip and Lee, Gun A. and
                            Billinghurst, Mark},<br />
                            title = {Pinpointing: Precise Head- and Eye-Based
                            Target Selection for Augmented Reality},<br />
                            booktitle = {Proceedings of the 2018 CHI Conference
                            on Human Factors in Computing Systems},<br />
                            series = {CHI '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5620-6},<br />
                            location = {Montreal QC, Canada},<br />
                            pages = {81:1--81:14},<br />
                            articleno = {81},<br />
                            numpages = {14},<br />
                            url =
                            {http://doi.acm.org/10.1145/3173574.3173655},<br />
                            doi = {10.1145/3173574.3173655},<br />
                            acmid = {3173655},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {augmented reality, eye tracking, gaze
                            interaction, head-worn display, refinement
                            techniques, target selection},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/ft_gateway.cfm?id=3173655&ftid=1958752&dwn=1&CFID=51906271&CFTOKEN=b63dc7f7afbcc656-4D4F3907-C934-F85B-2D539C0F52E3652A"
                              target="_blank"
                              >https://dl.acm.org/ft_gateway.cfm?id=3173655&ftid=1958752&dwn=1&CFID=51906271&CFTOKEN=b63dc7f7afbcc656-4D4F3907-C934-F85B-2D539C0F52E3652A</a
                            ><br />Video:
                            <a
                              href="https://youtu.be/nCX8zIEmv0s"
                              target="_blank"
                              >https://youtu.be/nCX8zIEmv0s</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Head and eye movement can be leveraged to improve
                            the user's interaction repertoire for wearable
                            displays. Head movements are deliberate and
                            accurate, and provide the current state-of-the-art
                            pointing technique. Eye gaze can potentially be
                            faster and more ergonomic, but suffers from low
                            accuracy due to calibration errors and drift of
                            wearable eye-tracking sensors. This work
                            investigates precise, multimodal selection
                            techniques using head motion and eye gaze. A
                            comparison of speed and pointing accuracy reveals
                            the relative merits of each method, including the
                            achievable target size for robust selection. We
                            demonstrate and discuss example applications for
                            augmented reality, including compact menus with deep
                            structure, and a proof-of-concept method for on-line
                            correction of calibration drift.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="100"
                              height="100"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1233-file2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1233-file2.jpg       100w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1233-file2-50x50.jpg  50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1233-file2-75x75.jpg  75w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1233-file2-85x85.jpg  85w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1233-file2-80x80.jpg  80w
                              "
                              sizes="(max-width: 100px) 100vw, 100px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Counterpoint: Exploring Mixed-Scale Gesture
                            Interaction for AR Applications
                          </h5>
                          <small
                            >Barrett Ens, Aaron Quigley, Hui-Shyong Yeo, Pourang
                            Irani, Thammathip Piumsomboon, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Barrett Ens, Aaron Quigley, Hui-Shyong Yeo, Pourang
                            Irani, Thammathip Piumsomboon, and Mark
                            Billinghurst. 2018. Counterpoint: Exploring
                            Mixed-Scale Gesture Interaction for AR Applications.
                            In Extended Abstracts of the 2018 CHI Conference on
                            Human Factors in Computing Systems (CHI EA '18).
                            ACM, New York, NY, USA, Paper LBW120, 6 pages. DOI:
                            https://doi.org/10.1145/3170427.3188513
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Ens:2018:CEM:3170427.3188513,<br />
                            author = {Ens, Barrett and Quigley, Aaron and Yeo,
                            Hui-Shyong and Irani, Pourang and Piumsomboon,
                            Thammathip and Billinghurst, Mark},<br />
                            title = {Counterpoint: Exploring Mixed-Scale Gesture
                            Interaction for AR Applications},<br />
                            booktitle = {Extended Abstracts of the 2018 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            series = {CHI EA '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5621-3},<br />
                            location = {Montreal QC, Canada},<br />
                            pages = {LBW120:1--LBW120:6},<br />
                            articleno = {LBW120},<br />
                            numpages = {6},<br />
                            url =
                            {http://doi.acm.org/10.1145/3170427.3188513},<br />
                            doi = {10.1145/3170427.3188513},<br />
                            acmid = {3188513},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {augmented reality, gesture interaction,
                            wearable computing},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://delivery.acm.org/10.1145/3190000/3188513/LBW120.pdf?ip=130.220.8.189&id=3188513&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531307952_cec04550a31b41dfba9a6865be86b8ac"
                              target="_blank"
                              >http://delivery.acm.org/10.1145/3190000/3188513/LBW120.pdf?ip=130.220.8.189&id=3188513&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531307952_cec04550a31b41dfba9a6865be86b8ac</a
                            ><br />Video:
                            <a
                              href="https://youtu.be/GZzC0Vhte4M"
                              target="_blank"
                              >https://youtu.be/GZzC0Vhte4M</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper presents ongoing work on a design
                            exploration for mixed-scale gestures, which
                            interleave microgestures with larger gestures for
                            computer interaction. We describe three prototype
                            applications that show various facets of this
                            multi-dimensional design space. These applications
                            portray various tasks on a Hololens Augmented
                            Reality display, using different combinations of
                            wearable sensors. Future work toward expanding the
                            design space and exploration is discussed, along
                            with plans toward evaluation of mixed-scale gesture
                            design.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="233"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139.jpg           3804w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-300x233.jpg    300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-768x597.jpg    768w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-1024x797.jpg  1024w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-188x146.jpg    188w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-50x39.jpg       50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-96x75.jpg       96w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-1280x1000.jpg 1280w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/lbw1139-960x750.jpg    960w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Levity: A Virtual Reality System that Responds to
                            Cognitive Load
                          </h5>
                          <small
                            >Lynda Gerry, Barrett Ens, Adam Drogemuller, Bruce
                            Thomas, Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Lynda Gerry, Barrett Ens, Adam Drogemuller, Bruce
                            Thomas, and Mark Billinghurst. 2018. Levity: A
                            Virtual Reality System that Responds to Cognitive
                            Load. In Extended Abstracts of the 2018 CHI
                            Conference on Human Factors in Computing Systems
                            (CHI EA '18). ACM, New York, NY, USA, Paper LBW610,
                            6 pages. DOI:
                            https://doi.org/10.1145/3170427.3188479
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Gerry:2018:LVR:3170427.3188479,<br />
                            author = {Gerry, Lynda and Ens, Barrett and
                            Drogemuller, Adam and Thomas, Bruce and
                            Billinghurst, Mark},<br />
                            title = {Levity: A Virtual Reality System That
                            Responds to Cognitive Load},<br />
                            booktitle = {Extended Abstracts of the 2018 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            series = {CHI EA '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5621-3},<br />
                            location = {Montreal QC, Canada},<br />
                            pages = {LBW610:1--LBW610:6},<br />
                            articleno = {LBW610},<br />
                            numpages = {6},<br />
                            url =
                            {http://doi.acm.org/10.1145/3170427.3188479},<br />
                            doi = {10.1145/3170427.3188479},<br />
                            acmid = {3188479},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {brain computer interface, cognitive
                            load, virtual reality, visual search task},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://delivery.acm.org/10.1145/3190000/3188479/LBW610.pdf?ip=130.220.8.189&id=3188479&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1531308138_ee3f5f719b239f1ee561612023b6fe1a"
                              target="_blank"
                              >http://delivery.acm.org/10.1145/3190000/3188479/LBW610.pdf?ip=130.220.8.189&id=3188479&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1531308138_ee3f5f719b239f1ee561612023b6fe1a</a
                            ><br />Video:
                            <a
                              href="https://youtu.be/r2csCoMvLeM"
                              target="_blank"
                              >https://youtu.be/r2csCoMvLeM</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper presents the ongoing development of a
                            proof-of-concept, adaptive system that uses a
                            neurocognitive signal to facilitate efficient
                            performance in a Virtual Reality visual search task.
                            The Levity system measures and interactively adjusts
                            the display of a visual array during a visual search
                            task based on the user's level of cognitive load,
                            measured with a 16-channel EEG device. Future
                            developments will validate the system and evaluate
                            its ability to improve search efficiency by
                            detecting and adapting to a user's cognitive
                            demands.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="251"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/snowdome.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/snowdome.png         450w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/snowdome-300x251.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/snowdome-175x146.png 175w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/snowdome-50x42.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/snowdome-90x75.png    90w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Snow Dome: A Multi-Scale Interaction in Mixed
                            Reality Remote Collaboration
                          </h5>
                          <small
                            >Thammathip Piumsomboon, Gun A Lee, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Thammathip Piumsomboon, Gun A. Lee, and Mark
                            Billinghurst. 2018. Snow Dome: A Multi-Scale
                            Interaction in Mixed Reality Remote Collaboration.
                            In Extended Abstracts of the 2018 CHI Conference on
                            Human Factors in Computing Systems (CHI EA '18).
                            ACM, New York, NY, USA, Paper D115, 4 pages. DOI:
                            https://doi.org/10.1145/3170427.3186495
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Piumsomboon:2018:SDM:3170427.3186495,<br />
                            author = {Piumsomboon, Thammathip and Lee, Gun A.
                            and Billinghurst, Mark},<br />
                            title = {Snow Dome: A Multi-Scale Interaction in
                            Mixed Reality Remote Collaboration},<br />
                            booktitle = {Extended Abstracts of the 2018 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            series = {CHI EA '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5621-3},<br />
                            location = {Montreal QC, Canada},<br />
                            pages = {D115:1--D115:4},<br />
                            articleno = {D115},<br />
                            numpages = {4},<br />
                            url =
                            {http://doi.acm.org/10.1145/3170427.3186495},<br />
                            doi = {10.1145/3170427.3186495},<br />
                            acmid = {3186495},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {augmented reality, avatar, mixed
                            reality, multiple, remote collaboration, remote
                            embodiment, scale, virtual reality},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://delivery.acm.org/10.1145/3190000/3186495/D115.pdf?ip=130.220.8.189&id=3186495&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531308619_3eface84d74bae70fd47b11af3589b10"
                              target="_blank"
                              >http://delivery.acm.org/10.1145/3190000/3186495/D115.pdf?ip=130.220.8.189&id=3186495&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531308619_3eface84d74bae70fd47b11af3589b10</a
                            ><br />Video:
                            <a
                              href="https://youtu.be/nm8A9wzobIE"
                              target="_blank"
                              >https://youtu.be/nm8A9wzobIE</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present Snow Dome, a Mixed Reality (MR) remote
                            collaboration application that supports a
                            multi-scale interaction for a Virtual Reality (VR)
                            user. We share a local Augmented Reality (AR) user's
                            reconstructed space with a remote VR user who has an
                            ability to scale themselves up into a giant or down
                            into a miniature for different perspectives and
                            interaction at that scale within the shared space.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="263"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.08.21-pm.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.08.21-pm.png         387w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.08.21-pm-300x263.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.08.21-pm-167x146.png 167w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.08.21-pm-50x44.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.08.21-pm-86x75.png    86w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Filtering Shared Social Data in AR
                          </h5>
                          <small
                            >Alaeddin Nassani, Huidong Bai, Gun Lee, Mark
                            Billinghurst, Tobias Langlotz, Robert W
                            Lindeman</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Alaeddin Nassani, Huidong Bai, Gun Lee, Mark
                            Billinghurst, Tobias Langlotz, and Robert W.
                            Lindeman. 2018. Filtering Shared Social Data in AR.
                            In Extended Abstracts of the 2018 CHI Conference on
                            Human Factors in Computing Systems (CHI EA '18).
                            ACM, New York, NY, USA, Paper LBW100, 6 pages. DOI:
                            https://doi.org/10.1145/3170427.3188609
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Nassani:2018:FSS:3170427.3188609,<br />
                            author = {Nassani, Alaeddin and Bai, Huidong and
                            Lee, Gun and Billinghurst, Mark and Langlotz, Tobias
                            and Lindeman, Robert W.},<br />
                            title = {Filtering Shared Social Data in AR},<br />
                            booktitle = {Extended Abstracts of the 2018 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            series = {CHI EA '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5621-3},<br />
                            location = {Montreal QC, Canada},<br />
                            pages = {LBW100:1--LBW100:6},<br />
                            articleno = {LBW100},<br />
                            numpages = {6},<br />
                            url =
                            {http://doi.acm.org/10.1145/3170427.3188609},<br />
                            doi = {10.1145/3170427.3188609},<br />
                            acmid = {3188609},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {360 panoramas, augmented reality, live
                            video stream, sharing social experiences, virtual
                            avatars},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://delivery.acm.org/10.1145/3190000/3188609/LBW100.pdf?ip=130.220.8.189&id=3188609&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531309037_59c3c6e906e725ca712e49b5a67d51af"
                              target="_blank"
                              >http://delivery.acm.org/10.1145/3190000/3188609/LBW100.pdf?ip=130.220.8.189&id=3188609&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531309037_59c3c6e906e725ca712e49b5a67d51af</a
                            ><br />Video:
                            <a
                              href="https://youtu.be/W-CDpBqe1yI"
                              target="_blank"
                              >https://youtu.be/W-CDpBqe1yI</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We describe a method and a prototype implementation
                            for filtering shared social data (eg, 360 video) in
                            a wearable Augmented Reality (eg, HoloLens)
                            application. The data filtering is based on
                            user-viewer relationships. For example, when sharing
                            a 360 video, if the user has an intimate
                            relationship with the viewer, then full fidelity (ie
                            the 360 video) of the user's environment is visible.
                            But if the two are strangers then only a snapshot
                            image is shared. By varying the fidelity of the
                            shared content, the viewer is able to focus more on
                            the data shared by their close relations and
                            differentiate this from other content. Also, the
                            approach enables the sharing-user to have more
                            control over the fidelity of the content shared with
                            their contacts for privacy.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="296"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001.jpg         743w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001-296x300.jpg 296w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001-144x146.jpg 144w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001-74x75.jpg    74w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/frobt-05-00037-g001-80x80.jpg    80w
                              "
                              sizes="(max-width: 296px) 100vw, 296px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Systematic Review of 10 Years of Augmented Reality
                            Usability Studies: 2005 to 2014
                          </h5>
                          <small
                            >Arindam Dey, Mark Billinghurst, Robert W Lindeman,
                            J Swan</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dey A, Billinghurst M, Lindeman RW and Swan JE II
                            (2018) A Systematic Review of 10 Years of Augmented
                            Reality Usability Studies: 2005 to 2014. Front.
                            Robot. AI 5:37. doi: 10.3389/frobt.2018.00037
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @ARTICLE{10.3389/frobt.2018.00037,<br />
                            AUTHOR={Dey, Arindam and Billinghurst, Mark and
                            Lindeman, Robert W. and Swan, J. Edward}, <br />
                            TITLE={A Systematic Review of 10 Years of Augmented
                            Reality Usability Studies: 2005 to 2014}, <br />
                            JOURNAL={Frontiers in Robotics and AI}, <br />
                            VOLUME={5}, <br />
                            PAGES={37}, <br />
                            YEAR={2018}, <br />
                            URL={https://www.frontiersin.org/article/10.3389/frobt.2018.00037},
                            <br />
                            DOI={10.3389/frobt.2018.00037}, <br />
                            ISSN={2296-9144}, <br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.frontiersin.org/articles/10.3389/frobt.2018.00037/full"
                              target="_blank"
                              >https://www.frontiersin.org/articles/10.3389/frobt.2018.00037/full</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) interfaces have been studied
                            extensively over the last few decades, with a
                            growing number of user-based experiments. In this
                            paper, we systematically review 10 years of the most
                            influential AR user studies, from 2005 to 2014. A
                            total of 291 papers with 369 individual user studies
                            have been reviewed and classified based on their
                            application areas. The primary contribution of the
                            review is to present the broad landscape of
                            user-based AR research, and to provide a high-level
                            view of how that landscape has changed. We summarize
                            the high-level contributions from each category of
                            papers, and present examples of the most influential
                            user studies. We also identify areas where there
                            have been few user studies, and opportunities for
                            future research. Among other things, we find that
                            there is a growing trend toward handheld AR user
                            studies, and that most studies are conducted in
                            laboratory settings and do not involve pilot
                            testing. This research will be useful for AR
                            researchers who want to follow best practices in
                            designing their own AR user studies.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="239"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.23.05-pm.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.23.05-pm.png         605w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.23.05-pm-300x239.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.23.05-pm-184x146.png 184w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.23.05-pm-50x40.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/Screen-Shot-2018-07-11-at-3.23.05-pm-94x75.png    94w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            He who hesitates is lost (... in thoughts over a
                            robot)
                          </h5>
                          <small
                            >James Wen, Amanda Stewart, Mark Billinghurst,
                            Arindam Dey, Chad Tossell, Victor Finomore</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            James Wen, Amanda Stewart, Mark Billinghurst,
                            Arindam Dey, Chad Tossell, and Victor Finomore.
                            2018. He who hesitates is lost (...in thoughts over
                            a robot). In Proceedings of the Technology, Mind,
                            and Society (TechMindSociety '18). ACM, New York,
                            NY, USA, Article 43, 6 pages. DOI:
                            https://doi.org/10.1145/3183654.3183703
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Wen:2018:HHL:3183654.3183703,<br />
                            author = {Wen, James and Stewart, Amanda and
                            Billinghurst, Mark and Dey, Arindam and Tossell,
                            Chad and Finomore, Victor},<br />
                            title = {He Who Hesitates is Lost (...In Thoughts
                            over a Robot)},<br />
                            booktitle = {Proceedings of the Technology, Mind,
                            and Society},<br />
                            series = {TechMindSociety '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-5420-2},<br />
                            location = {Washington, DC, USA},<br />
                            pages = {43:1--43:6},<br />
                            articleno = {43},<br />
                            numpages = {6},<br />
                            url =
                            {http://doi.acm.org/10.1145/3183654.3183703},<br />
                            doi = {10.1145/3183654.3183703},<br />
                            acmid = {3183703},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {Anthropomorphism, Empathy, Human Machine
                            Team, Robotics, User Study},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://delivery.acm.org/10.1145/3190000/3183703/a43-wen.pdf?ip=130.220.8.189&id=3183703&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531309905_0fa85296a71f979937dfcc53d8648333"
                              target="_blank"
                              >http://delivery.acm.org/10.1145/3190000/3183703/a43-wen.pdf?ip=130.220.8.189&id=3183703&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E66BF2BADDFDC7DE0%2EF0418AF7A4636953%2E4D4702B0C3E38B35&__acm__=1531309905_0fa85296a71f979937dfcc53d8648333</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In a team, the strong bonds that can form between
                            teammates are often seen as critical for reaching
                            peak performance. This perspective may need to be
                            reconsidered, however, if some team members are
                            autonomous robots since establishing bonds with
                            fundamentally inanimate and expendable objects may
                            prove counterproductive. Previous work has measured
                            empathic responses towards robots as singular events
                            at the conclusion of experimental sessions. As
                            relationships extend over long periods of time,
                            sustained empathic behavior towards robots would be
                            of interest. In order to measure user actions that
                            may vary over time and are affected by empathy
                            towards a robot teammate, we created the TEAMMATE
                            simulation system. Our findings suggest that
                            inducing empathy through a back story narrative can
                            significantly change participant decisions in
                            actions that may have consequences for a robot
                            companion over time. The results of our study can
                            have strong implications for the overall performance
                            of human machine teams.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="216"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/10278_2017_2_Fig4_HTML-e1531289250459.gif"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A hybrid 2D/3D user Interface for radiological
                            diagnosis
                          </h5>
                          <small
                            >Veera Bhadra Harish Mandalika, Alexander I
                            Chernoglazov, Mark Billinghurst, Christoph Bartneck,
                            Michael A Hurrell, Niels de Ruiter, Anthony PH
                            Butler, Philip H Butler</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            A hybrid 2D/3D user Interface for radiological
                            diagnosis Veera Bhadra Harish Mandalika, Alexander I
                            Chernoglazov, Mark Billinghurst, Christoph Bartneck,
                            Michael A Hurrell, Niels de Ruiter, Anthony PH
                            Butler, Philip H ButlerJournal of digital imaging 31
                            (1), 56-73
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @Article{Mandalika2018,<br />
                            author="Mandalika, Veera Bhadra Harish<br />
                            and Chernoglazov, Alexander I.<br />
                            and Billinghurst, Mark<br />
                            and Bartneck, Christoph<br />
                            and Hurrell, Michael A.<br />
                            and Ruiter, Niels de<br />
                            and Butler, Anthony P. H.<br />
                            and Butler, Philip H.",<br />
                            title="A Hybrid 2D/3D User Interface for
                            Radiological Diagnosis",<br />
                            journal="Journal of Digital Imaging",<br />
                            year="2018",<br />
                            month="Feb",<br />
                            day="01",<br />
                            volume="31",<br />
                            number="1",<br />
                            pages="56--73",<br />
                            abstract="This paper presents a novel 2D/3D desktop
                            virtual reality hybrid user interface for radiology
                            that focuses on improving 3D manipulation required
                            in some diagnostic tasks. An evaluation of our
                            system revealed that our hybrid interface is more
                            efficient for novice users and more accurate for
                            both novice and experienced users when compared to
                            traditional 2D only interfaces. This is a
                            significant finding because it indicates, as the
                            techniques mature, that hybrid interfaces can
                            provide significant benefit to image evaluation. Our
                            hybrid system combines a zSpace stereoscopic display
                            with 2D displays, and mouse and keyboard input. It
                            allows the use of 2D and 3D components
                            interchangeably, or simultaneously. The system was
                            evaluated against a 2D only interface with a user
                            study that involved performing a scoliosis diagnosis
                            task. There were two user groups: medical students
                            and radiology residents. We found improvements in
                            completion time for medical students, and in
                            accuracy for both groups. In particular, the
                            accuracy of medical students improved to match that
                            of the residents.",<br />
                            issn="1618-727X",<br />
                            doi="10.1007/s10278-017-0002-6",<br />
                            url="https://doi.org/10.1007/s10278-017-0002-6"<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10278-017-0002-6#citeas"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10278-017-0002-6#citeas</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper presents a novel 2D/3D desktop virtual
                            reality hybrid user interface for radiology that
                            focuses on improving 3D manipulation required in
                            some diagnostic tasks. An evaluation of our system
                            revealed that our hybrid interface is more efficient
                            for novice users and more accurate for both novice
                            and experienced users when compared to traditional
                            2D only interfaces. This is a significant finding
                            because it indicates, as the techniques mature, that
                            hybrid interfaces can provide significant benefit to
                            image evaluation. Our hybrid system combines a
                            zSpace stereoscopic display with 2D displays, and
                            mouse and keyboard input. It allows the use of 2D
                            and 3D components interchangeably, or
                            simultaneously. The system was evaluated against a
                            2D only interface with a user study that involved
                            performing a scoliosis diagnosis task. There were
                            two user groups: medical students and radiology
                            residents. We found improvements in completion time
                            for medical students, and in accuracy for both
                            groups. In particular, the accuracy of medical
                            students improved to match that of the residents.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="251"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/10606_2018_9324_Fig2_HTML-e1531289549648.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/10606_2018_9324_Fig2_HTML-e1531289549648.png         388w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/10606_2018_9324_Fig2_HTML-e1531289549648-300x251.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/10606_2018_9324_Fig2_HTML-e1531289549648-174x146.png 174w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/10606_2018_9324_Fig2_HTML-e1531289549648-50x42.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/10606_2018_9324_Fig2_HTML-e1531289549648-90x75.png    90w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Effect of Collaboration Styles and View
                            Independence on Video-Mediated Remote Collaboration
                          </h5>
                          <small
                            >Seungwon Kim, Mark Billinghurst, Gun Lee</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kim, S., Billinghurst, M., & Lee, G. (2018). The
                            Effect of Collaboration Styles and View Independence
                            on Video-Mediated Remote Collaboration. Computer
                            Supported Cooperative Work (CSCW), 1-39.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @Article{Kim2018,<br />
                            author="Kim, Seungwon<br />
                            and Billinghurst, Mark<br />
                            and Lee, Gun",<br />
                            title="The Effect of Collaboration Styles and View
                            Independence on Video-Mediated Remote
                            Collaboration",<br />
                            journal="Computer Supported Cooperative Work
                            (CSCW)",<br />
                            year="2018",<br />
                            month="Jun",<br />
                            day="02",<br />
                            abstract="This paper investigates how different
                            collaboration styles and view independence affect
                            remote collaboration. Our remote collaboration
                            system shares a live video of a local user's
                            real-world task space with a remote user. The remote
                            user can have an independent view or a dependent
                            view of a shared real-world object manipulation task
                            and can draw virtual annotations onto the real-world
                            objects as a visual communication cue. With the
                            system, we investigated two different collaboration
                            styles; (1) remote expert collaboration where a
                            remote user has the solution and gives instructions
                            to a local partner and (2) mutual collaboration
                            where neither user has a solution but both remote
                            and local users share ideas and discuss ways to
                            solve the real-world task. In the user study, the
                            remote expert collaboration showed a number of
                            benefits over the mutual collaboration. With the
                            remote expert collaboration, participants had better
                            communication from the remote user to the local
                            user, more aligned focus between participants, and
                            the remote participants' feeling of enjoyment and
                            togetherness. However, the benefits were not always
                            apparent at the local participants' end, especially
                            with measures of enjoyment and togetherness. The
                            independent view also had several benefits over the
                            dependent view, such as allowing remote participants
                            to freely navigate around the workspace while having
                            a wider fully zoomed-out view. The benefits of the
                            independent view were more prominent in the mutual
                            collaboration than in the remote expert
                            collaboration, especially in enabling the remote
                            participants to see the workspace.",<br />
                            issn="1573-7551",<br />
                            doi="10.1007/s10606-018-9324-2",<br />
                            url="https://doi.org/10.1007/s10606-018-9324-2"<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10606-018-9324-2"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10606-018-9324-2</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper investigates how different collaboration
                            styles and view independence affect remote
                            collaboration. Our remote collaboration system
                            shares a live video of a local user’s real-world
                            task space with a remote user. The remote user can
                            have an independent view or a dependent view of a
                            shared real-world object manipulation task and can
                            draw virtual annotations onto the real-world objects
                            as a visual communication cue. With the system, we
                            investigated two different collaboration styles; (1)
                            remote expert collaboration where a remote user has
                            the solution and gives instructions to a local
                            partner and (2) mutual collaboration where neither
                            user has a solution but both remote and local users
                            share ideas and discuss ways to solve the real-world
                            task. In the user study, the remote expert
                            collaboration showed a number of benefits over the
                            mutual collaboration. With the remote expert
                            collaboration, participants had better communication
                            from the remote user to the local user, more aligned
                            focus between participants, and the remote
                            participants’ feeling of enjoyment and togetherness.
                            However, the benefits were not always apparent at
                            the local participants’ end, especially with
                            measures of enjoyment and togetherness. The
                            independent view also had several benefits over the
                            dependent view, such as allowing remote participants
                            to freely navigate around the workspace while having
                            a wider fully zoomed-out view. The benefits of the
                            independent view were more prominent in the mutual
                            collaboration than in the remote expert
                            collaboration, especially in enabling the remote
                            participants to see the workspace.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="175"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/access-gagraphic-2801028-e1531289908653.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/access-gagraphic-2801028-e1531289908653.jpg         506w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/access-gagraphic-2801028-e1531289908653-300x175.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/access-gagraphic-2801028-e1531289908653-250x146.jpg 250w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/access-gagraphic-2801028-e1531289908653-50x29.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/access-gagraphic-2801028-e1531289908653-129x75.jpg  129w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Robust tracking through the design of high quality
                            fiducial markers: An optimization tool for ARToolKit
                          </h5>
                          <small
                            >Dawar Khan, Sehat Ullah, Dong-Ming Yan, Ihsan
                            Rabbi, Paul Richard, Thuong Hoang, Mark
                            Billinghurst, Xiaopeng Zhang</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            D. Khan et al., "Robust Tracking Through the Design
                            of High Quality Fiducial Markers: An Optimization
                            Tool for ARToolKit," in IEEE Access, vol. 6, pp.
                            22421-22433, 2018. doi: 10.1109/ACCESS.2018.2801028
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @ARTICLE{8287815, <br />
                            author={D. Khan and S. Ullah and D. M. Yan and I.
                            Rabbi and P. Richard and T. Hoang and M.
                            Billinghurst and X. Zhang}, <br />
                            journal={IEEE Access}, <br />
                            title={Robust Tracking Through the Design of High
                            Quality Fiducial Markers: An Optimization Tool for
                            ARToolKit}, <br />
                            year={2018}, <br />
                            volume={6}, <br />
                            number={}, <br />
                            pages={22421-22433}, <br />
                            keywords={augmented reality;image recognition;object
                            tracking;optical tracking;pose estimation;ARToolKit
                            markers;B:W;augmented reality applications;camera
                            tracking;edge sharpness;fiducial marker
                            optimizer;high quality fiducial markers;optimization
                            tool;pose estimation;robust tracking;specialized
                            image processing algorithms;Cameras;Complexity
                            theory;Fiducial
                            markers;Libraries;Robustness;Tools;ARToolKit;Fiducial
                            markers;augmented reality;marker tracking;robust
                            recognition}, <br />
                            doi={10.1109/ACCESS.2018.2801028}, <br />
                            ISSN={}, <br />
                            month={},}
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8287815&isnumber=8274985"
                              target="_blank"
                              >http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8287815&isnumber=8274985</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Fiducial markers are images or landmarks placed in
                            real environment, typically used for pose estimation
                            and camera tracking. Reliable fiducials are strongly
                            desired for many augmented reality (AR)
                            applications, but currently there is no systematic
                            method to design highly reliable fiducials. In this
                            paper, we present fiducial marker optimizer (FMO), a
                            tool to optimize the design attributes of ARToolKit
                            markers, including black to white (B:W) ratio, edge
                            sharpness, and information complexity, and to reduce
                            inter-marker confusion. For these operations, the
                            FMO provides a user friendly interface at the
                            front-end and specialized image processing
                            algorithms at the back-end. We tested manually
                            designed markers and FMO optimized markers in
                            ARToolKit and found that the latter were more
                            robust. The FMO will be used for designing highly
                            reliable fiducials in easy to use fashion. It will
                            improve the application's performance, where it is
                            used.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="171"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/mirror2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/mirror2.jpg         595w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/mirror2-300x171.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/mirror2-256x146.jpg 256w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/mirror2-50x29.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/mirror2-131x75.jpg  131w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            User Interface Agents for Guiding Interaction with
                            Augmented Virtual Mirrors
                          </h5>
                          <small
                            >Gun Lee, Omprakash Rudhru, Hye Sun Park, Ho Won
                            Kim, and Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gun Lee, Omprakash Rudhru, Hye Sun Park, Ho Won Kim,
                            and Mark Billinghurst. User Interface Agents for
                            Guiding Interaction with Augmented Virtual Mirrors.
                            In Proceedings of ICAT-EGVE 2017 - International
                            Conference on Artificial Reality and Telexistence
                            and Eurographics Symposium on Virtual Environments,
                            109-116. http://dx.doi.org/10.2312/egve.20171347
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings {egve.20171347,<br />
                            booktitle = {ICAT-EGVE 2017 - International
                            Conference on Artificial Reality and Telexistence
                            and Eurographics Symposium on Virtual
                            Environments},<br />
                            editor = {Robert W. Lindeman and Gerd Bruder and
                            Daisuke Iwai},<br />
                            title = {{User Interface Agents for Guiding
                            Interaction with Augmented Virtual Mirrors}},<br />
                            author = {Lee, Gun A. and Rudhru, Omprakash and
                            Park, Hye Sun and Kim, Ho Won and Billinghurst,
                            Mark},<br />
                            year = {2017},<br />
                            publisher = {The Eurographics Association},<br />
                            ISSN = {1727-530X},<br />
                            ISBN = {978-3-03868-038-3},<br />
                            DOI = {10.2312/egve.20171347}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://dx.doi.org/10.2312/egve.20171347"
                              target="_blank"
                              >http://dx.doi.org/10.2312/egve.20171347</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This research investigates using user interface (UI)
                            agents for guiding gesture based interaction with
                            Augmented Virtual Mirrors. Compared to prior work in
                            gesture interaction, where graphical symbols are
                            used for guiding user interaction, we propose using
                            UI agents. We explore two approaches for using UI
                            agents: 1) using a UI agent as a delayed cursor and
                            2) using a UI agent as an interactive button. We
                            conducted two user studies to evaluate the proposed
                            designs. The results from the user studies show that
                            UI agents are effective for guiding user
                            interactions in a similar way as a traditional
                            graphical user interface providing visual cues,
                            while they are useful in emotionally engaging with
                            users.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="208"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/image2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/image2.jpg         734w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/image2-300x208.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/image2-211x146.jpg 211w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/image2-50x35.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/image2-108x75.jpg  108w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Improving Collaboration in Augmented Video
                            Conference using Mutually Shared Gaze
                          </h5>
                          <small
                            >Gun Lee, Seungwon Kim, Youngho Lee, Arindam Dey,
                            Thammathip Piumsomboon, Mitchell Norman and Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gun Lee, Seungwon Kim, Youngho Lee, Arindam Dey,
                            Thammathip Piumsomboon, Mitchell Norman and Mark
                            Billinghurst. 2017. Improving Collaboration in
                            Augmented Video Conference using Mutually Shared
                            Gaze. In Proceedings of ICAT-EGVE 2017 -
                            International Conference on Artificial Reality and
                            Telexistence and Eurographics Symposium on Virtual
                            Environments, pp. 197-204.
                            http://dx.doi.org/10.2312/egve.20171359
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings {egve.20171359,<br />
                            booktitle = {ICAT-EGVE 2017 - International
                            Conference on Artificial Reality and Telexistence
                            and Eurographics Symposium on Virtual
                            Environments},<br />
                            editor = {Robert W. Lindeman and Gerd Bruder and
                            Daisuke Iwai},<br />
                            title = {{Improving Collaboration in Augmented Video
                            Conference using Mutually Shared Gaze}},<br />
                            author = {Lee, Gun A. and Kim, Seungwon and Lee,
                            Youngho and Dey, Arindam and Piumsomboon, Thammathip
                            and Norman, Mitchell and Billinghurst, Mark},<br />
                            year = {2017},<br />
                            publisher = {The Eurographics Association},<br />
                            ISSN = {1727-530X},<br />
                            ISBN = {978-3-03868-038-3},<br />
                            DOI = {10.2312/egve.20171359}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://dx.doi.org/10.2312/egve.20171359"
                              target="_blank"
                              >http://dx.doi.org/10.2312/egve.20171359</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            To improve remote collaboration in video
                            conferencing systems, researchers have been
                            investigating augmenting visual cues onto a shared
                            live video stream. In such systems, a person wearing
                            a head-mounted display (HMD) and camera can share
                            her view of the surrounding real-world with a remote
                            collaborator to receive assistance on a real-world
                            task. While this concept of augmented video
                            conferencing (AVC) has been actively investigated,
                            there has been little research on how sharing gaze
                            cues might affect the collaboration in video
                            conferencing. This paper investigates how sharing
                            gaze in both directions between a local worker and
                            remote helper in an AVC system affects the
                            collaboration and communication. Using a prototype
                            AVC system that shares the eye gaze of both users,
                            we conducted a user study that compares four
                            conditions with different combinations of eye gaze
                            sharing between the two users. The results showed
                            that sharing each other’s gaze significantly
                            improved collaboration and communication.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="233"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/setup.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup.jpg          1080w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-300x233.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-768x597.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-1024x796.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-188x146.jpg   188w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-50x39.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-96x75.jpg      96w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/setup-960x750.jpg   960w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploring Natural Eye-Gaze-Based Interaction for
                            Immersive Virtual Reality
                          </h5>
                          <small
                            >Thammathip Piumsomboon, Gun Lee, Robert W. Lindeman
                            and Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Thammathip Piumsomboon, Gun Lee, Robert W. Lindeman
                            and Mark Billinghurst. 2017. Exploring Natural
                            Eye-Gaze-Based Interaction for Immersive Virtual
                            Reality. In 2017 IEEE Symposium on 3D User
                            Interfaces (3DUI), pp. 36-39.
                            https://doi.org/10.1109/3DUI.2017.7893315
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @INPROCEEDINGS{7893315, <br />
                            author={T. Piumsomboon and G. Lee and R. W. Lindeman
                            and M. Billinghurst}, <br />
                            booktitle={2017 IEEE Symposium on 3D User Interfaces
                            (3DUI)}, <br />
                            title={Exploring natural eye-gaze-based interaction
                            for immersive virtual reality}, <br />
                            year={2017}, <br />
                            volume={}, <br />
                            number={}, <br />
                            pages={36-39}, <br />
                            keywords={gaze tracking;gesture recognition;helmet
                            mounted displays;virtual reality;Duo-Reticles;Nod
                            and Roll;Radial Pursuit;cluttered-object
                            selection;eye tracking technology;eye-gaze
                            selection;head-gesture-based
                            interaction;head-mounted display;immersive virtual
                            reality;inertial reticles;natural eye
                            movements;natural eye-gaze-based interaction;smooth
                            pursuit;vestibulo-ocular reflex;Electronic
                            mail;Erbium;Gaze tracking;Painting;Portable
                            computers;Resists;Two dimensional displays;H.5.2
                            [Information Interfaces and Presentation]: User
                            Interfaces—Interaction styles}, <br />
                            doi={10.1109/3DUI.2017.7893315}, <br />
                            ISSN={}, <br />
                            month={March},}
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1109/3DUI.2017.7893315"
                              target="_blank"
                              >https://doi.org/10.1109/3DUI.2017.7893315</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Eye tracking technology in a head-mounted display
                            has undergone rapid advancement in recent years,
                            making it possible for researchers to explore new
                            interaction techniques using natural eye movements.
                            This paper explores three novel eye-gaze-based
                            interaction techniques: (1) Duo-Reticles, eye-gaze
                            selection based on eye-gaze and inertial reticles,
                            (2) Radial Pursuit, cluttered-object selection that
                            takes advantage of smooth pursuit, and (3) Nod and
                            Roll, head-gesture-based interaction based on the
                            vestibulo-ocular reflex. In an initial user study,
                            we compare each technique against a baseline
                            condition in a scenario that demonstrates its
                            strengths and weaknesses.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="207"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/dyswis.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/dyswis.jpg         488w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/dyswis-300x207.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/dyswis-211x146.jpg 211w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/dyswis-50x35.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/dyswis-109x75.jpg  109w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Do You See What I See? The Effect of Gaze Tracking
                            on Task Space Remote Collaboration
                          </h5>
                          <small
                            >Kunal Gupta, Gun A. Lee and Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kunal Gupta, Gun A. Lee and Mark Billinghurst. 2016.
                            Do You See What I See? The Effect of Gaze Tracking
                            on Task Space Remote Collaboration. IEEE
                            Transactions on Visualization and Computer Graphics
                            Vol.22, No.11, pp.2413-2422.
                            https://doi.org/10.1109/TVCG.2016.2593778
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @ARTICLE{7523400, <br />
                            author={K. Gupta and G. A. Lee and M. Billinghurst},
                            <br />
                            journal={IEEE Transactions on Visualization and
                            Computer Graphics}, <br />
                            title={Do You See What I See? The Effect of Gaze
                            Tracking on Task Space Remote Collaboration}, <br />
                            year={2016}, <br />
                            volume={22}, <br />
                            number={11}, <br />
                            pages={2413-2422}, <br />
                            keywords={cameras;gaze tracking;helmet mounted
                            displays;eye-tracking camera;gaze
                            tracking;head-mounted camera;head-mounted
                            display;remote collaboration;task space remote
                            collaboration;virtual gaze information;virtual
                            pointer;wearable
                            interface;Cameras;Collaboration;Computers;Gaze
                            tracking;Head;Prototypes;Teleconferencing;Computer
                            conferencing;Computer-supported collaborative
                            work;teleconferencing;videoconferencing}, <br />
                            doi={10.1109/TVCG.2016.2593778}, <br />
                            ISSN={1077-2626}, <br />
                            month={Nov},}
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1109/TVCG.2016.2593778"
                              target="_blank"
                              >https://doi.org/10.1109/TVCG.2016.2593778</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present results from research exploring the
                            effect of sharing virtual gaze and pointing cues in
                            a wearable interface for remote collaboration. A
                            local worker wears a Head-mounted Camera,
                            Eye-tracking camera and a Head-Mounted Display and
                            shares video and virtual gaze information with a
                            remote helper. The remote helper can provide
                            feedback using a virtual pointer on the live video
                            view. The prototype system was evaluated with a
                            formal user study. Comparing four conditions, (1)
                            NONE (no cue), (2) POINTER, (3) EYE-TRACKER and (4)
                            BOTH (both pointer and eye-tracker cues), we
                            observed that the task completion performance was
                            best in the BOTH condition with a significant
                            difference of POINTER and EYETRACKER individually.
                            The use of eye-tracking and a pointer also
                            significantly improved the co-presence felt between
                            the users. We discuss the implications of this
                            research and the limitations of the developed system
                            that could be improved in further work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="188"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2.jpg         594w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-300x188.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-233x146.jpg 233w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-50x31.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses2-120x75.jpg  120w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Remote Collaboration System with Empathy Glasses
                          </h5>
                          <small></small>
                          <p style="font-style: italic; font-size: 12px">
                            Y. Lee, K. Masai, K. Kunze, M. Sugimoto and M.
                            Billinghurst. 2016. A Remote Collaboration System
                            with Empathy Glasses. 2016 IEEE International
                            Symposium on Mixed and Augmented Reality
                            (ISMAR-Adjunct)(ISMARW), Merida, pp. 342-343.
                            http://doi.ieeecomputersociety.org/10.1109/ISMAR-Adjunct.2016.0112
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @INPROCEEDINGS{7836533,<br />
                            author = {Y. Lee and K. Masai and K. Kunze and M.
                            Sugimoto and M. Billinghurst},<br />
                            booktitle = {2016 IEEE International Symposium on
                            Mixed and Augmented Reality
                            (ISMAR-Adjunct)(ISMARW)},<br />
                            title = {A Remote Collaboration System with Empathy
                            Glasses},<br />
                            year = {2017},<br />
                            volume = {00},<br />
                            number = {},<br />
                            pages = {342-343},<br />
                            keywords={Collaboration;Glass;Heart rate;Biomedical
                            monitoring;Cameras;Hardware;Computers},<br />
                            doi = {10.1109/ISMAR-Adjunct.2016.0112},<br />
                            url =
                            {doi.ieeecomputersociety.org/10.1109/ISMAR-Adjunct.2016.0112},<br />
                            ISSN = {},<br />
                            month={Sept.}<br />
                            }<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://doi.ieeecomputersociety.org/10.1109/ISMAR-Adjunct.2016.0112"
                              target="_blank"
                              >http://doi.ieeecomputersociety.org/10.1109/ISMAR-Adjunct.2016.0112</a
                            ><br />Video:
                            <a
                              href="https://www.youtube.com/watch?v=CdgWVDbMwp4"
                              target="_blank"
                              >https://www.youtube.com/watch?v=CdgWVDbMwp4</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we describe a demonstration of remote
                            collaboration system using Empathy glasses. Using
                            our system, a local worker can share a view of their
                            environment with a remote helper, as well as their
                            gaze, facial expressions, and physiological signals.
                            The remote user can send back visual cues via a
                            see-through head mounted display to help them
                            perform better on a real world task. The system also
                            provides some indication of the remote users face
                            expression using face tracking technology.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="181"
                              src="https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses1.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses1.jpg         615w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses1-300x181.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses1-241x146.jpg 241w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses1-50x30.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2018/07/empathy-glasses1-124x75.jpg  124w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Empathy Glasses
                          </h5>
                          <small
                            >Katsutoshi Masai, Kai Kunze, Maki Sugimoto, and
                            Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Katsutoshi Masai, Kai Kunze, Maki Sugimoto, and Mark
                            Billinghurst. 2016. Empathy Glasses. In Proceedings
                            of the 2016 CHI Conference Extended Abstracts on
                            Human Factors in Computing Systems (CHI EA '16).
                            ACM, New York, NY, USA, 1257-1263.
                            https://doi.org/10.1145/2851581.2892370
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{Masai:2016:EG:2851581.2892370,<br />
                            author = {Masai, Katsutoshi and Kunze, Kai and
                            sugimoto, Maki and Billinghurst, Mark},<br />
                            title = {Empathy Glasses},<br />
                            booktitle = {Proceedings of the 2016 CHI Conference
                            Extended Abstracts on Human Factors in Computing
                            Systems},<br />
                            series = {CHI EA '16},<br />
                            year = {2016},<br />
                            isbn = {978-1-4503-4082-3},<br />
                            location = {San Jose, California, USA},<br />
                            pages = {1257--1263},<br />
                            numpages = {7},<br />
                            url =
                            {http://doi.acm.org/10.1145/2851581.2892370},<br />
                            doi = {10.1145/2851581.2892370},<br />
                            acmid = {2892370},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {emotional interface, facial expression,
                            remote collaboration, wearables},<br />
                            }<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1145/2851581.2892370"
                              target="_blank"
                              >https://doi.org/10.1145/2851581.2892370</a
                            ><br />Video:
                            <a
                              href="https://www.youtube.com/watch?v=CdgWVDbMwp4"
                              target="_blank"
                              >https://www.youtube.com/watch?v=CdgWVDbMwp4</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we describe Empathy Glasses, a head
                            worn prototype designed to create an empathic
                            connection between remote collaborators. The main
                            novelty of our system is that it is the first to
                            combine the following technologies together: (1)
                            wearable facial expression capture hardware, (2) eye
                            tracking, (3) a head worn camera, and (4) a
                            see-through head mounted display, with a focus on
                            remote collaboration. Using the system, a local user
                            can send their information and a view of their
                            environment to a remote helper who can send back
                            visual cues on the local user's see-through display
                            to help them perform a real world task. A pilot user
                            study was conducted to explore how effective the
                            Empathy Glasses were at supporting remote
                            collaboration. We describe the implications that can
                            be drawn from this user study.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="205"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018.jpg         906w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018-300x205.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018-768x526.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018-213x146.jpg 213w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018-50x34.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/TeoOzCHI2018-110x75.jpg  110w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Hand gestures and visual annotation in live 360
                            panorama-based mixed reality remote collaboration
                          </h5>
                          <small
                            >Theophilus Teo, Gun A. Lee, Mark Billinghurst, Matt
                            Adcock</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Theophilus Teo, Gun A. Lee, Mark Billinghurst, and
                            Matt Adcock. 2018. Hand gestures and visual
                            annotation in live 360 panorama-based mixed reality
                            remote collaboration. In Proceedings of the 30th
                            Australian Conference on Computer-Human Interaction
                            (OzCHI '18). ACM, New York, NY, USA, 406-410. DOI:
                            https://doi.org/10.1145/3292147.3292200
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            BibTeX | EndNote | ACM Ref<br />
                            @inproceedings{Teo:2018:HGV:3292147.3292200,<br />
                            author = {Teo, Theophilus and Lee, Gun A. and
                            Billinghurst, Mark and Adcock, Matt},<br />
                            title = {Hand Gestures and Visual Annotation in Live
                            360 Panorama-based Mixed Reality Remote
                            Collaboration},<br />
                            booktitle = {Proceedings of the 30th Australian
                            Conference on Computer-Human Interaction},<br />
                            series = {OzCHI '18},<br />
                            year = {2018},<br />
                            isbn = {978-1-4503-6188-0},<br />
                            location = {Melbourne, Australia},<br />
                            pages = {406--410},<br />
                            numpages = {5},<br />
                            url =
                            {http://doi.acm.org/10.1145/3292147.3292200},<br />
                            doi = {10.1145/3292147.3292200},<br />
                            acmid = {3292200},<br />
                            publisher = {ACM},<br />
                            address = {New York, NY, USA},<br />
                            keywords = {gesture communication, mixed reality,
                            remote collaboration},<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?doid=3292147.3292200"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?doid=3292147.3292200</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we investigate hand gestures and
                            visual annotation cues overlaid in a live 360
                            panorama-based Mixed Reality remote collaboration.
                            The prototype system captures 360 live panorama
                            video of the surroundings of a local user and shares
                            it with another person in a remote location. The two
                            users wearing Augmented Reality or Virtual Reality
                            head-mounted displays can collaborate using
                            augmented visual communication cues such as virtual
                            hand gestures, ray pointing, and drawing
                            annotations. Our preliminary user evaluation
                            comparing these cues found that using visual
                            annotation cues (ray pointing and drawing
                            annotation) helps local users perform collaborative
                            tasks faster, easier, making less errors and with
                            better understanding, compared to using only virtual
                            hand gestures.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="191"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage.jpg          1592w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage-300x191.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage-768x489.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage-1024x652.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage-229x146.jpg   229w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage-50x32.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/10/EminCogImage-118x75.jpg    118w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Assessing the Relationship between Cognitive Load
                            and the Usability of a Mobile Augmented Reality
                            Tutorial System: A Study of Gender Effects
                          </h5>
                          <small>E Ibili, M Billinghurst </small>
                          <p style="font-style: italic; font-size: 12px">
                            Ibili, E., & Billinghurst, M. (2019). Assessing the
                            Relationship between Cognitive Load and the
                            Usability of a Mobile Augmented Reality Tutorial
                            System: A Study of Gender Effects. International
                            Journal of Assessment Tools in Education, 6(3),
                            378-395.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ibili2019assessing,<br />
                            title={Assessing the Relationship between Cognitive
                            Load and the Usability of a Mobile Augmented Reality
                            Tutorial System: A Study of Gender Effects},<br />
                            author={Ibili, Emin and Billinghurst, Mark},<br />
                            journal={International Journal of Assessment Tools
                            in Education},<br />
                            volume={6},<br />
                            number={3},<br />
                            pages={378--395},<br />
                            year={2019}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://submit.ijate.net/tr/download/article-file/766736"
                              target="_blank"
                              >http://submit.ijate.net/tr/download/article-file/766736</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this study, the relationship between the
                            usability of a mobile Augmented Reality (AR)
                            tutorial system and cognitive load was examined. In
                            this context, the relationship between perceived
                            usefulness, the perceived ease of use, and the
                            perceived natural interaction factors and intrinsic,
                            extraneous, germane cognitive load were
                            investigated. In addition, the effect of gender on
                            this relationship was investigated. The research
                            results show that there was a strong relationship
                            between the perceived ease of use and the extraneous
                            load in males, and there was a strong relationship
                            between the perceived usefulness and the intrinsic
                            load in females. Both the perceived usefulness and
                            the perceived ease of use had a strong relationship
                            with the germane cognitive load. Moreover, the
                            perceived natural interaction had a strong
                            relationship with the perceived usefulness in
                            females and the perceived ease of use in males. This
                            research will provide significant clues to AR
                            software developers and researchers to help reduce
                            or control cognitive load in the development of
                            AR-based instructional software.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="212"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang.jpg          1654w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang-300x212.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang-768x544.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang-1024x725.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang-206x146.jpg   206w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang-50x35.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Huang-106x75.jpg    106w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Sharing hand gesture and sketch cues in remote
                            collaboration
                          </h5>
                          <small
                            >W. Huang, S. Kim, M. Billinghurst, L. Alem</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Huang, W., Kim, S., Billinghurst, M., & Alem, L.
                            (2019). Sharing hand gesture and sketch cues in
                            remote collaboration. Journal of Visual
                            Communication and Image Representation, 58, 428-438.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{huang2019sharing,<br />
                            title={Sharing hand gesture and sketch cues in
                            remote collaboration},<br />
                            author={Huang, Weidong and Kim, Seungwon and
                            Billinghurst, Mark and Alem, Leila},<br />
                            journal={Journal of Visual Communication and Image
                            Representation},<br />
                            volume={58},<br />
                            pages={428--438},<br />
                            year={2019},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S1047320318303365"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S1047320318303365</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Many systems have been developed to support remote
                            guidance, where a local worker manipulates objects
                            under guidance of a remote expert helper. These
                            systems typically use speech and visual cues between
                            the local worker and the remote helper, where the
                            visual cues could be pointers, hand gestures, or
                            sketches. However, the effects of combining visual
                            cues together in remote collaboration has not been
                            fully explored. We conducted a user study comparing
                            remote collaboration with an interface that combined
                            hand gestures and sketching (the HandsInTouch
                            interface) to one that only used hand gestures, when
                            solving two tasks; Lego assembly and repairing a
                            laptop. In the user study, we found that (1) adding
                            sketch cues improved the task completion time, only
                            with the repairing task as this had complex object
                            manipulation but (2) using gesture and sketching
                            together created a higher task load for the user.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="205"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands.jpg          1154w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands-300x205.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands-768x524.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands-1024x699.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands-214x146.jpg   214w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands-50x34.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-2.5Dhands-110x75.jpg    110w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            2.5 DHANDS: a gesture-based MR remote collaborative
                            platform
                          </h5>
                          <small
                            >Wang, P., Zhang, S., Bai, X., Billinghurst, M., He,
                            W., Sun, M</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, P., Zhang, S., Bai, X., Billinghurst, M., He,
                            W., Sun, M., ... & Ji, H. (2019). 2.5 DHANDS: a
                            gesture-based MR remote collaborative platform. The
                            International Journal of Advanced Manufacturing
                            Technology, 102(5-8), 1339-1353.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{wang20192,<br />
                            title={2.5 DHANDS: a gesture-based MR remote
                            collaborative platform},<br />
                            author={Wang, Peng and Zhang, Shusheng and Bai,
                            Xiaoliang and Billinghurst, Mark and He, Weiping and
                            Sun, Mengmeng and Chen, Yongxing and Lv, Hao and Ji,
                            Hongyu},<br />
                            journal={The International Journal of Advanced
                            Manufacturing Technology},<br />
                            volume={102},<br />
                            number={5-8},<br />
                            pages={1339--1353},<br />
                            year={2019},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/content/pdf/10.1007/s00170-018-03237-1.pdf"
                              target="_blank"
                              >https://link.springer.com/content/pdf/10.1007/s00170-018-03237-1.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Current remote collaborative systems in
                            manufacturing are mainly based on video-conferencing
                            technology. Their primary aim is to transmit
                            manufacturing process knowledge between remote
                            experts and local workers. However, it does not
                            provide the experts with the same hands-on
                            experience as when synergistically working on site
                            in person. The mixed reality (MR) and increasing
                            networking performances have the capacity to enhance
                            the experience and communication between
                            collaborators in geographically distributed
                            locations. In this paper, therefore, we propose a
                            new gesture-based remote collaborative platform
                            using MR technology that enables a remote expert to
                            collaborate with local workers on physical tasks.
                            Besides, we concentrate on collaborative remote
                            assembly as an illustrative use case. The key
                            advantage compared to other remote collaborative MR
                            interfaces is that it projects the remote expert’s
                            gestures into the real worksite to improve the
                            performance, co-presence awareness, and user
                            collaboration experience. We aim to study the
                            effects of sharing the remote expert’s gestures in
                            remote collaboration using a projector-based MR
                            system in manufacturing. Furthermore, we show the
                            capabilities of our framework on a prototype
                            consisting of a VR HMD, Leap Motion, and a
                            projector. The prototype system was evaluated with a
                            pilot study comparing with the POINTER (adding AR
                            annotations on the task space view through the
                            mouse), which is the most popular method used to
                            augment remote collaboration at present. The
                            assessment adopts the following aspects: the
                            performance, user’s satisfaction, and the
                            user-perceived collaboration quality in terms of the
                            interaction and cooperation. Our results demonstrate
                            a clear difference between the POINTER and 2.5DHANDS
                            interface in the performance time. Additionally, the
                            2.5DHANDS interface was statistically significantly
                            higher than the POINTER interface in terms of the
                            awareness of user’s attention, manipulation,
                            self-confidence, and co-presence.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="237"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR.jpg          1554w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR-300x237.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR-768x607.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR-1024x809.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR-185x146.jpg   185w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR-50x40.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-COVAR-95x75.jpg      95w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The effects of sharing awareness cues in
                            collaborative mixed reality
                          </h5>
                          <small
                            >Piumsomboon, T., Dey, A., Ens, B., Lee, G., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Piumsomboon, T., Dey, A., Ens, B., Lee, G., &
                            Billinghurst, M. (2019). The effects of sharing
                            awareness cues in collaborative mixed reality.
                            Front. Rob, 6(5).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{piumsomboon2019effects,<br />
                            title={The effects of sharing awareness cues in
                            collaborative mixed reality},<br />
                            author={Piumsomboon, Thammathip and Dey, Arindam and
                            Ens, Barrett and Lee, Gun and Billinghurst,
                            Mark},<br />
                            year={2019}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.frontiersin.org/articles/10.3389/frobt.2019.00005/full"
                              target="_blank"
                              >https://www.frontiersin.org/articles/10.3389/frobt.2019.00005/full</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented and Virtual Reality provide unique
                            capabilities for Mixed Reality collaboration. This
                            paper explores how different combinations of virtual
                            awareness cues can provide users with valuable
                            information about their collaborator's attention and
                            actions. In a user study (<i>n</i> = 32, 16 pairs),
                            we compared different combinations of three cues:
                            Field-of-View (FoV) frustum, Eye-gaze ray, and
                            Head-gaze ray against a baseline condition showing
                            only virtual representations of each collaborator's
                            head and hands. Through a collaborative object
                            finding and placing task, the results showed that
                            awareness cues significantly improved user
                            performance, usability, and subjective preferences,
                            with the combination of the FoV frustum and the
                            Head-gaze ray being best. This work establishes the
                            feasibility of room-scale MR collaboration and the
                            utility of providing virtual awareness cues.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="181"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1.jpg         876w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1-300x181.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1-768x463.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1-242x146.jpg 242w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1-50x30.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Ens-1-124x75.jpg  124w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Revisiting collaboration through mixed reality: The
                            evolution of groupware
                          </h5>
                          <small
                            >Ens, B., Lanir, J., Tang, A., Bateman, S., Lee, G.,
                            Piumsomboon, T., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Ens, B., Lanir, J., Tang, A., Bateman, S., Lee, G.,
                            Piumsomboon, T., & Billinghurst, M. (2019).
                            Revisiting collaboration through mixed reality: The
                            evolution of groupware. International Journal of
                            Human-Computer Studies.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ens2019revisiting,<br />
                            title={Revisiting collaboration through mixed
                            reality: The evolution of groupware},<br />
                            author={Ens, Barrett and Lanir, Joel and Tang,
                            Anthony and Bateman, Scott and Lee, Gun and
                            Piumsomboon, Thammathip and Billinghurst, Mark},<br />
                            journal={International Journal of Human-Computer
                            Studies},<br />
                            year={2019},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://hcitang.org/papers/2019-ijhcs-revisiting-collaboration.pdf"
                              target="_blank"
                              >http://hcitang.org/papers/2019-ijhcs-revisiting-collaboration.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Collaborative Mixed Reality (MR) systems are at a
                            critical point in time as they are soon to become
                            more commonplace. However, MR technology has only
                            recently matured to the point where researchers can
                            focus deeply on the nuances of supporting
                            collaboration, rather than needing to focus on
                            creating the enabling technology. In parallel, but
                            largely independently, the field of Computer
                            Supported Cooperative Work (CSCW) has focused on the
                            fundamental concerns that underlie human
                            communication and collaboration over the past
                            30-plus years. Since MR research is now on the brink
                            of moving into the real world, we reflect on three
                            decades of collaborative MR research and try to
                            reconcile it with existing theory from CSCW, to help
                            position MR researchers to pursue fruitful
                            directions for their work. To do this, we review the
                            history of collaborative MR systems, investigating
                            how the common taxonomies and frameworks in CSCW and
                            MR research can be applied to existing work on
                            collaborative MR systems, exploring where they have
                            fallen behind, and look for new ways to describe
                            current trends. Through identifying emergent trends,
                            we suggest future directions for MR, and also find
                            where CSCW researchers can explore new theory that
                            more fully represents the future of working, playing
                            and being with others.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="258"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Deixis.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Deixis.jpg         594w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Deixis-300x258.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Deixis-170x146.jpg 170w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Deixis-50x43.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Deixis-87x75.jpg    87w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            WARPING DEIXIS: Distorting Gestures to Enhance
                            Collaboration
                          </h5>
                          <small
                            >Sousa, M., dos Anjos, R. K., Mendes, D.,
                            Billinghurst, M., & Jorge, J.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Sousa, M., dos Anjos, R. K., Mendes, D.,
                            Billinghurst, M., & Jorge, J. (2019, April). WARPING
                            DEIXIS: Distorting Gestures to Enhance
                            Collaboration. In Proceedings of the 2019 CHI
                            Conference on Human Factors in Computing Systems (p.
                            608). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{sousa2019warping,<br />
                            title={WARPING DEIXIS: Distorting Gestures to
                            Enhance Collaboration},<br />
                            author={Sousa, Maur{\'\i}cio and dos Anjos, Rafael
                            Kufner and Mendes, Daniel and Billinghurst, Mark and
                            Jorge, Joaquim},<br />
                            booktitle={Proceedings of the 2019 CHI Conference on
                            Human Factors in Computing Systems},<br />
                            pages={608},<br />
                            year={2019},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Mauricio_Sousa3/publication/330634716_Warping_Deixis_Distorting_Gestures_to_Enhance_Collaboration/links/5c4b1402299bf12be3e2f455/Warping-Deixis-Distorting-Gestures-to-Enhance-Collaboration.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Mauricio_Sousa3/publication/330634716_Warping_Deixis_Distorting_Gestures_to_Enhance_Collaboration/links/5c4b1402299bf12be3e2f455/Warping-Deixis-Distorting-Gestures-to-Enhance-Collaboration.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            When engaged in communication, people often rely on
                            pointing gestures to refer to out-of-reach content.
                            However, observers frequently misinterpret the
                            target of a pointing gesture. Previous research
                            suggests that to perform a pointing gesture, people
                            place the index finger on or close to a line
                            connecting the eye to the referent, while observers
                            interpret pointing gestures by extrapolating the
                            referent using a vector defined by the arm and index
                            finger. In this paper we present Warping Deixis, a
                            novel approach to improving the perception of
                            pointing gestures and facilitate communication in
                            collaborative Extended Reality environments. By
                            warping the virtual representation of the pointing
                            individual, we are able to match the pointing
                            expression to the observer’s perception. We
                            evaluated our approach in a colocated side by side
                            virtual reality scenario. Results suggest that our
                            approach is effective in improving the
                            interpretation of pointing gestures in shared
                            virtual environments.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="223"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis.jpg          1268w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis-300x223.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis-768x571.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis-1024x761.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis-197x146.jpg   197w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis-50x37.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-TableTennis-101x75.jpg    101w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Getting your game on: Using virtual reality to
                            improve real table tennis skills
                          </h5>
                          <small
                            >Michalski, S. C., Szpak, A., Saredakis, D., Ross,
                            T. J., Billinghurst, M., & Loetscher, T.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Michalski, S. C., Szpak, A., Saredakis, D., Ross, T.
                            J., Billinghurst, M., & Loetscher, T. (2019).
                            Getting your game on: Using virtual reality to
                            improve real table tennis skills. PloS one, 14(9).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{michalski2019getting,<br />
                            title={Getting your game on: Using virtual reality
                            to improve real table tennis skills},<br />
                            author={Michalski, Stefan Carlo and Szpak, Ancret
                            and Saredakis, Dimitrios and Ross, Tyler James and
                            Billinghurst, Mark and Loetscher, Tobias},<br />
                            journal={PloS one},<br />
                            volume={14},<br />
                            number={9},<br />
                            year={2019},<br />
                            publisher={Public Library of Science}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://journals.plos.org/plosone/article/file?type=printable&id=10.1371/journal.pone.0222351"
                              target="_blank"
                              >https://journals.plos.org/plosone/article/file?type=printable&id=10.1371/journal.pone.0222351</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <strong>Background: </strong>A key assumption of VR
                            training is that the learned skills and experiences
                            transfer to the real world. Yet, in certain
                            application areas, such as VR sports training, the
                            research testing this assumption is sparse.

                            <strong>Design:</strong> Real-world table tennis
                            performance was assessed using a mixed-model
                            analysis of variance. The analysis comprised a
                            between-subjects (VR training group vs control
                            group) and a within-subjects (pre- and
                            post-training) factor.

                            <strong>Method:</strong> Fifty-seven participants
                            (23 females) were either assigned to a VR training
                            group (n = 29) or no-training control group (n =
                            28). During VR training, participants were immersed
                            in competitive table tennis matches against an
                            artificial intelligence opponent. An expert table
                            tennis coach evaluated participants on real-world
                            table tennis playing before and after the training
                            phase. Blinded regarding participant’s group
                            assignment, the expert assessed participants’
                            backhand, forehand and serving on quantitative
                            aspects (e.g. count of rallies without errors) and
                            quality of skill aspects (e.g. technique and
                            consistency).

                            <strong>Results: </strong>VR training significantly
                            improved participants’ real-world table tennis
                            performance compared to a no-training control group
                            in both quantitative (p &lt; .001, Cohen’s d = 1.08)
                            and quality of skill assessments (p &lt; .001,
                            Cohen’s d = 1.10).

                            <strong>Conclusions:</strong> This study adds to a
                            sparse yet expanding literature, demonstrating
                            real-world skill transfer from Virtual Reality in an
                            athletic task
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="234"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant.jpg           1344w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-300x234.jpg    300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-768x600.jpg    768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-1024x800.jpg  1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-187x146.jpg    187w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-50x39.jpg       50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-96x75.jpg       96w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-1280x1000.jpg 1280w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Giant-960x750.jpg    960w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            On the Shoulder of the Giant: A Multi-Scale Mixed
                            Reality Collaboration with 360 Video Sharing and
                            Tangible Interaction
                          </h5>
                          <small
                            >Piumsomboon, T., Lee, G. A., Irlitti, A., Ens, B.,
                            Thomas, B. H., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Piumsomboon, T., Lee, G. A., Irlitti, A., Ens, B.,
                            Thomas, B. H., & Billinghurst, M. (2019, April). On
                            the Shoulder of the Giant: A Multi-Scale Mixed
                            Reality Collaboration with 360 Video Sharing and
                            Tangible Interaction. In Proceedings of the 2019 CHI
                            Conference on Human Factors in Computing Systems (p.
                            228). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{piumsomboon2019shoulder,<br />
                            title={On the Shoulder of the Giant: A Multi-Scale
                            Mixed Reality Collaboration with 360 Video Sharing
                            and Tangible Interaction},<br />
                            author={Piumsomboon, Thammathip and Lee, Gun A and
                            Irlitti, Andrew and Ens, Barrett and Thomas, Bruce H
                            and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 2019 CHI Conference on
                            Human Factors in Computing Systems},<br />
                            pages={228},<br />
                            year={2019},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/332742001_On_the_Shoulder_of_the_Giant_A_Multi-Scale_Mixed_Reality_Collaboration_with_360_Video_Sharing_and_Tangible_Interaction/links/5ce662aea6fdccc9ddc734b8/On-the-Shoulder-of-the-Giant-A-Multi-Scale-Mixed-Reality-Collaboration-with-360-Video-Sharing-and-Tangible-Interaction.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/332742001_On_the_Shoulder_of_the_Giant_A_Multi-Scale_Mixed_Reality_Collaboration_with_360_Video_Sharing_and_Tangible_Interaction/links/5ce662aea6fdccc9ddc734b8/On-the-Shoulder-of-the-Giant-A-Multi-Scale-Mixed-Reality-Collaboration-with-360-Video-Sharing-and-Tangible-Interaction.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We propose a multi-scale Mixed Reality (MR)
                            collaboration between the Giant, a local Augmented
                            Reality user, and the Miniature, a remote Virtual
                            Reality user, in Giant-Miniature Collaboration
                            (GMC). The Miniature is immersed in a 360-video
                            shared by the Giant who can physically manipulate
                            the Miniature through a tangible interface, a
                            combined 360-camera with a 6 DOF tracker. We
                            implemented a prototype system as a proof of concept
                            and conducted a user study (n=24) comprising of four
                            parts comparing: A) two types of virtual
                            representations, B) three levels of Miniature
                            control, C) three levels of 360-video view
                            dependencies, and D) four 360-camera placement
                            positions on the Giant. The results show users
                            prefer a shoulder mounted camera view, while a view
                            frustum with a complimentary avatar is a good
                            visualization for the Miniature virtual
                            representation. From the results, we give design
                            recommendations and demonstrate an example
                            Giant-Miniature Interaction.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="272"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Hayun.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Hayun.jpg         696w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Hayun-300x272.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Hayun-161x146.jpg 161w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Hayun-50x45.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Hayun-83x75.jpg    83w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Evaluating the Combination of Visual Communication
                            Cues for HMD-based Mixed Reality Remote
                            Collaboration
                          </h5>
                          <small
                            >Kim, S., Lee, G., Huang, W., Kim, H., Woo, W., &
                            Billinghurst, M.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Kim, S., Lee, G., Huang, W., Kim, H., Woo, W., &
                            Billinghurst, M. (2019, April). Evaluating the
                            Combination of Visual Communication Cues for
                            HMD-based Mixed Reality Remote Collaboration. In
                            Proceedings of the 2019 CHI Conference on Human
                            Factors in Computing Systems (p. 173). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{kim2019evaluating,<br />
                            title={Evaluating the Combination of Visual
                            Communication Cues for HMD-based Mixed Reality
                            Remote Collaboration},<br />
                            author={Kim, Seungwon and Lee, Gun and Huang,
                            Weidong and Kim, Hayun and Woo, Woontack and
                            Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 2019 CHI Conference on
                            Human Factors in Computing Systems},<br />
                            pages={173},<br />
                            year={2019},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3300403"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3300403</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Many researchers have studied various visual
                            communication cues (e.g. pointer, sketching, and
                            hand gesture) in Mixed Reality remote collaboration
                            systems for real-world tasks. However, the effect of
                            combining them has not been so well explored. We
                            studied the effect of these cues in four
                            combinations: hand only, hand + pointer, hand +
                            sketch, and hand + pointer + sketch, with three
                            problem tasks: Lego, Tangram, and Origami. The study
                            results showed that the participants completed the
                            task significantly faster and felt a significantly
                            higher level of usability when the sketch cue is
                            added to the hand gesture cue, but not with adding
                            the pointer cue. Participants also preferred the
                            combinations including hand and sketch cues over the
                            other combinations. However, using additional cues
                            (pointer or sketch) increased the perceived mental
                            effort and did not improve the feeling of
                            co-presence. We discuss the implications of these
                            results and future research directions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="253"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo.jpg          1060w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo-300x253.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo-768x648.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo-1024x864.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo-173x146.jpg   173w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo-50x42.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Teo-89x75.jpg      89w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Mixed Reality Remote Collaboration Combining 360
                            Video and 3D Reconstruction
                          </h5>
                          <small
                            >Teo, T., Lawrence, L., Lee, G. A., Billinghurst,
                            M., & Adcock, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Teo, T., Lawrence, L., Lee, G. A., Billinghurst, M.,
                            & Adcock, M. (2019, April). Mixed Reality Remote
                            Collaboration Combining 360 Video and 3D
                            Reconstruction. In Proceedings of the 2019 CHI
                            Conference on Human Factors in Computing Systems (p.
                            201). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{teo2019mixed,<br />
                            title={Mixed Reality Remote Collaboration Combining
                            360 Video and 3D Reconstruction},<br />
                            author={Teo, Theophilus and Lawrence, Louise and
                            Lee, Gun A and Billinghurst, Mark and Adcock,
                            Matt},<br />
                            booktitle={Proceedings of the 2019 CHI Conference on
                            Human Factors in Computing Systems},<br />
                            pages={201},<br />
                            year={2019},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3300431"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3300431</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Remote Collaboration using Virtual Reality (VR) and
                            Augmented Reality (AR) has recently become a popular
                            way for people from different places to work
                            together. Local workers can collaborate with remote
                            helpers by sharing 360-degree live video or 3D
                            virtual reconstruction of their surroundings.
                            However, each of these techniques has benefits and
                            drawbacks. In this paper we explore mixing 360 video
                            and 3D reconstruction together for remote
                            collaboration, by preserving benefits of both
                            systems while reducing drawbacks of each. We
                            developed a hybrid prototype and conducted user
                            study to compare benefits and problems of using 360
                            or 3D alone to clarify the needs for mixing the two,
                            and also to evaluate the prototype system. We found
                            participants performed significantly better on
                            collaborative search tasks in 360 and felt higher
                            social presence, yet 3D also showed potential to
                            complement. Participant feedback collected after
                            trying our hybrid system provided directions for
                            improvement.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="184"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha.jpg          1198w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha-300x184.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha-768x471.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha-1024x627.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha-238x146.jpg   238w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha-50x31.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Miha-122x75.jpg    122w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Using Augmented Reality with Speech Input for
                            Non-Native Children's Language Learning
                          </h5>
                          <small
                            >Dalim, C. S. C., Sunar, M. S., Dey, A., &
                            Billinghurst, M.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Dalim, C. S. C., Sunar, M. S., Dey, A., &
                            Billinghurst, M. (2019). Using Augmented Reality
                            with Speech Input for Non-Native Children's Language
                            Learning. International Journal of Human-Computer
                            Studies.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{dalim2019using,<br />
                            title={Using Augmented Reality with Speech Input for
                            Non-Native Children's Language Learning},<br />
                            author={Dalim, Che Samihah Che and Sunar, Mohd
                            Shahrizal and Dey, Arindam and Billinghurst,
                            Mark},<br />
                            journal={International Journal of Human-Computer
                            Studies},<br />
                            year={2019},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/abs/pii/S1071581918303161"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/abs/pii/S1071581918303161</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) offers an enhanced learning
                            environment which could potentially influence
                            children's experience and knowledge gain during the
                            language learning process. Teaching English or other
                            foreign languages to children with different native
                            language can be difficult and requires an effective
                            strategy to avoid boredom and detachment from the
                            learning activities. With the growing numbers of AR
                            education applications and the increasing
                            pervasiveness of speech recognition, we are keen to
                            understand how these technologies benefit non-native
                            young children in learning English. In this paper,
                            we explore children's experience in terms of
                            knowledge gain and enjoyment when learning through a
                            combination of AR and speech recognition
                            technologies. We developed a prototype AR interface
                            called TeachAR, and ran two experiments to
                            investigate how effective the combination of AR and
                            speech recognition was towards the learning of 1)
                            English terms for color and shapes, and 2) English
                            words for spatial relationships. We found
                            encouraging results by creating a novel teaching
                            strategy using these two technologies, not only in
                            terms of increase in knowledge gain and enjoyment
                            when compared with traditional strategy but also
                            enables young children to finish the certain task
                            faster and easier.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="182"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze.jpg         1020w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze-300x182.jpg  300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze-768x465.jpg  768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze-241x146.jpg  241w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze-50x30.jpg     50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-SeungwonGaze-124x75.jpg   124w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Sharing Emotion by Displaying a Partner Near the
                            Gaze Point in a Telepresence System
                          </h5>
                          <small
                            >Kim, S., Billinghurst, M., Lee, G., Norman, M.,
                            Huang, W., & He, J.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Kim, S., Billinghurst, M., Lee, G., Norman, M.,
                            Huang, W., & He, J. (2019, July). Sharing Emotion by
                            Displaying a Partner Near the Gaze Point in a
                            Telepresence System. In 2019 23rd International
                            Conference in Information Visualization–Part II (pp.
                            86-91). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{kim2019sharing,<br />
                            title={Sharing Emotion by Displaying a Partner Near
                            the Gaze Point in a Telepresence System},<br />
                            author={Kim, Seungwon and Billinghurst, Mark and
                            Lee, Gun and Norman, Mitchell and Huang, Weidong and
                            He, Jian},<br />
                            booktitle={2019 23rd International Conference in
                            Information Visualization--Part II},<br />
                            pages={86--91},<br />
                            year={2019},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8811937"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8811937</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we explore the effect of showing a
                            remote partner close to user gaze point in a
                            teleconferencing system. We implemented a gaze
                            following function in a teleconferencing system and
                            investigate if this improves the user's feeling of
                            emotional interdependence. We developed a prototype
                            system that shows a remote partner close to the
                            user's current gaze point and conducted a user study
                            comparing it to a condition displaying the partner
                            fixed in the corner of a screen. Our results showed
                            that showing a partner close to their gaze point
                            helped users feel a higher level of emotional
                            interdependence. In addition, we compared the effect
                            of our method between small and big displays, but
                            there was no significant difference in the users'
                            feeling of emotional interdependence even though the
                            big display was preferred.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="127"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo.jpg          1766w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo-300x127.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo-768x325.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo-1024x434.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo-260x110.jpg   260w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo-50x21.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEEVR-Teo-150x64.jpg    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Supporting Visual Annotation Cues in a Live 360
                            Panorama-based Mixed Reality Remote Collaboration
                          </h5>
                          <small
                            >Teo, T., Lee, G. A., Billinghurst, M., & Adcock,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Teo, T., Lee, G. A., Billinghurst, M., & Adcock, M.
                            (2019, March). Supporting Visual Annotation Cues in
                            a Live 360 Panorama-based Mixed Reality Remote
                            Collaboration. In 2019 IEEE Conference on Virtual
                            Reality and 3D User Interfaces (VR) (pp. 1187-1188).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{teo2019supporting,<br />
                            title={Supporting Visual Annotation Cues in a Live
                            360 Panorama-based Mixed Reality Remote
                            Collaboration},<br />
                            author={Teo, Theophilus and Lee, Gun A and
                            Billinghurst, Mark and Adcock, Matt},<br />
                            booktitle={2019 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={1187--1188},<br />
                            year={2019},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8798128"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8798128</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We propose enhancing live 360 panorama-based Mixed
                            Reality (MR) remote collaboration through supporting
                            visual annotation cues. Prior work on live 360
                            panorama-based collaboration used MR visualization
                            to overlay visual cues, such as view frames and
                            virtual hands, yet they were not registered onto the
                            shared physical workspace, hence had limitations in
                            accuracy for pointing or marking objects. Our
                            prototype system uses spatial mapping and tracking
                            feature of an Augmented Reality head-mounted display
                            to show visual annotation cues accurately registered
                            onto the physical environment. We describe the
                            design and implementation details of our prototype
                            system, and discuss on how such feature could help
                            improve MR remote collaboration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="250"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam.jpg         906w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam-300x250.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam-768x639.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam-175x146.jpg 175w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam-50x42.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Arindam-90x75.jpg    90w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploration of an EEG-Based Cognitively Adaptive
                            Training System in Virtual Reality
                          </h5>
                          <small
                            >Dey, A., Chatourn, A., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dey, A., Chatburn, A., & Billinghurst, M. (2019,
                            March). Exploration of an EEG-Based Cognitively
                            Adaptive Training System in Virtual Reality. In 2019
                            IEEE Conference on Virtual Reality and 3D User
                            Interfaces (VR) (pp. 220-226). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2019exploration,<br />
                            title={Exploration of an EEG-Based Cognitively
                            Adaptive Training System in Virtual Reality},<br />
                            author={Dey, Arindam and Chatburn, Alex and
                            Billinghurst, Mark},<br />
                            booktitle={2019 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={220--226},<br />
                            year={2019},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8797840"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8797840</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual Reality (VR) is effective in various
                            training scenarios across multiple domains, such as
                            education, health and defense. However, most of
                            those applications are not adaptive to the real-time
                            cognitive or subjectively experienced load placed on
                            the trainee. In this paper, we explore a cognitively
                            adaptive training system based on real-time
                            measurement of task related alpha activity in the
                            brain. This measurement was made by a 32-channel
                            mobile Electroencephalography (EEG) system, and was
                            used to adapt the task difficulty to an ideal level
                            which challenged our participants, and thus
                            theoretically induces the best level of performance
                            gains as a result of training. Our system required
                            participants to select target objects in VR and the
                            complexity of the task adapted to the alpha activity
                            in the brain. A total of 14 participants undertook
                            our training and completed 20 levels of increasing
                            complexity. Our study identified significant
                            differences in brain activity in response to
                            increasing levels of task complexity, but response
                            time did not alter as a function of task difficulty.
                            Collectively, we interpret this to indicate the
                            brain's ability to compensate for higher task load
                            without affecting behaviourally measured visuomotor
                            performance.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="225"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES.jpg          1310w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES-300x225.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES-768x577.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES-1024x769.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES-194x146.jpg   194w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES-50x38.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Amit-AES-100x75.jpg    100w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Binaural Spatialization over a Bone Conduction
                            Headset: The Perception of Elevation
                          </h5>
                          <small
                            >Barde, A., Lindeman, R. W., Lee, G., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Barde, A., Lindeman, R. W., Lee, G., & Billinghurst,
                            M. (2019, August). Binaural Spatialization over a
                            Bone Conduction Headset: The Perception of
                            Elevation. In Audio Engineering Society Conference:
                            2019 AES INTERNATIONAL CONFERENCE ON HEADPHONE
                            TECHNOLOGY. Audio Engineering Society.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{barde2019binaural,<br />
                            title={Binaural Spatialization over a Bone
                            Conduction Headset: The Perception of Elevation},<br />
                            author={Barde, Amit and Lindeman, Robert W and Lee,
                            Gun and Billinghurst, Mark},<br />
                            booktitle={Audio Engineering Society Conference:
                            2019 AES INTERNATIONAL CONFERENCE ON HEADPHONE
                            TECHNOLOGY},<br />
                            year={2019},<br />
                            organization={Audio Engineering Society}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://www.aes.org/e-lib/browse.cfm?elib=20518"
                              target="_blank"
                              >http://www.aes.org/e-lib/browse.cfm?elib=20518</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Binaural spatialization over a bone conduction
                            headset in the vertical plane was investigated using
                            inexpensive and commercially available hardware and
                            software components. The aim of the study was to
                            assess the acuity of binaurally spatialized
                            presentations in the vertical plane. The level of
                            externalization achievable was also explored.
                            Results demonstrate good correlation between
                            established perceptual traits for headphone based
                            auditory localization using non-individualized
                            HRTFs, though localization accuracy appears to be
                            significant worse. A distinct pattern of compressed
                            localization judgments is observed with participants
                            tending to localize the presented stimulus within an
                            approximately 20° range on either side of the
                            inter-aural plane. Localization error was
                            approximately 21° in the vertical plane.
                            Participants reported a good level of
                            externalization. We’ve been able to demonstrate an
                            acceptable level of spatial resolution and
                            externalization is achievable using an inexpensive
                            bone conduction headset and software components.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="172"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael.jpg          1434w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael-300x172.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael-768x441.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael-1024x588.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael-254x146.jpg   254w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael-50x29.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-IEEVR-Michael-131x75.jpg    131w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Head Pointer or Eye Gaze: Which Helps More in MR
                            Remote Collaboration?
                          </h5>
                          <small
                            >Wang, P., Zhang, S., Bai, X., Billinghurst, M., He,
                            W., Wang, S., & Chen, Y.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, P., Zhang, S., Bai, X., Billinghurst, M., He,
                            W., Wang, S., ... & Chen, Y. (2019, March). Head
                            Pointer or Eye Gaze: Which Helps More in MR Remote
                            Collaboration?. In 2019 IEEE Conference on Virtual
                            Reality and 3D User Interfaces (VR) (pp. 1219-1220).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{wang2019head,<br />
                            title={Head Pointer or Eye Gaze: Which Helps More in
                            MR Remote Collaboration?},<br />
                            author={Wang, Peng and Zhang, Shusheng and Bai,
                            Xiaoliang and Billinghurst, Mark and He, Weiping and
                            Wang, Shuxia and Zhang, Xiaokun and Du, Jiaxiang and
                            Chen, Yongxing},<br />
                            booktitle={2019 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={1219--1220},<br />
                            year={2019},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8798024"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8798024</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper investigates how two different unique
                            gaze visualizations (the head pointer(HP), eye
                            gaze(EG)) affect table-size physical tasks in Mixed
                            Reality (MR) remote collaboration. We developed a
                            remote collaborative MR Platform which supports
                            sharing of the remote expert's HP and EG. The
                            prototype was evaluated with a user study comparing
                            two conditions: sharing HP and EG with respect to
                            their effectiveness in the performance and quality
                            of cooperation. There was a statistically
                            significant difference between two conditions on the
                            performance time, and HP is a good proxy for EG in
                            remote collaboration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="197"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem.jpg          1494w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem-300x197.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem-768x504.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem-1024x672.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem-223x146.jpg   223w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem-50x33.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminSelfEsteem-114x75.jpg    114w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Relationship between Self-Esteem and Social
                            Network Loneliness: A Study of Trainee School
                            Counsellors.
                          </h5>
                          <small>Ibili, E., & Billinghurst, M.</small>
                          <p style="font-style: italic; font-size: 12px">
                            Ibili, E., & Billinghurst, M. (2019). The
                            Relationship between Self-Esteem and Social Network
                            Loneliness: A Study of Trainee School Counsellors.
                            Malaysian Online Journal of Educational Technology,
                            7(3), 39-56.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ibili2019relationship,<br />
                            title={The Relationship between Self-Esteem and
                            Social Network Loneliness: A Study of Trainee School
                            Counsellors.},<br />
                            author={Ibili, Emin and Billinghurst, Mark},<br />
                            journal={Malaysian Online Journal of Educational
                            Technology},<br />
                            volume={7},<br />
                            number={3},<br />
                            pages={39--56},<br />
                            year={2019},<br />
                            publisher={ERIC}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://files.eric.ed.gov/fulltext/EJ1220997.pdf"
                              target="_blank"
                              >https://files.eric.ed.gov/fulltext/EJ1220997.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this study, the relationship was investigated
                            between self‐esteem and loneliness in social
                            networks among students in a guidance and
                            psychological counselling teaching department. The
                            study was conducted during the 2017‐2018 academic
                            year with 312 trainee school counsellors from
                            Turkey. In terms of data collection, the Social
                            Network Loneliness Scale, and the Self‐esteem Scale
                            were employed, and a statistical analysis of the
                            data was conducted. We found a negative relationship
                            between self‐esteem and loneliness as experienced in
                            social networks, although neither differs according
                            to sex, age and class level. It was also found that
                            those who use the Internet for communication
                            purposes have high levels of loneliness and
                            self‐esteem in social networks. While self‐esteem
                            levels among users of the Internet are high, those
                            who use it to read about or watch the news have high
                            levels of loneliness. No relationship was found
                            between self‐ esteem and social network loneliness
                            levels and among those who use the Internet for
                            playing games. Regular sporting habits were found to
                            have a positive effect on self‐esteem, but no effect
                            on the level of loneliness in social networks.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="189"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang.jpg          1452w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang-300x189.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang-768x484.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang-1024x646.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang-231x146.jpg   231w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang-50x32.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Wang-119x75.jpg    119w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A comprehensive survey of AR/MR-based co-design in
                            manufacturing
                          </h5>
                          <small
                            >Wang, P., Zhang, S., Billinghurst, M., Bai, X., He,
                            W., Wang, S., Zhang, X.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, P., Zhang, S., Billinghurst, M., Bai, X., He,
                            W., Wang, S., ... & Zhang, X. (2019). A
                            comprehensive survey of AR/MR-based co-design in
                            manufacturing. Engineering with Computers, 1-24.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{wang2019comprehensive,<br />
                            title={A comprehensive survey of AR/MR-based
                            co-design in manufacturing},<br />
                            author={Wang, Peng and Zhang, Shusheng and
                            Billinghurst, Mark and Bai, Xiaoliang and He,
                            Weiping and Wang, Shuxia and Sun, Mengmeng and
                            Zhang, Xu},<br />
                            journal={Engineering with Computers},<br />
                            pages={1--24},<br />
                            year={2019},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s00366-019-00792-3"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s00366-019-00792-3</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            For more than 2 decades, Augmented Reality
                            (AR)/Mixed Reality (MR) has received an increasing
                            amount of attention by researchers and practitioners
                            in the manufacturing community, because it has
                            applications in many fields, such as product design,
                            training, maintenance, assembly, and other
                            manufacturing operations. However, to the best of
                            our knowledge, there has been no comprehensive
                            review of AR-based co-design in manufacturing. This
                            paper presents a comprehensive survey of existing
                            research, projects, and technical characteristics
                            between 1990 and 2017 in the domain of co-design
                            based on AR technology. Among these papers, more
                            than 90% of them were published between 2000 and
                            2017, and these recent relevant works are discussed
                            at length. The paper provides a comprehensive
                            academic roadmap and useful insight into the
                            state-of-the-art of AR-based co-design systems and
                            developments in manufacturing for future researchers
                            all over the world. This work will be useful to
                            researchers who plan to utilize AR as a tool for
                            design research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="226"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2.jpg          1482w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2-300x226.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2-768x577.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2-1024x770.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2-194x146.jpg   194w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2-50x38.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-Emin2-100x75.jpg    100w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Applying the technology acceptance model to
                            understand maths teachers’ perceptions towards an
                            augmented reality tutoring system
                          </h5>
                          <small
                            >Ibili, E., Resnyansky, D., & Billinghurst,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Ibili, E., Resnyansky, D., & Billinghurst, M.
                            (2019). Applying the technology acceptance model to
                            understand maths teachers’ perceptions towards an
                            augmented reality tutoring system. Education and
                            Information Technologies, 1-23.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ibili2019applying,<br />
                            title={Applying the technology acceptance model to
                            understand maths teachers’ perceptions towards an
                            augmented reality tutoring system},<br />
                            author={Ibili, Emin and Resnyansky, Dmitry and
                            Billinghurst, Mark},<br />
                            journal={Education and Information Technologies},<br />
                            pages={1--23},<br />
                            year={2019},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10639-019-09925-z"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10639-019-09925-z</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper examines mathematics teachers’ level of
                            acceptance and intention to use the Augmented
                            Reality Geometry Tutorial System (ARGTS), a mobile
                            Augmented Reality (AR) application developed to
                            enhance students’ 3D geometric thinking skills.
                            ARGTS was shared with mathematics teachers, who were
                            then surveyed using the Technology Acceptance Model
                            (TAM) to understand their acceptance of the
                            technology. We also examined the external variables
                            of Anxiety, Social Norms and Satisfaction. The
                            effect of the teacher’s gender, degree of graduate
                            status and number of years of teaching experience on
                            the subscales of the TAM model were examined. We
                            found that the Perceived Ease of Use (PEU) had a
                            direct effect on the Perceived Usefulness (PU) in
                            accordance with the Technology Acceptance Model
                            (TAM). Both variables together affect Satisfaction
                            (SF), however PEU had no direct effect on Attitude
                            (AT). In addition, while Social Norms (SN) had a
                            direct effect on PU and PEU, there was no direct
                            effect on Behavioural Intention (BI). Anxiety (ANX)
                            had a direct effect on PEU, but no effect on PU and
                            SF. While there was a direct effect of SF on PEU, no
                            direct effect was found on BI. We explain how the
                            results of this study could help improve the
                            understanding of AR acceptance by teachers and
                            provide important guidelines for AR researchers,
                            developers and practitioners.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="178"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry.jpg          1694w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry-300x178.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry-768x455.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry-1024x607.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry-246x146.jpg   246w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry-50x30.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2019-EminGeometry-127x75.jpg    127w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            An assessment of geometry teaching supported with
                            augmented reality teaching materials to enhance
                            students’ 3D geometry thinking skills
                          </h5>
                          <small
                            >İbili, E., Çat, M., Resnyansky, D., Şahin, S., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            İbili, E., Çat, M., Resnyansky, D., Şahin, S., &
                            Billinghurst, M. (2019). An assessment of geometry
                            teaching supported with augmented reality teaching
                            materials to enhance students’ 3D geometry thinking
                            skills. International Journal of Mathematical
                            Education in Science and Technology, 1-23.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ibili2019assessment,<br />
                            title={An assessment of geometry teaching supported
                            with augmented reality teaching materials to enhance
                            students’ 3D geometry thinking skills},<br />
                            author={{\.I}bili, Emin and {\c{C}}at, Mevl{\"u}t
                            and Resnyansky, Dmitry and {\c{S}}ahin, Sami and
                            Billinghurst, Mark},<br />
                            journal={International Journal of Mathematical
                            Education in Science and Technology},<br />
                            pages={1--23},<br />
                            year={2019},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/abs/10.1080/0020739X.2019.1583382"
                              target="_blank"
                              >https://www.tandfonline.com/doi/abs/10.1080/0020739X.2019.1583382</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The aim of this research was to examine the effect
                            of Augmented Reality (AR) supported geometry
                            teaching on students’ 3D thinking skills. This
                            research consisted of 3 steps: (1) developing a 3D
                            thinking ability scale, (ii) design and development
                            of an AR Geometry Tutorial System (ARGTS) and (iii)
                            implementation and assessment of geometry teaching
                            supported with ARGTS. A 3D thinking ability scale
                            was developed and tested with experimental and
                            control groups as a pre- and post-test evaluation.
                            An AR Geometry Tutorial System (ARGTS) and AR
                            teaching materials and environments were developed
                            to enhance 3D thinking skills. A user study with
                            these materials found that geometry teaching
                            supported by ARGTS significantly increased the
                            students’ 3D thinking skills. The increase in
                            average scores of Structuring 3D arrays of cubes and
                            Calculation of the volume and the area of solids
                            thinking skills was not statistically significant
                            (<i>p</i> &gt; 0.05). In terms of other 3D geometric
                            thinking skills’ subfactors of the scale a
                            statistically significant difference was found in
                            favour of the experimental group in pre-test and
                            post-test scores (<i>p</i> &lt; 0.05). The biggest
                            difference was found on ability to recognize and
                            create 3D shapes (<i>p</i> &lt; 0.01).The results of
                            this research are particularly important for
                            identifying individual differences in 3D thinking
                            skills of secondary school students and creating
                            personalized dynamic intelligent learning
                            environments.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="196"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire.jpg          1292w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire-300x196.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire-768x502.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire-1024x669.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire-223x146.jpg   223w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire-50x33.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire-115x75.jpg    115w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Effect of Immersive Displays on Situation
                            Awareness in Virtual Environments for Aerial
                            Firefighting Air Attack Supervisor Training.
                          </h5>
                          <small
                            >Clifford, R. M., Khan, H., Hoermann, S.,
                            Billinghurst, M., & Lindeman, R. W.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Clifford, R. M., Khan, H., Hoermann, S.,
                            Billinghurst, M., & Lindeman, R. W. (2018, March).
                            The Effect of Immersive Displays on Situation
                            Awareness in Virtual Environments for Aerial
                            Firefighting Air Attack Supervisor Training. In 2018
                            IEEE Conference on Virtual Reality and 3D User
                            Interfaces (VR) (pp. 1-2). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{clifford2018effect,<br />
                            title={The Effect of Immersive Displays on Situation
                            Awareness in Virtual Environments for Aerial
                            Firefighting Air Attack Supervisor Training},<br />
                            author={Clifford, Rory MS and Khan, Humayun and
                            Hoermann, Simon and Billinghurst, Mark and Lindeman,
                            Robert W},<br />
                            booktitle={2018 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={1--2},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.computer.org/csdl/proceedings-article/vr/2018/08446139/13bd1AIBM1Q"
                              target="_blank"
                              >https://www.computer.org/csdl/proceedings-article/vr/2018/08446139/13bd1AIBM1Q</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Situation Awareness (SA) is an essential skill in
                            Air Attack Supervision (AAS) for aerial based
                            wildfire firefighting. The display types used for
                            Virtual Reality Training Systems (VRTS) afford
                            different visual SA depending on the Field of View
                            (FoV) as well as the sense of presence users can
                            obtain in the virtual environment. We conducted a
                            study with 36 participants to evaluate SA
                            acquisition in three display types: a
                            high-definition TV (HDTV), an Oculus Rift
                            Head-Mounted Display (HMD) and a 270° cylindrical
                            simulation projection display called the SimPit. We
                            found a significant difference between the HMD and
                            the HDTV, as well as with the SimPit and the HDTV
                            for the three levels of SA.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="191"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey.jpg          1154w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey-300x191.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey-768x490.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey-1024x653.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey-229x146.jpg   229w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey-50x32.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KangsooSurvey-118x75.jpg    118w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Revisiting trends in augmented reality research: A
                            review of the 2nd decade of ISMAR (2008–2017)
                          </h5>
                          <small
                            >Kim, K., Billinghurst, M., Bruder, G., Duh, H. B.
                            L., & Welch, G. F.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kim, K., Billinghurst, M., Bruder, G., Duh, H. B.
                            L., & Welch, G. F. (2018). Revisiting trends in
                            augmented reality research: A review of the 2nd
                            decade of ISMAR (2008–2017). IEEE transactions on
                            visualization and computer graphics, 24(11),
                            2947-2962.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{kim2018revisiting,<br />
                            title={Revisiting trends in augmented reality
                            research: A review of the 2nd decade of ISMAR
                            (2008--2017)},<br />
                            author={Kim, Kangsoo and Billinghurst, Mark and
                            Bruder, Gerd and Duh, Henry Been-Lirn and Welch,
                            Gregory F},<br />
                            journal={IEEE transactions on visualization and
                            computer graphics},<br />
                            volume={24},<br />
                            number={11},<br />
                            pages={2947--2962},<br />
                            year={2018},<br />
                            publisher={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8456568"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8456568</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In 2008, Zhou et al. presented a survey paper
                            summarizing the previous ten years of ISMAR
                            publications, which provided invaluable insights
                            into the research challenges and trends associated
                            with that time period. Ten years later, we review
                            the research that has been presented at ISMAR
                            conferences since the survey of Zhou et al., at a
                            time when both academia and the AR industry are
                            enjoying dramatic technological changes. Here we
                            consider the research results and trends of the last
                            decade of ISMAR by carefully reviewing the ISMAR
                            publications from the period of 2008-2017, in the
                            context of the first ten years. The numbers of
                            papers for different research topics and their
                            impacts by citations were analyzed while reviewing
                            them-which reveals that there is a sharp increase in
                            AR evaluation and rendering research. Based on this
                            review we offer some observations related to
                            potential future research areas or trends, which
                            could be helpful to AR researchers and industry
                            members looking ahead.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="227"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro.jpg         856w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro-300x227.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro-768x581.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro-193x146.jpg 193w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro-50x38.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Caro-99x75.jpg    99w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Narrative and Spatial Memory for Jury Viewings in a
                            Reconstructed Virtual Environment
                          </h5>
                          <small
                            >Reichherzer, C., Cunningham, A., Walsh, J., Kohler,
                            M., Billinghurst, M., & Thomas, B. H.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Reichherzer, C., Cunningham, A., Walsh, J., Kohler,
                            M., Billinghurst, M., & Thomas, B. H. (2018).
                            Narrative and Spatial Memory for Jury Viewings in a
                            Reconstructed Virtual Environment. IEEE transactions
                            on visualization and computer graphics, 24(11),
                            2917-2926.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{reichherzer2018narrative,<br />
                            title={Narrative and Spatial Memory for Jury
                            Viewings in a Reconstructed Virtual Environment},<br />
                            author={Reichherzer, Carolin and Cunningham, Andrew
                            and Walsh, James and Kohler, Mark and Billinghurst,
                            Mark and Thomas, Bruce H},<br />
                            journal={IEEE transactions on visualization and
                            computer graphics},<br />
                            volume={24},<br />
                            number={11},<br />
                            pages={2917--2926},<br />
                            year={2018},<br />
                            publisher={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://uploads-ssl.webflow.com/5cd23e823ab9b1f01f815a54/5cfa0fae444ab0e632fb3ab4_Narrative%20and%20Spatial%20Memory%20for%20Jury%20Viewings%20in%20a%20Reconstructed%20Virtual%20Environment.pdf"
                              target="_blank"
                              >https://uploads-ssl.webflow.com/5cd23e823ab9b1f01f815a54/5cfa0fae444ab0e632fb3ab4_Narrative%20and%20Spatial%20Memory%20for%20Jury%20Viewings%20in%20a%20Reconstructed%20Virtual%20Environment.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper showcases one way of how virtual
                            reconstruction can be used in a courtroom. The
                            results of a pilot study on narrative and spatial
                            memory are presented in the context of viewing real
                            and virtual copies of a simulated crime scene. Based
                            on current court procedures, three different viewing
                            options were compared: photographs, a real life
                            visit, and a 3D virtual reconstruction of the scene
                            viewed in a Virtual Reality headset. Participants
                            were also given a written narrative that included
                            the spatial locations of stolen goods and were
                            measured on their ability to recall and understand
                            these spatial relationships of those stolen items.
                            The results suggest that Virtual Reality is more
                            reliable for spatial memory compared to photographs
                            and that Virtual Reality provides a compromise for
                            when physical viewing of crime scenes are not
                            possible. We conclude that Virtual Reality is a
                            promising medium for the court.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="200"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben.jpg         890w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben-300x200.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben-768x513.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben-219x146.jpg 219w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben-50x33.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Ben-112x75.jpg  112w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Comparison of Predictive Spatial Augmented Reality
                            Cues for Procedural Tasks
                          </h5>
                          <small
                            >Volmer, B., Baumeister, J., Von Itzstein, S.,
                            Bornkessel-Schlesewsky, I., Schlesewsky, M.,
                            Billinghurst, M., & Thomas, B. H.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Volmer, B., Baumeister, J., Von Itzstein, S.,
                            Bornkessel-Schlesewsky, I., Schlesewsky, M.,
                            Billinghurst, M., & Thomas, B. H. (2018). A
                            Comparison of Predictive Spatial Augmented Reality
                            Cues for Procedural Tasks. IEEE transactions on
                            visualization and computer graphics, 24(11),
                            2846-2856.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{volmer2018comparison,<br />
                            title={A Comparison of Predictive Spatial Augmented
                            Reality Cues for Procedural Tasks},<br />
                            author={Volmer, Benjamin and Baumeister, James and
                            Von Itzstein, Stewart and Bornkessel-Schlesewsky,
                            Ina and Schlesewsky, Matthias and Billinghurst, Mark
                            and Thomas, Bruce H},<br />
                            journal={IEEE transactions on visualization and
                            computer graphics},<br />
                            volume={24},<br />
                            number={11},<br />
                            pages={2846--2856},<br />
                            year={2018},<br />
                            publisher={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8493594"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8493594</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <div class="abstract-text row">
                              <div class="col-12">
                                <div class="u-mb-1">
                                  <div>
                                    Previous research has demonstrated that
                                    Augmented Reality can reduce a user's task
                                    response time and mental effort when
                                    completing a procedural task. This paper
                                    investigates techniques to improve user
                                    performance and reduce mental effort by
                                    providing projector-based Spatial Augmented
                                    Reality predictive cues for future
                                    responses. The objective of the two
                                    experiments conducted in this study was to
                                    isolate the performance and mental effort
                                    differences from several different
                                    annotation cueing techniques for simple
                                    (Experiment 1) and complex (Experiment 2)
                                    button-pressing tasks. Comporting with
                                    existing cognitive neuroscience literature
                                    on prediction, attentional orienting, and
                                    interference, we hypothesized that for both
                                    simple procedural tasks and complex
                                    search-based tasks, having a visual cue
                                    guiding to the next task's location would
                                    positively impact performance relative to a
                                    baseline, no-cue condition. Additionally, we
                                    predicted that direction-based cues would
                                    provide a more significant positive impact
                                    than target-based cues. The results
                                    indicated that providing a line to the next
                                    task was the most effective technique for
                                    improving the users' task time and mental
                                    effort in both the simple and complex tasks.
                                  </div>
                                </div>
                              </div>
                            </div>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="261"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ThamSuperman.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ThamSuperman.jpg         744w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ThamSuperman-300x261.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ThamSuperman-168x146.jpg 168w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ThamSuperman-50x44.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ThamSuperman-86x75.jpg    86w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Superman vs giant: a study on spatial perception for
                            a multi-scale mixed reality flying telepresence
                            interface
                          </h5>
                          <small
                            >Piumsomboon, T., Lee, G. A., Ens, B., Thomas, B.
                            H., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Piumsomboon, T., Lee, G. A., Ens, B., Thomas, B. H.,
                            & Billinghurst, M. (2018). Superman vs giant: a
                            study on spatial perception for a multi-scale mixed
                            reality flying telepresence interface. IEEE
                            transactions on visualization and computer graphics,
                            24(11), 2974-2982.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{piumsomboon2018superman,<br />
                            title={Superman vs giant: a study on spatial
                            perception for a multi-scale mixed reality flying
                            telepresence interface},<br />
                            author={Piumsomboon, Thammathip and Lee, Gun A and
                            Ens, Barrett and Thomas, Bruce H and Billinghurst,
                            Mark},<br />
                            journal={IEEE transactions on visualization and
                            computer graphics},<br />
                            volume={24},<br />
                            number={11},<br />
                            pages={2974--2982},<br />
                            year={2018},<br />
                            publisher={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/327707577_Superman_vs_Giant_A_Study_on_Spatial_Perception_for_a_Multi-Scale_Mixed_Reality_Flying_Telepresence_Interface/links/5ce6632ea6fdccc9ddc735b2/Superman-vs-Giant-A-Study-on-Spatial-Perception-for-a-Multi-Scale-Mixed-Reality-Flying-Telepresence-Interface.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/327707577_Superman_vs_Giant_A_Study_on_Spatial_Perception_for_a_Multi-Scale_Mixed_Reality_Flying_Telepresence_Interface/links/5ce6632ea6fdccc9ddc735b2/Superman-vs-Giant-A-Study-on-Spatial-Perception-for-a-Multi-Scale-Mixed-Reality-Flying-Telepresence-Interface.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The advancements in Mixed Reality (MR), Unmanned
                            Aerial Vehicle, and multi-scale collaborative
                            virtual environments have led to new interface
                            opportunities for remote collaboration. This paper
                            explores a novel concept of flying telepresence for
                            multi-scale mixed reality remote collaboration. This
                            work could enable remote collaboration at a larger
                            scale such as building construction. We conducted a
                            user study with three experiments. The first
                            experiment compared two interfaces, static and
                            dynamic IPD, on simulator sickness and body size
                            perception. The second experiment tested the user
                            perception of a virtual object size under three
                            levels of IPD and movement gain manipulation with a
                            fixed eye height in a virtual environment having
                            reduced or rich visual cues. Our last experiment
                            investigated the participant’s body size perception
                            for two levels of manipulation of the IPDs and
                            heights using stereo video footage to simulate a
                            flying telepresence experience. The studies found
                            that manipulating IPDs and eye height influenced the
                            user’s size perception. We present our findings and
                            share the recommendations for designing a
                            multi-scale MR flying telepresence interface.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-300x300.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-300x300.jpg    300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-150x150.jpg    150w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-768x768.jpg    768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-1024x1024.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-146x146.jpg    146w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-50x50.jpg       50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-75x75.jpg       75w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-85x85.jpg       85w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley-80x80.jpg       80w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-BRadley.jpg           1085w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Design considerations for combining augmented
                            reality with intelligent tutors
                          </h5>
                          <small
                            >Herbert, B., Ens, B., Weerasinghe, A.,
                            Billinghurst, M., & Wigley, G.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Herbert, B., Ens, B., Weerasinghe, A., Billinghurst,
                            M., & Wigley, G. (2018). Design considerations for
                            combining augmented reality with intelligent tutors.
                            Computers & Graphics, 77, 166-182.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{herbert2018design,<br />
                            title={Design considerations for combining augmented
                            reality with intelligent tutors},<br />
                            author={Herbert, Bradley and Ens, Barrett and
                            Weerasinghe, Amali and Billinghurst, Mark and
                            Wigley, Grant},<br />
                            journal={Computers \& Graphics},<br />
                            volume={77},<br />
                            pages={166--182},<br />
                            year={2018},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/abs/pii/S0097849318301523"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/abs/pii/S0097849318301523</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality overlays virtual objects on the
                            real world in real-time and has the potential to
                            enhance education, however, few AR training systems
                            provide personalised learning support. Combining AR
                            with intelligent tutoring systems has the potential
                            to improve training outcomes by providing
                            personalised learner support, such as feedback on
                            the AR environment. This paper reviews the current
                            state of AR training systems combined with ITSs and
                            proposes a series of requirements for combining the
                            two paradigms. In addition, this paper identifies a
                            growing need to provide more research in the context
                            of design and implementation of adaptive augmented
                            reality tutors (ARATs). These include possibilities
                            of evaluating the user interfaces of ARAT and
                            potential domains where an ARAT might be considered
                            effective.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="219"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2.jpg         1016w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2-300x219.jpg  300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2-768x559.jpg  768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2-200x146.jpg  200w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2-50x36.jpg     50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFire2-103x75.jpg   103w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Development of a Multi-Sensory Virtual Reality
                            Training Simulator for Airborne Firefighters
                            Supervising Aerial Wildfire Suppression
                          </h5>
                          <small
                            >Clifford, R. M., Khan, H., Hoermann, S.,
                            Billinghurst, M., & Lindeman, R. W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Clifford, R. M., Khan, H., Hoermann, S.,
                            Billinghurst, M., & Lindeman, R. W. (2018, March).
                            Development of a Multi-Sensory Virtual Reality
                            Training Simulator for Airborne Firefighters
                            Supervising Aerial Wildfire Suppression. In 2018
                            IEEE Workshop on Augmented and Virtual Realities for
                            Good (VAR4Good) (pp. 1-5). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{clifford2018development,<br />
                            title={Development of a Multi-Sensory Virtual
                            Reality Training Simulator for Airborne Firefighters
                            Supervising Aerial Wildfire Suppression},<br />
                            author={Clifford, Rory MS and Khan, Humayun and
                            Hoermann, Simon and Billinghurst, Mark and Lindeman,
                            Robert W},<br />
                            booktitle={2018 IEEE Workshop on Augmented and
                            Virtual Realities for Good (VAR4Good)},<br />
                            pages={1--5},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8576892"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8576892</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Wildfire firefighting is difficult to train for in
                            the real world due to a variety of reasons, cost and
                            environmental impact being the major barriers to
                            effective training. Virtual Reality offers greater
                            training opportunities to practice crucial skills,
                            difficult to obtain without experiencing the actual
                            environment. Situation Awareness (SA) is a critical
                            aspect of Air Attack Supervision (AAS). Timely
                            decisions need to be made by the AAS based on the
                            information gathered while airborne. The type of
                            display used in virtual reality training systems
                            afford different levels of SA due to factors such as
                            field of view, as well as presence within the
                            virtual environment and the system. We conducted a
                            study with 36 participants to evaluate SA
                            acquisition and immersion in three display types: a
                            high-definition TV (HDTV), an Oculus Rift
                            Head-Mounted Display (HMD) and a 270° cylindrical
                            projection system (SimPit). We found a significant
                            difference between the HMD and the HDTV, as well as
                            with the SimPit and the HDTV for SA levels.
                            Preference was given more to the HMD for immersion
                            and portability, but the SimPit gave the best
                            environment for the actual role.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="168"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA.jpg          1186w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA-300x168.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA-768x430.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA-1024x573.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA-260x146.jpg   260w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA-50x28.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-CIA-134x75.jpg    134w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Collaborative immersive analytics.
                          </h5>
                          <small
                            >Billinghurst, M., Cordeil, M., Bezerianos, A., &
                            Margolis, T.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Billinghurst, M., Cordeil, M., Bezerianos, A., &
                            Margolis, T. (2018). Collaborative immersive
                            analytics. In Immersive Analytics (pp. 221-257).
                            Springer, Cham.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @incollection{billinghurst2018collaborative,<br />
                            title={Collaborative immersive analytics},<br />
                            author={Billinghurst, Mark and Cordeil, Maxime and
                            Bezerianos, Anastasia and Margolis, Todd},<br />
                            booktitle={Immersive Analytics},<br />
                            pages={221--257},<br />
                            year={2018},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/chapter/10.1007/978-3-030-01388-2_8"
                              target="_blank"
                              >https://link.springer.com/chapter/10.1007/978-3-030-01388-2_8</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Many of the problems being addressed by Immersive
                            Analytics require groups of people to solve. This
                            chapter introduces the concept of Collaborative
                            Immersive Analytics (CIA) and reviews how immersive
                            technologies can be combined with Visual Analytics
                            to facilitate co-located and remote collaboration.
                            We provide a definition of Collaborative Immersive
                            Analytics and then an overview of the different
                            types of possible collaboration. The chapter also
                            discusses the various roles in collaborative
                            systems, and how to support shared interaction with
                            the data being presented. Finally, we summarize the
                            opportunities for future research in this domain.
                            The aim of the chapter is to provide enough of an
                            introduction to CIA and key directions for future
                            research, so that practitioners will be able to
                            begin working in the field.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="186"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2.jpg          1170w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2-300x186.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2-768x475.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2-1024x634.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2-236x146.jpg   236w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2-50x31.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-RoryFlight2-121x75.jpg    121w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Evaluating the effects of realistic communication
                            disruptions in VR training for aerial firefighting
                          </h5>
                          <small
                            >Clifford, R. M., Hoermann, S., Marcadet, N.,
                            Oliver, H., Billinghurst, M., & Lindeman, R.
                            W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Clifford, R. M., Hoermann, S., Marcadet, N., Oliver,
                            H., Billinghurst, M., & Lindeman, R. W. (2018,
                            September). Evaluating the effects of realistic
                            communication disruptions in VR training for aerial
                            firefighting. In 2018 10th International Conference
                            on Virtual Worlds and Games for Serious Applications
                            (VS-Games) (pp. 1-8). IEEE. Clifford, Rory MS, Simon
                            Hoermann, Nicolas Marcade
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{clifford2018evaluating,<br />
                            title={Evaluating the effects of realistic
                            communication disruptions in VR training for aerial
                            firefighting},<br />
                            author={Clifford, Rory MS and Hoermann, Simon and
                            Marcadet, Nicolas and Oliver, Hamish and
                            Billinghurst, Mark and Lindeman, Robert W},<br />
                            booktitle={2018 10th International Conference on
                            Virtual Worlds and Games for Serious Applications
                            (VS-Games)},<br />
                            pages={1--8},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8525704"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8525704</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Aerial firefighting takes place in stressful
                            environments where decision making and communication
                            are paramount, and skills need to be practiced and
                            trained regularly. An experiment was performed to
                            test the effects of disrupting the communications
                            ability of the users on their stress levels in a
                            noisy environment. The goal of this research is to
                            investigate how realistic disruption of
                            communication systems can be simulated in a virtual
                            environment and to what extent they induce stress.
                            We found that aerial firefighting experts maintained
                            a better Heart Rate Variability (HRV) during
                            disruptions than novices. Experts showed better
                            ability to manage stress based on the change in HRV
                            during the experiment. Our main finding is that
                            communication disruptions in virtual reality (e.g.,
                            broken transmissions) significantly impacted the
                            level of stress experienced by participants.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="244"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate.jpg         826w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate-300x244.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate-768x625.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate-179x146.jpg 179w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate-50x41.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Teammate-92x75.jpg    92w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            TEAMMATE: A Scalable System for Measuring Affect in
                            Human-Machine Teams
                          </h5>
                          <small
                            >Wen, J., Stewart, A., Billinghurst, M., & Tossel,
                            C.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Wen, J., Stewart, A., Billinghurst, M., & Tossel, C.
                            (2018, August). TEAMMATE: A Scalable System for
                            Measuring Affect in Human-Machine Teams. In 2018
                            27th IEEE International Symposium on Robot and Human
                            Interactive Communication (RO-MAN) (pp. 991-996).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{wen2018teammate,<br />
                            title={TEAMMATE: A Scalable System for Measuring
                            Affect in Human-Machine Teams},<br />
                            author={Wen, James and Stewart, Amanda and
                            Billinghurst, Mark and Tossel, Chad},<br />
                            booktitle={2018 27th IEEE International Symposium on
                            Robot and Human Interactive Communication
                            (RO-MAN)},<br />
                            pages={991--996},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8525704"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8525704</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Strong empathic bonding between members of a team
                            can elevate team performance tremendously but it is
                            not clear how such bonding within human-machine
                            teams may impact upon mission success. Prior work
                            using self-reporting surveys and end-of-task metrics
                            do not capture how such bonding may evolve over time
                            and impact upon task fulfillment. Furthermore,
                            sensor-based measures do not scale easily to
                            facilitate the need to collect substantial data for
                            measuring potentially subtle effects. We introduce
                            TEAMMATE, a system designed to provide insights into
                            the emotional dynamics humans may form for machine
                            teammates that could critically impact upon the
                            design of human machine teams.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="286"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands.jpg          1426w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands-286x300.jpg   286w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands-768x807.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands-975x1024.jpg  975w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands-139x146.jpg   139w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands-48x50.jpg      48w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hands-71x75.jpg      71w
                              "
                              sizes="(max-width: 286px) 100vw, 286px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Designing an Augmented Reality Multimodal Interface
                            for 6DOF Manipulation Techniques
                          </h5>
                          <small
                            >Ismail, A. W., Billinghurst, M., Sunar, M. S., &
                            Yusof, C. S.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Ismail, A. W., Billinghurst, M., Sunar, M. S., &
                            Yusof, C. S. (2018, September). Designing an
                            Augmented Reality Multimodal Interface for 6DOF
                            Manipulation Techniques. In Proceedings of SAI
                            Intelligent Systems Conference (pp. 309-322).
                            Springer, Cham.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{ismail2018designing,<br />
                            title={Designing an Augmented Reality Multimodal
                            Interface for 6DOF Manipulation Techniques},<br />
                            author={Ismail, Ajune Wanis and Billinghurst, Mark
                            and Sunar, Mohd Shahrizal and Yusof, Cik
                            Suhaimi},<br />
                            booktitle={Proceedings of SAI Intelligent Systems
                            Conference},<br />
                            pages={309--322},<br />
                            year={2018},<br />
                            organization={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/chapter/10.1007/978-3-030-01054-6_22"
                              target="_blank"
                              >https://link.springer.com/chapter/10.1007/978-3-030-01054-6_22</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) supports natural interaction
                            in physical and virtual worlds, so it has recently
                            given rise to a number of novel interaction
                            modalities. This paper presents a method for using
                            hand-gestures with speech input for multimodal
                            interaction in AR. It focuses on providing an
                            intuitive AR environment which supports natural
                            interaction with virtual objects while sustaining
                            accessible real tasks and interaction mechanisms.
                            The paper reviews previous multimodal interfaces and
                            describes recent studies in AR that employ gesture
                            and speech inputs for multimodal input. It describes
                            an implementation of gesture interaction with speech
                            input in AR for virtual object manipulation.
                            Finally, the paper presents a user evaluation of the
                            technique, showing that it can be used to improve
                            the interaction between virtual and physical
                            elements in an AR environment.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="207"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion.jpg         956w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion-300x207.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion-768x530.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion-211x146.jpg 211w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion-50x35.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Emotion-109x75.jpg  109w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Emotion Sharing and Augmentation in Cooperative
                            Virtual Reality Games
                          </h5>
                          <small
                            >Hart, J. D., Piumsomboon, T., Lawrence, L., Lee, G.
                            A., Smith, R. T., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Hart, J. D., Piumsomboon, T., Lawrence, L., Lee, G.
                            A., Smith, R. T., & Billinghurst, M. (2018,
                            October). Emotion Sharing and Augmentation in
                            Cooperative Virtual Reality Games. In Proceedings of
                            the 2018 Annual Symposium on Computer-Human
                            Interaction in Play Companion Extended Abstracts
                            (pp. 453-460). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{hart2018emotion,<br />
                            title={Emotion Sharing and Augmentation in
                            Cooperative Virtual Reality Games},<br />
                            author={Hart, Jonathon D and Piumsomboon, Thammathip
                            and Lawrence, Louise and Lee, Gun A and Smith, Ross
                            T and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 2018 Annual Symposium
                            on Computer-Human Interaction in Play Companion
                            Extended Abstracts},<br />
                            pages={453--460},<br />
                            year={2018},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3271543"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3271543</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present preliminary findings from sharing and
                            augmenting facial expression in cooperative social
                            Virtual Reality (VR) games. We implemented a
                            prototype system for capturing and sharing facial
                            expression between VR players through their avatar.
                            We describe our current prototype system and how it
                            could be assimilated into a system for enhancing
                            social VR experience. Two social VR games were
                            created for a preliminary user study. We discuss our
                            findings from the user study, potential games for
                            this system, and future directions for this
                            research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="201"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary.jpg         934w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary-300x201.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary-768x515.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary-218x146.jpg 218w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary-50x34.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Scary-112x75.jpg  112w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Effects of Manipulating Physiological Feedback in
                            Immersive Virtual Environments
                          </h5>
                          <small
                            >Dey, A., Chen, H., Billinghurst, M., & Lindeman, R.
                            W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dey, A., Chen, H., Billinghurst, M., & Lindeman, R.
                            W. (2018, October). Effects of Manipulating
                            Physiological Feedback in Immersive Virtual
                            Environments. In Proceedings of the 2018 Annual
                            Symposium on Computer-Human Interaction in Play (pp.
                            101-111). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2018effects,<br />
                            title={Effects of Manipulating Physiological
                            Feedback in Immersive Virtual Environments},<br />
                            author={Dey, Arindam and Chen, Hao and Billinghurst,
                            Mark and Lindeman, Robert W},<br />
                            booktitle={Proceedings of the 2018 Annual Symposium
                            on Computer-Human Interaction in Play},<br />
                            pages={101--111},<br />
                            year={2018},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Arindam_Dey5/publication/328549307_Effects_of_Manipulating_Physiological_Feedback_in_Immersive_Virtual_Environments/links/5bdfe538299bf1124fbb8579/Effects-of-Manipulating-Physiological-Feedback-in-Immersive-Virtual-Environments.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Arindam_Dey5/publication/328549307_Effects_of_Manipulating_Physiological_Feedback_in_Immersive_Virtual_Environments/links/5bdfe538299bf1124fbb8579/Effects-of-Manipulating-Physiological-Feedback-in-Immersive-Virtual-Environments.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <p class="p1">
                              Virtual environments have been proven to be
                              effective in evoking emotions. Earlier research
                              has found that physiological data is a valid
                              measurement of the emotional state of the user.
                              Being able to see one’s physiological feedback in
                              a virtual environment has proven to make the
                              application more enjoyable. In this paper, we have
                              investigated the effects of manipulating heart
                              rate feedback provided to the participants in a
                              single user immersive virtual environment. Our
                              results show that providing slightly faster or
                              slower real-time heart rate feedback can alter
                              participants’ emotions more than providing
                              unmodified feedback. However, altering the
                              feedback does not alter real physiological
                              signals.
                            </p>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="241"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao.jpg         796w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao-300x241.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao-768x617.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao-182x146.jpg 182w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao-50x40.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Gao-93x75.jpg    93w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Real-time visual representations for mobile mixed
                            reality remote collaboration.
                          </h5>
                          <small
                            >Gao, L., Bai, H., He, W., Billinghurst, M., &
                            Lindeman, R. W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gao, L., Bai, H., He, W., Billinghurst, M., &
                            Lindeman, R. W. (2018, December). Real-time visual
                            representations for mobile mixed reality remote
                            collaboration. In SIGGRAPH Asia 2018 Virtual &
                            Augmented Reality (p. 15). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gao2018real,<br />
                            title={Real-time visual representations for mobile
                            mixed reality remote collaboration},<br />
                            author={Gao, Lei and Bai, Huidong and He, Weiping
                            and Billinghurst, Mark and Lindeman, Robert W},<br />
                            booktitle={SIGGRAPH Asia 2018 Virtual \& Augmented
                            Reality},<br />
                            pages={15},<br />
                            year={2018},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3275515"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3275515</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this study we present a Mixed-Reality based
                            mobile remote collaboration system that enables an
                            expert providing real-time assistance over a
                            physical distance. By using the Google ARCore
                            position tracking, we can integrate the keyframes
                            captured with one external depth sensor attached to
                            the mobile phone as one single 3D point-cloud data
                            set to present the local physical environment into
                            the VR world. This captured local scene is then
                            wirelessly streamed to the remote side for the
                            expert to view while wearing a mobile VR headset
                            (HTC VIVE Focus). In this case, the remote expert
                            can immerse himself/herself in the VR scene and
                            provide guidance just as sharing the same work
                            environment with the local worker. In addition, the
                            remote guidance is also streamed back to the local
                            side as an AR cue overlaid on top of the local video
                            see-through display. Our proposed mobile remote
                            collaboration system supports a pair of participants
                            performing as one remote expert guiding one local
                            worker on some physical tasks in a more natural and
                            efficient way in a large scale work space from a
                            distance by simulating the face-to-face co-work
                            experience using the Mixed-Reality technique.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="252"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2.jpg         802w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2-300x252.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2-768x645.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2-174x146.jpg 174w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2-50x42.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-James2-89x75.jpg    89w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Band of Brothers and Bolts: Caring About Your Robot
                            Teammate
                          </h5>
                          <small
                            >Wen, J., Stewart, A., Billinghurst, M., & Tossell,
                            C.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Wen, J., Stewart, A., Billinghurst, M., & Tossell,
                            C. (2018, October). Band of Brothers and Bolts:
                            Caring About Your Robot Teammate. In 2018 IEEE/RSJ
                            International Conference on Intelligent Robots and
                            Systems (IROS) (pp. 1853-1858). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{wen2018band,<br />
                            title={Band of Brothers and Bolts: Caring About Your
                            Robot Teammate},<br />
                            author={Wen, James and Stewart, Amanda and
                            Billinghurst, Mark and Tossell, Chad},<br />
                            booktitle={2018 IEEE/RSJ International Conference on
                            Intelligent Robots and Systems (IROS)},<br />
                            pages={1853--1858},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8594324"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8594324</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            It has been observed that a robot shown as suffering
                            is enough to cause an empathic response from a
                            person. Whether the response is a fleeting reaction
                            with no consequences or a meaningful perspective
                            change with associated behavior modifications is not
                            clear. Existing work has been limited to
                            measurements made at the end of empathy inducing
                            experimental trials rather measurements made over
                            time to capture consequential behavioral pattern. We
                            report on preliminary results collected from a study
                            that attempts to measure how the actions of a
                            participant may be altered by empathy for a robot
                            companion. Our findings suggest that induced empathy
                            can in fact have a significant impact on a person's
                            behavior to the extent that the ability to fulfill a
                            mission may be affected.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="142"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise.jpg          1704w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise-300x142.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise-768x362.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise-1024x483.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise-260x123.jpg   260w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise-50x24.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Louise-150x71.jpg    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The effect of video placement in AR conferencing
                            applications
                          </h5>
                          <small
                            >Lawrence, L., Dey, A., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Lawrence, L., Dey, A., & Billinghurst, M. (2018,
                            December). The effect of video placement in AR
                            conferencing applications. In Proceedings of the
                            30th Australian Conference on Computer-Human
                            Interaction (pp. 453-457). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{lawrence2018effect,<br />
                            title={The effect of video placement in AR
                            conferencing applications},<br />
                            author={Lawrence, Louise and Dey, Arindam and
                            Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 30th Australian
                            Conference on Computer-Human Interaction},<br />
                            pages={453--457},<br />
                            year={2018},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We ran a pilot study to investigate the impact of
                            video placement in augmented reality conferencing on
                            communication, social presence and user preference.
                            In addition, we explored the influence of different
                            tasks, assembly and negotiation. We discovered a
                            correlation between video placement and the type of
                            the tasks, with some significant results in social
                            presence indicators.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="211"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony.jpg          1206w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony-300x211.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony-768x540.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony-1024x720.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony-208x146.jpg   208w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony-50x35.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Tony-107x75.jpg    107w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            HandsInTouch: sharing gestures in remote
                            collaboration
                          </h5>
                          <small
                            >Huang, W., Billinghurst, M., Alem, L., & Kim,
                            S.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Huang, W., Billinghurst, M., Alem, L., & Kim, S.
                            (2018, December). HandsInTouch: sharing gestures in
                            remote collaboration. In Proceedings of the 30th
                            Australian Conference on Computer-Human Interaction
                            (pp. 396-400). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{huang2018handsintouch,<br />
                            title={HandsInTouch: sharing gestures in remote
                            collaboration},<br />
                            author={Huang, Weidong and Billinghurst, Mark and
                            Alem, Leila and Kim, Seungwon},<br />
                            booktitle={Proceedings of the 30th Australian
                            Conference on Computer-Human Interaction},<br />
                            pages={396--400},<br />
                            year={2018},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3292177"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3292177</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Many systems have been developed to support remote
                            collaboration, where hand gestures or sketches can
                            be shared. However, the effect of combining gesture
                            and sketching together has not been fully explored
                            and understood. In this paper we describe
                            HandsInTouch, a system in which both hand gestures
                            and sketches made by a remote helper are shown to a
                            local user in real time. We conducted a user study
                            to test the usability of the system and the
                            usefulness of combing gesture and sketching for
                            remote collaboration. We discuss results and make
                            recommendations for system design and future work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="251"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT.jpg         924w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT-300x251.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT-768x643.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT-174x146.jpg 174w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT-50x42.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-GRAT-90x75.jpg    90w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A generalized, rapid authoring tool for intelligent
                            tutoring systems
                          </h5>
                          <small
                            >Herbert, B., Billinghurst, M., Weerasinghe, A.,
                            Ens, B., & Wigley, G.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Herbert, B., Billinghurst, M., Weerasinghe, A., Ens,
                            B., & Wigley, G. (2018, December). A generalized,
                            rapid authoring tool for intelligent tutoring
                            systems. In Proceedings of the 30th Australian
                            Conference on Computer-Human Interaction (pp.
                            368-373). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{herbert2018generalized,<br />
                            title={A generalized, rapid authoring tool for
                            intelligent tutoring systems},<br />
                            author={Herbert, Bradley and Billinghurst, Mark and
                            Weerasinghe, Amali and Ens, Barret and Wigley,
                            Grant},<br />
                            booktitle={Proceedings of the 30th Australian
                            Conference on Computer-Human Interaction},<br />
                            pages={368--373},<br />
                            year={2018},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?doid=3292147.3292202"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?doid=3292147.3292202</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            As computer-based training systems become
                            increasingly integrated into real-world training,
                            tools which rapidly author courses for such systems
                            are emerging. However, inconsistent user interface
                            design and limited support for a variety of domains
                            makes them time consuming and difficult to use. We
                            present a Generalized, Rapid Authoring Tool (GRAT),
                            which simplifies creation of Intelligent Tutoring
                            Systems (ITSs) using a unified web-based
                            wizard-style graphical user interface and
                            programming-by-demonstration approaches to reduce
                            technical knowledge needed to author ITS logic. We
                            implemented a prototype, which authors courses for
                            two kinds of tasks: A network cabling task and a
                            console device configuration task to demonstrate the
                            tool's potential. We describe the limitations of our
                            prototype and present opportunities for evaluating
                            the tool's usability and perceived effectiveness.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="233"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm.jpg         954w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm-300x233.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm-768x596.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm-188x146.jpg 188w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm-50x39.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-KIm-97x75.jpg    97w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Using Freeze Frame and Visual Notifications in an
                            Annotation Drawing Interface for Remote
                            Collaboration.
                          </h5>
                          <small
                            >Kim, S., Billinghurst, M., Lee, C., & Lee, G</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kim, S., Billinghurst, M., Lee, C., & Lee, G.
                            (2018). Using Freeze Frame and Visual Notifications
                            in an Annotation Drawing Interface for Remote
                            Collaboration. KSII Transactions on Internet &
                            Information Systems, 12(12).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{kim2018using,<br />
                            title={Using Freeze Frame and Visual Notifications
                            in an Annotation Drawing Interface for Remote
                            Collaboration.},<br />
                            author={Kim, Seungwon and Billinghurst, Mark and
                            Lee, Chilwoo and Lee, Gun},<br />
                            journal={KSII Transactions on Internet \&
                            Information Systems},<br />
                            volume={12},<br />
                            number={12},<br />
                            year={2018}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://itiis.org/digital-library/manuscript/file/2209/TIIS+Vol+12,+No+12-23.pdf"
                              target="_blank"
                              >http://itiis.org/digital-library/manuscript/file/2209/TIIS+Vol+12,+No+12-23.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <p class="p2">
                              This paper describes two user studies in remote
                              collaboration between two users with a video
                              conferencing system where a remote user can draw
                              annotations on the live video of the local user’s
                              workspace. In these two studies, the local user
                              had the control of the view when sharing the
                              first-person view, but our interfaces provided
                              instant control of the shared view to the remote
                              users. The first study investigates methods for
                              assisting drawing annotations. The auto-freeze
                              method, a novel solution for drawing annotations,
                              is compared to a prior solution (manual freeze
                              method) and a baseline (non-freeze) condition.
                              Results show that both local and remote users
                              preferred the auto-freeze method, which is easy to
                              use and allows users to quickly draw annotations.
                              The manual-freeze method supported precise
                              drawing, but was less preferred because of the
                              need for manual input. The second study explores
                              visual notification for better local user
                              awareness. We propose two designs: the red-box and
                              both-freeze notifications, and compare these to
                              the baseline, no notification condition. Users
                              preferred the less obtrusive red-box notification
                              that improved awareness of when annotations were
                              made by remote users, and had a significantly
                              lower level of interruption compared to the
                              both-freeze condition.
                            </p>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="242"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry.jpg          1086w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry-300x242.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry-768x619.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry-1024x826.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry-181x146.jpg   181w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry-50x40.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Dmitry-93x75.jpg      93w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Potential of Augmented Reality for Computer
                            Science Education
                          </h5>
                          <small
                            >Resnyansky, D., İbili, E., & Billinghurst,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Resnyansky, D., İbili, E., & Billinghurst, M. (2018,
                            December). The Potential of Augmented Reality for
                            Computer Science Education. In 2018 IEEE
                            International Conference on Teaching, Assessment,
                            and Learning for Engineering (TALE) (pp. 350-356).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{resnyansky2018potential,<br />
                            title={The Potential of Augmented Reality for
                            Computer Science Education},<br />
                            author={Resnyansky, Dmitry and {\.I}bili, Emin and
                            Billinghurst, Mark},<br />
                            booktitle={2018 IEEE International Conference on
                            Teaching, Assessment, and Learning for Engineering
                            (TALE)},<br />
                            pages={350--356},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://acikerisim.aksaray.edu.tr/xmlui/bitstream/handle/20.500.12451/3044/resnyansky-dmitry-2019.pdf?sequence=1"
                              target="_blank"
                              >http://acikerisim.aksaray.edu.tr/xmlui/bitstream/handle/20.500.12451/3044/resnyansky-dmitry-2019.pdf?sequence=1</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <em
                              >Innovative approaches in the teaching of computer
                              science are required to address the needs of
                              diverse target audiences, including groups with
                              minimal mathematical background and insufficient
                              abstract thinking ability. In order to tackle this
                              problem, new pedagogical approaches as needed,
                              such as using new technologies such as Virtual and
                              Augmented Reality, Tangible User Interfaces, and
                              3D graphics. This paper draws upon relevant
                              pedagogical and technological literature to
                              determine how Augmented Reality can be more fully
                              applied to computer science education.</em
                            >
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="227"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam.jpg         856w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam-300x227.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam-768x581.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam-193x146.jpg 193w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam-50x38.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-ISMAR-Arindam-99x75.jpg    99w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Effects of Sharing Real-Time Multi-Sensory Heart
                            Rate Feedback in Different Immersive Collaborative
                            Virtual Environments
                          </h5>
                          <small
                            >Dey, A., Chen, H., Zhuang, C., Billinghurst, M., &
                            Lindeman, R. W.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Dey, A., Chen, H., Zhuang, C., Billinghurst, M., &
                            Lindeman, R. W. (2018, October). Effects of Sharing
                            Real-Time Multi-Sensory Heart Rate Feedback in
                            Different Immersive Collaborative Virtual
                            Environments. In 2018 IEEE International Symposium
                            on Mixed and Augmented Reality (ISMAR) (pp.
                            165-173). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2018effects,<br />
                            title={Effects of Sharing Real-Time Multi-Sensory
                            Heart Rate Feedback in Different Immersive
                            Collaborative Virtual Environments},<br />
                            author={Dey, Arindam and Chen, Hao and Zhuang, Chang
                            and Billinghurst, Mark and Lindeman, Robert W},<br />
                            booktitle={2018 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR)},<br />
                            pages={165--173},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8613762"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8613762</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Collaboration is an important application area for
                            virtual reality (VR). However, unlike in the real
                            world, collaboration in VR misses important
                            empathetic cues that can make collaborators aware of
                            each other's emotional states. Providing
                            physiological feedback, such as heart rate or
                            respiration rate, to users in VR has been shown to
                            create a positive impact in single user
                            environments. In this paper, through a rigorous
                            mixed-factorial user experiment, we evaluated how
                            providing heart rate feedback to collaborators
                            influences their collaboration in three different
                            environments requiring different kinds of
                            collaboration. We have found that when provided with
                            real-time heart rate feedback participants felt the
                            presence of the collaborator more and felt that they
                            understood their collaborator's emotional state
                            more. Heart rate feedback also made participants
                            feel more dominant when performing the task. We
                            discuss the implication of this research for
                            collaborative VR environments, provide design
                            guidelines, and directions for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="182"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart.jpg         868w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart-300x182.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart-768x467.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart-240x146.jpg 240w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart-50x30.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Hart-123x75.jpg  123w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Sharing and Augmenting Emotion in Collaborative
                            Mixed Reality
                          </h5>
                          <small
                            >Hart, J. D., Piumsomboon, T., Lee, G., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Hart, J. D., Piumsomboon, T., Lee, G., &
                            Billinghurst, M. (2018, October). Sharing and
                            Augmenting Emotion in Collaborative Mixed Reality.
                            In 2018 IEEE International Symposium on Mixed and
                            Augmented Reality Adjunct (ISMAR-Adjunct) (pp.
                            212-213). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{hart2018sharing,<br />
                            title={Sharing and Augmenting Emotion in
                            Collaborative Mixed Reality},<br />
                            author={Hart, Jonathon D and Piumsomboon, Thammathip
                            and Lee, Gun and Billinghurst, Mark},<br />
                            booktitle={2018 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={212--213},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/332757691_Sharing_and_Augmenting_Emotion_in_Collaborative_Mixed_Reality/links/5ce661cc299bf14d95b20b04/Sharing-and-Augmenting-Emotion-in-Collaborative-Mixed-Reality.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/332757691_Sharing_and_Augmenting_Emotion_in_Collaborative_Mixed_Reality/links/5ce661cc299bf14d95b20b04/Sharing-and-Augmenting-Emotion-in-Collaborative-Mixed-Reality.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present a concept of emotion sharing and
                            augmentation for collaborative mixed-reality. To
                            depict the ideal use case of such system, we give
                            two example scenarios. We describe our prototype
                            system for capturing and augmenting emotion through
                            facial expression, eye-gaze, voice, physiological
                            data and share them through their virtual
                            representation, and discuss on future research
                            directions with potential applications.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="257"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR.jpg         952w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-300x257.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-768x658.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-170x146.jpg 170w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-50x43.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-88x75.jpg    88w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Filtering 3D Shared Surrounding Environments by
                            Social Proximity in AR
                          </h5>
                          <small
                            >Nassani, A., Bai, H., Lee, G., Langlotz, T.,
                            Billinghurst, M., & Lindeman, R. W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Nassani, A., Bai, H., Lee, G., Langlotz, T.,
                            Billinghurst, M., & Lindeman, R. W. (2018, October).
                            Filtering 3D Shared Surrounding Environments by
                            Social Proximity in AR. In 2018 IEEE International
                            Symposium on Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct) (pp. 123-124). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{nassani2018filtering,<br />
                            title={Filtering 3D Shared Surrounding Environments
                            by Social Proximity in AR},<br />
                            author={Nassani, Alaeddin and Bai, Huidong and Lee,
                            Gun and Langlotz, Tobias and Billinghurst, Mark and
                            Lindeman, Robert W},<br />
                            booktitle={2018 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={123--124},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8699336"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8699336</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this poster, we explore the social sharing of
                            surrounding environments on wearable Augmented
                            Reality (AR) devices. In particular, we propose
                            filtering the level of detail of sharing the
                            surrounding environment based on the social
                            proximity between the viewer and the sharer. We test
                            the effect of having the filter (varying levels of
                            detail) on the shared surrounding environment on the
                            sense of privacy from both viewer and sharer
                            perspectives and conducted a pilot study using
                            HoloLens. We report on semi-structured questionnaire
                            results and suggest future directions in the social
                            sharing of surrounding environments.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="275"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang.jpg         850w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang-300x275.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang-768x705.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang-159x146.jpg 159w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang-50x46.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Zhang-82x75.jpg    82w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Effect of AR Based Emotional Interaction Among
                            Personified Physical Objects in Manual Operation
                          </h5>
                          <small
                            >Zhang, L., Ha, W., Bai, X., Chen, Y., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zhang, L., Ha, W., Bai, X., Chen, Y., &
                            Billinghurst, M. (2018, October). The Effect of AR
                            Based Emotional Interaction Among Personified
                            Physical Objects in Manual Operation. In 2018 IEEE
                            International Symposium on Mixed and Augmented
                            Reality Adjunct (ISMAR-Adjunct) (pp. 216-221). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{zhang2018effect,<br />
                            title={The Effect of AR Based Emotional Interaction
                            Among Personified Physical Objects in Manual
                            Operation},<br />
                            author={Zhang, Li and Ha, Weiping and Bai, Xiaoliang
                            and Chen, Yongxing and Billinghurst, Mark},<br />
                            booktitle={2018 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={216--221},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8699229"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8699229</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we explore how Augmented Reality (AR)
                            and anthropomorphism can be used to assign emotions
                            to common physical objects based on their needs. We
                            developed a novel emotional interaction model among
                            personified physical objects so that they could
                            react to other objects by changing virtual facial
                            expressions. To explore the effect of such an
                            emotional interface, we conducted a user study
                            comparing three types of virtual cues shown on the
                            real objects: (1) information only, (2) emotion only
                            and (3) both information and emotional cues. A
                            significant difference was found in task completion
                            time and the quality of work when adding emotional
                            cues to an informational AR-based guiding system.
                            This implies that adding emotion feedback to
                            informational cues may produce better task results
                            than using informational cues alone.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="224"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR.jpg          1044w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR-300x224.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR-768x572.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR-1024x763.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR-196x146.jpg   196w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR-50x37.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Wang-ISMAR-101x75.jpg    101w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Do you know what i mean? an mr-based collaborative
                            platform
                          </h5>
                          <small
                            >Wang, P., Zhang, S., Bai, X., Billinghurst, M., He,
                            W., Zhang, L., Wang, S.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, P., Zhang, S., Bai, X., Billinghurst, M., He,
                            W., Zhang, L., ... & Wang, S. (2018, October). Do
                            you know what i mean? an mr-based collaborative
                            platform. In 2018 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
                            (pp. 77-78). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{wang2018you,<br />
                            title={Do you know what i mean? an mr-based
                            collaborative platform},<br />
                            author={Wang, Peng and Zhang, Shusheng and Bai,
                            Xiaoliang and Billinghurst, Mark and He, Weiping and
                            Zhang, Li and Du, Jiaxiang and Wang, Shuxia},<br />
                            booktitle={2018 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={77--78},<br />
                            year={2018},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8699227"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8699227</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The Mixed Reality (MR) technology can be used to
                            create unique collaborative experiences. In this
                            paper, we propose a new remote collaboration
                            platform using MR and eye-tracking that enables a
                            remote helper to assist a local worker in an
                            assembly task. We present results from research
                            exploring the effect of sharing virtual gaze and
                            annotations cues in an MR-based projector interface
                            for remote collaboration. The key advantage compared
                            to other remote collaborative MR interfaces is that
                            it projects the remote expert's eye gaze into the
                            real worksite to improve co-presence. The prototype
                            system was evaluated with a pilot study comparing
                            two conditions: POINTER and ET (eye-tracker cues).
                            We observed that the task completion performance was
                            better in the ET condition. And that sharing gaze
                            significantly improved the awareness of each other's
                            focus and co-presence.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="229"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame.jpg         854w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame-300x229.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame-768x586.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame-191x146.jpg 191w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame-50x38.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Exergame-98x75.jpg    98w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Enhancing player engagement through game balancing
                            in digitally augmented physical games
                          </h5>
                          <small
                            >Altimira, D., Clarke, J., Lee, G., Billinghurst,
                            M., & Bartneck, C.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Altimira, D., Clarke, J., Lee, G., Billinghurst, M.,
                            & Bartneck, C. (2017). Enhancing player engagement
                            through game balancing in digitally augmented
                            physical games. International Journal of
                            Human-Computer Studies, 103, 35-47.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{altimira2017enhancing,<br />
                            title={Enhancing player engagement through game
                            balancing in digitally augmented physical games},<br />
                            author={Altimira, David and Clarke, Jenny and Lee,
                            Gun and Billinghurst, Mark and Bartneck, Christoph
                            and others},<br />
                            journal={International Journal of Human-Computer
                            Studies},<br />
                            volume={103},<br />
                            pages={35--47},<br />
                            year={2017},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://exertiongameslab.org/wp-content/uploads/2018/02/enhancing_player_ijhcs2017.pdf"
                              target="_blank"
                              >https://exertiongameslab.org/wp-content/uploads/2018/02/enhancing_player_ijhcs2017.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Game balancing can be used to compensate for
                            differences in players' skills, in particular in
                            games where players compete against each other. It
                            can help providing the right level of challenge and
                            hence enhance engagement. However, there is a lack
                            of understanding of game balancing design and how
                            different game adjustments affect player engagement.
                            This understanding is important for the design of
                            balanced physical games. In this paper we report on
                            how altering the game equipment in a digitally
                            augmented table tennis game, such as the table size
                            and bat-head size statically and dynamically, can
                            affect game balancing and player engagement. We
                            found these adjustments enhanced player engagement
                            compared to the no-adjustment condition. The
                            understanding of how the adjustments impacted on
                            player engagement helped us to derive a set of
                            balancing strategies to facilitate engaging game
                            experiences. We hope that this understanding can
                            contribute to improve physical activity experiences
                            and encourage people to get engaged in physical
                            activity.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-300x300.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-300x300.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-768x765.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-147x146.jpg 147w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-75x75.jpg    75w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR-80x80.jpg    80w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-EmotionVR.jpg         962w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Effects of sharing physiological states of players
                            in a collaborative virtual reality gameplay
                          </h5>
                          <small
                            >Dey, A., Piumsomboon, T., Lee, Y., & Billinghurst,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dey, A., Piumsomboon, T., Lee, Y., & Billinghurst,
                            M. (2017, May). Effects of sharing physiological
                            states of players in a collaborative virtual reality
                            gameplay. In Proceedings of the 2017 CHI Conference
                            on Human Factors in Computing Systems (pp.
                            4045-4056). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2017effects,<br />
                            title={Effects of sharing physiological states of
                            players in a collaborative virtual reality
                            gameplay},<br />
                            author={Dey, Arindam and Piumsomboon, Thammathip and
                            Lee, Youngho and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 2017 CHI Conference on
                            Human Factors in Computing Systems},<br />
                            pages={4045--4056},<br />
                            year={2017},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3026028"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3026028</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Interfaces for collaborative tasks, such as
                            multiplayer games can enable more effective and
                            enjoyable collaboration. However, in these systems,
                            the emotional states of the users are often not
                            communicated properly due to their remoteness from
                            one another. In this paper, we investigate the
                            effects of showing emotional states of one
                            collaborator to the other during an immersive
                            Virtual Reality (VR) gameplay experience. We created
                            two collaborative immersive VR games that display
                            the real-time heart-rate of one player to the other.
                            The two different games elicited different emotions,
                            one joyous and the other scary. We tested the
                            effects of visualizing heart-rate feedback in
                            comparison with conditions where such a feedback was
                            absent. The games had significant main effects on
                            the overall emotional experience.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="286"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hessam1.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hessam1.jpg         668w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hessam1-300x286.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hessam1-153x146.jpg 153w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hessam1-50x48.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hessam1-79x75.jpg    79w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            User evaluation of hand gestures for designing an
                            intelligent in-vehicle interface
                          </h5>
                          <small
                            >Jahani, H., Alyamani, H. J., Kavakli, M., Dey, A.,
                            & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Jahani, H., Alyamani, H. J., Kavakli, M., Dey, A., &
                            Billinghurst, M. (2017, May). User evaluation of
                            hand gestures for designing an intelligent
                            in-vehicle interface. In International Conference on
                            Design Science Research in Information System and
                            Technology (pp. 104-121). Springer, Cham.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{jahani2017user,<br />
                            title={User evaluation of hand gestures for
                            designing an intelligent in-vehicle interface},<br />
                            author={Jahani, Hessam and Alyamani, Hasan J and
                            Kavakli, Manolya and Dey, Arindam and Billinghurst,
                            Mark},<br />
                            booktitle={International Conference on Design
                            Science Research in Information System and
                            Technology},<br />
                            pages={104--121},<br />
                            year={2017},<br />
                            organization={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/chapter/10.1007%2F978-3-319-59144-5_7"
                              target="_blank"
                              >https://link.springer.com/chapter/10.1007%2F978-3-319-59144-5_7</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Driving a car is a high cognitive-load task
                            requiring full attention behind the wheel.
                            Intelligent navigation, transportation, and
                            in-vehicle interfaces have introduced a safer and
                            less demanding driving experience. However, there is
                            still a gap for the existing interaction systems to
                            satisfy the requirements of actual user experience.
                            Hand gesture as an interaction medium, is natural
                            and less visually demanding while driving. This
                            paper aims to conduct a user-study with 79
                            participants to validate mid-air gestures for 18
                            major in-vehicle secondary tasks. We have
                            demonstrated a detailed analysis on 900 mid-air
                            gestures investigating preferences of gestures for
                            in-vehicle tasks, their physical affordance, and
                            driving errors. The outcomes demonstrate that
                            employment of mid-air gestures reduces driving
                            errors by up to 50% compared to traditional
                            air-conditioning control. Results can be used for
                            the development of vision-based in-vehicle gestural
                            interfaces.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="189"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-AIED.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-AIED.jpg         706w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-AIED-300x189.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-AIED-232x146.jpg 232w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-AIED-50x31.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-AIED-119x75.jpg  119w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Intelligent Augmented Reality Tutoring for Physical
                            Tasks with Medical Professionals
                          </h5>
                          <small
                            >Almiyad, M. A., Oakden-Rayner, L., Weerasinghe, A.,
                            & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Almiyad, M. A., Oakden-Rayner, L., Weerasinghe, A.,
                            & Billinghurst, M. (2017, June). Intelligent
                            Augmented Reality Tutoring for Physical Tasks with
                            Medical Professionals. In International Conference
                            on Artificial Intelligence in Education (pp.
                            450-454). Springer, Cham.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{almiyad2017intelligent,<br />
                            title={Intelligent Augmented Reality Tutoring for
                            Physical Tasks with Medical Professionals},<br />
                            author={Almiyad, Mohammed A and Oakden-Rayner, Luke
                            and Weerasinghe, Amali and Billinghurst, Mark},<br />
                            booktitle={International Conference on Artificial
                            Intelligence in Education},<br />
                            pages={450--454},<br />
                            year={2017},<br />
                            organization={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/chapter/10.1007/978-3-319-61425-0_38"
                              target="_blank"
                              >https://link.springer.com/chapter/10.1007/978-3-319-61425-0_38</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Percutaneous radiology procedures often require the
                            repeated use of medical radiation in the form of
                            computed tomography (CT) scanning, to demonstrate
                            the position of the needle in the underlying
                            tissues. The angle of the insertion and the distance
                            travelled by the needle inside the patient play a
                            major role in successful procedures, and must be
                            estimated by the practitioner and confirmed
                            periodically by the use of the scanner. Junior
                            radiology trainees, who are already highly trained
                            professionals, currently learn this task
                            “on-the-job” by performing the procedures on real
                            patients with varying levels of guidance. Therefore,
                            we present a novel Augmented Reality (AR)-based
                            system that provides multiple layers of intuitive
                            and adaptive feedback to assist junior radiologists
                            in achieving competency in image-guided procedures.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="293"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Outdoor.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Outdoor.jpg         678w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Outdoor-300x293.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Outdoor-150x146.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Outdoor-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Outdoor-77x75.jpg    77w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Augmented reality entertainment: taking gaming out
                            of the box
                          </h5>
                          <small
                            >Von Itzstein, G. S., Billinghurst, M., Smith, R.
                            T., & Thomas, B. H.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Von Itzstein, G. S., Billinghurst, M., Smith, R. T.,
                            & Thomas, B. H. (2017). Augmented reality
                            entertainment: taking gaming out of the box.
                            Encyclopedia of Computer Graphics and Games, 1-9.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{von2017augmented,<br />
                            title={Augmented reality entertainment: taking
                            gaming out of the box},<br />
                            author={Von Itzstein, G Stewart and Billinghurst,
                            Mark and Smith, Ross T and Thomas, Bruce H},<br />
                            journal={Encyclopedia of Computer Graphics and
                            Games},<br />
                            pages={1--9},<br />
                            year={2017},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/content/pdf/10.1007/978-3-319-08234-9_81-1.pdf"
                              target="_blank"
                              >https://link.springer.com/content/pdf/10.1007/978-3-319-08234-9_81-1.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this chapter, an overview of using AR for gaming
                            and entertainment is provided, one of the most
                            popular application areas. There are many possible
                            AR entertainment applications. For example, the
                            Pokémon Go mobile phone game has an AR element that
                            allows people to see virtual Pokémon to appear in
                            the live camera view, seemingly inhabiting the real
                            world. In this case, Pokémon Go satisfies Azuma’s
                            three AR criteria: the virtual Pokémon appears in
                            the real world, the user can interact with them, and
                            they appear fixed in space.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="209"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking.jpg         948w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking-300x209.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking-768x536.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking-209x146.jpg 209w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking-50x35.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GazeTracking-107x75.jpg  107w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Estimating Gaze Depth Using Multi-Layer Perceptron
                          </h5>
                          <small></small>
                          <p style="font-style: italic; font-size: 12px">
                            Lee, Y., Shin, C., Plopski, A., Itoh, Y.,
                            Piumsomboon, T., Dey, A., ... & Billinghurst, M.
                            (2017, June). Estimating Gaze Depth Using
                            Multi-Layer Perceptron. In 2017 International
                            Symposium on Ubiquitous Virtual Reality (ISUVR) (pp.
                            26-29). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{lee2017estimating,<br />
                            title={Estimating Gaze Depth Using Multi-Layer
                            Perceptron},<br />
                            author={Lee, Youngho and Shin, Choonsung and
                            Plopski, Alexander and Itoh, Yuta and Piumsomboon,
                            Thammathip and Dey, Arindam and Lee, Gun and Kim,
                            Seungwon and Billinghurst, Mark},<br />
                            booktitle={2017 International Symposium on
                            Ubiquitous Virtual Reality (ISUVR)},<br />
                            pages={26--29},<br />
                            year={2017},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://imd.naist.jp/imdweb/pub/plopski_isuvr17/paper.pdf"
                              target="_blank"
                              >http://imd.naist.jp/imdweb/pub/plopski_isuvr17/paper.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper we describe a new method for
                            determining gaze depth in a head mounted
                            eye-tracker. Eyetrackers are being incorporated into
                            head mounted displays (HMDs), and eye-gaze is being
                            used for interaction in Virtual and Augmented
                            Reality. For some interaction methods, it is
                            important to accurately measure the x- and
                            y-direction of the eye-gaze and especially the focal
                            depth information. Generally, eye tracking
                            technology has a high accuracy in x- and
                            y-directions, but not in depth. We used a binocular
                            gaze tracker with two eye cameras, and the gaze
                            vector was input to an MLP neural network for
                            training and estimation. For the performance
                            evaluation, data was obtained from 13 people gazing
                            at fixed points at distances from 1m to 5m. The gaze
                            classification into fixed distances produced an
                            average classification error of nearly 10%, and an
                            average error distance of 0.42m. This is sufficient
                            for some Augmented Reality applications, but more
                            research is needed to provide an estimate of a
                            user’s gaze moving in continuous space.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="286"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Empathic.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Empathic.jpg         614w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Empathic-300x286.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Empathic-153x146.jpg 153w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Empathic-50x48.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Empathic-79x75.jpg    79w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Empathic mixed reality: Sharing what you feel and
                            interacting with what you see
                          </h5>
                          <small
                            >Piumsomboon, T., Lee, Y., Lee, G. A., Dey, A., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Piumsomboon, T., Lee, Y., Lee, G. A., Dey, A., &
                            Billinghurst, M. (2017, June). Empathic mixed
                            reality: Sharing what you feel and interacting with
                            what you see. In 2017 International Symposium on
                            Ubiquitous Virtual Reality (ISUVR) (pp. 38-41).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{piumsomboon2017empathic,<br />
                            title={Empathic mixed reality: Sharing what you feel
                            and interacting with what you see},<br />
                            author={Piumsomboon, Thammathip and Lee, Youngho and
                            Lee, Gun A and Dey, Arindam and Billinghurst,
                            Mark},<br />
                            booktitle={2017 International Symposium on
                            Ubiquitous Virtual Reality (ISUVR)},<br />
                            pages={38--41},<br />
                            year={2017},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/318665483_Empathic_Mixed_Reality_Sharing_What_You_Feel_and_Interacting_with_What_You_See/links/5b1bf957a6fdcca67b681ce7/Empathic-Mixed-Reality-Sharing-What-You-Feel-and-Interacting-with-What-You-See.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/318665483_Empathic_Mixed_Reality_Sharing_What_You_Feel_and_Interacting_with_What_You_See/links/5b1bf957a6fdcca67b681ce7/Empathic-Mixed-Reality-Sharing-What-You-Feel-and-Interacting-with-What-You-See.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Empathic Computing is a research field that aims to
                            use technology to create deeper shared understanding
                            or empathy between people. At the same time, Mixed
                            Reality (MR) technology provides an immersive
                            experience that can make an ideal interface for
                            collaboration. In this paper, we present some of our
                            research into how MR technology can be applied to
                            creating Empathic Computing experiences. This
                            includes exploring how to share gaze in a remote
                            collaboration between Augmented Reality (AR) and
                            Virtual Reality (VR) environments, using
                            physiological signals to enhance collaborative VR,
                            and supporting interaction through eye-gaze in VR.
                            Early outcomes indicate that as we design
                            collaborative interfaces to enhance empathy between
                            people, this could also benefit the personal
                            experience of the individual interacting with the
                            interface.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="257"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1.jpg         952w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1-300x257.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1-768x658.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1-170x146.jpg 170w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1-50x43.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2018-Alaeddin-ISMAR-1-88x75.jpg    88w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Social AR Continuum: Concept and User Study
                          </h5>
                          <small
                            >Nassani, A., Lee, G., Billinghurst, M., Langlotz,
                            T., Hoermann, S., & Lindeman, R. W.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Nassani, A., Lee, G., Billinghurst, M., Langlotz,
                            T., Hoermann, S., & Lindeman, R. W. (2017, October).
                            [POSTER] The Social AR Continuum: Concept and User
                            Study. In 2017 IEEE International Symposium on Mixed
                            and Augmented Reality (ISMAR-Adjunct) (pp. 7-8).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{nassani2017poster,<br />
                            title={[POSTER] The Social AR Continuum: Concept and
                            User Study},<br />
                            author={Nassani, Alaeddin and Lee, Gun and
                            Billinghurst, Mark and Langlotz, Tobias and
                            Hoermann, Simon and Lindeman, Robert W},<br />
                            booktitle={2017 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR-Adjunct)},<br />
                            pages={7--8},<br />
                            year={2017},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8088437"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8088437</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this poster, we describe The Social AR Continuum,
                            a space that encompasses different dimensions of
                            Augmented Reality (AR) for sharing social
                            experiences. We explore various dimensions, discuss
                            options for each dimension, and brainstorm possible
                            scenarios where these options might be useful. We
                            describe a prototype interface using the contact
                            placement dimension, and report on feedback from
                            potential users which supports its usefulness for
                            visualising social contacts. Based on this concept
                            work, we suggest user studies in the social AR
                            space, and give insights into future directions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="216"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze.jpg          1322w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze-300x216.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze-768x554.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze-1024x739.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze-202x146.jpg   202w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze-50x36.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-GunGaze-104x75.jpg    104w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Mutually Shared Gaze in Augmented Video Conference
                          </h5>
                          <small
                            >Lee, G., Kim, S., Lee, Y., Dey, A., Piumsomboon,
                            T., Norman, M., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Lee, G., Kim, S., Lee, Y., Dey, A., Piumsomboon, T.,
                            Norman, M., & Billinghurst, M. (2017, October).
                            Mutually Shared Gaze in Augmented Video Conference.
                            In Adjunct Proceedings of the 2017 IEEE
                            International Symposium on Mixed and Augmented
                            Reality, ISMAR-Adjunct 2017 (pp. 79-80). Institute
                            of Electrical and Electronics Engineers Inc..
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{lee2017mutually,<br />
                            title={Mutually Shared Gaze in Augmented Video
                            Conference},<br />
                            author={Lee, Gun and Kim, Seungwon and Lee, Youngho
                            and Dey, Arindam and Piumsomboon, Thammatip and
                            Norman, Mitchell and Billinghurst, Mark},<br />
                            booktitle={Adjunct Proceedings of the 2017 IEEE
                            International Symposium on Mixed and Augmented
                            Reality, ISMAR-Adjunct 2017},<br />
                            pages={79--80},<br />
                            year={2017},<br />
                            organization={Institute of Electrical and
                            Electronics Engineers Inc.}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/document/8088453"
                              target="_blank"
                              >https://ieeexplore.ieee.org/document/8088453</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmenting video conference with additional visual
                            cues has been studied to improve remote
                            collaboration. A common setup is a person wearing a
                            head-mounted display (HMD) and camera sharing her
                            view of the workspace with a remote collaborator and
                            getting assistance on a real-world task. While this
                            configuration has been extensively studied, there
                            has been little research on how sharing gaze cues
                            might affect the collaboration. This research
                            investigates how sharing gaze in both directions
                            between a local worker and remote helper affects the
                            collaboration and communication. We developed a
                            prototype system that shares the eye gaze of both
                            users, and conducted a user study. Preliminary
                            results showed that sharing gaze significantly
                            improves the awareness of each other's focus, hence
                            improving collaboration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="206"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen.jpg          1032w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen-300x206.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen-768x527.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen-1024x703.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen-213x146.jpg   213w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen-50x34.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Chen-109x75.jpg    109w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The effect of user embodiment in AV cinematic
                            experience
                          </h5>
                          <small
                            >Chen, J., Lee, G., Billinghurst, M., Lindeman, R.
                            W., and Bartneck, C.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Chen, J., Lee, G., Billinghurst, M., Lindeman, R.
                            W., & Bartneck, C. (2017). The effect of user
                            embodiment in AV cinematic experience.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{chen2017effect,<br />
                            title={The effect of user embodiment in AV cinematic
                            experience},<br />
                            author={Chen, Joshua and Lee, Gun and Billinghurst,
                            Mark and Lindeman, Robert W and Bartneck,
                            Christoph},<br />
                            year={2017}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ir.canterbury.ac.nz/bitstream/handle/10092/15405/icat-egve2017-JoshuaChen.pdf?sequence=2"
                              target="_blank"
                              >https://ir.canterbury.ac.nz/bitstream/handle/10092/15405/icat-egve2017-JoshuaChen.pdf?sequence=2</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual Reality (VR) is becoming a popular medium
                            for viewing immersive cinematic experiences using
                            360◦ panoramic movies and head mounted displays.
                            There are previous research on user embodiment in
                            real-time rendered VR, but not in relation to
                            cinematic VR based on 360 panoramic video. In this
                            paper we explore the effects of introducing the
                            user’s real body into cinematic VR experiences. We
                            conducted a study evaluating how the type of movie
                            and user embodiment affects the sense of presence
                            and user engagement. We found that when participants
                            were able to see their own body in the VR movie,
                            there was significant increase in the sense of
                            Presence, yet user engagement was not significantly
                            affected. We discuss on the implications of the
                            results and how it can be expanded in the future.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="222"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho.jpg          1128w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho-300x222.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho-768x569.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho-1024x759.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho-197x146.jpg   197w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho-50x37.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Youngho-101x75.jpg    101w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A gaze-depth estimation technique with an implicit
                            and continuous data acquisition for OST-HMDs
                          </h5>
                          <small
                            >Lee, Y., Piumsomboon, T., Ens, B., Lee, G., Dey,
                            A., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Lee, Y., Piumsomboon, T., Ens, B., Lee, G., Dey, A.,
                            & Billinghurst, M. (2017, November). A gaze-depth
                            estimation technique with an implicit and continuous
                            data acquisition for OST-HMDs. In Proceedings of the
                            27th International Conference on Artificial Reality
                            and Telexistence and 22nd Eurographics Symposium on
                            Virtual Environments: Posters and Demos (pp. 1-2).
                            Eurographics Association.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{lee2017gaze,<br />
                            title={A gaze-depth estimation technique with an
                            implicit and continuous data acquisition for
                            OST-HMDs},<br />
                            author={Lee, Youngho and Piumsomboon, Thammathip and
                            Ens, Barrett and Lee, Gun and Dey, Arindam and
                            Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 27th International
                            Conference on Artificial Reality and Telexistence
                            and 22nd Eurographics Symposium on Virtual
                            Environments: Posters and Demos},<br />
                            pages={1--2},<br />
                            year={2017},<br />
                            organization={Eurographics Association}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3298874"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3298874</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <p class="p1">
                              The rapid developement of machine learning
                              algorithms can be leveraged for potential software
                              solutions in many domains including techniques for
                              depth estimation of human eye gaze. In this paper,
                              we propose an implicit and continuous data
                              acquisition method for 3D gaze depth estimation
                              for an optical see-Through head mounted display
                              (OST-HMD) equipped with an eye tracker. Our method
                              constantly monitoring and generating user gaze
                              data for training our machine learning algorithm.
                              The gaze data acquired through the eye-tracker
                              include the inter-pupillary distance (IPD) and the
                              gaze distance to the real andvirtual target for
                              each eye.
                            </p>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="222"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation.jpg         784w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation-300x222.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation-768x568.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation-197x146.jpg 197w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Dilation-101x75.jpg  101w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploring pupil dilation in emotional virtual
                            reality environments.
                          </h5>
                          <small
                            >Chen, H., Dey, A., Billinghurst, M., & Lindeman, R.
                            W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Chen, H., Dey, A., Billinghurst, M., & Lindeman, R.
                            W. (2017, November). Exploring pupil dilation in
                            emotional virtual reality environments. In
                            Proceedings of the 27th International Conference on
                            Artificial Reality and Telexistence and 22nd
                            Eurographics Symposium on Virtual Environments (pp.
                            169-176). Eurographics Association.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{chen2017exploring,<br />
                            title={Exploring pupil dilation in emotional virtual
                            reality environments},<br />
                            author={Chen, Hao and Dey, Arindam and Billinghurst,
                            Mark and Lindeman, Robert W},<br />
                            booktitle={Proceedings of the 27th International
                            Conference on Artificial Reality and Telexistence
                            and 22nd Eurographics Symposium on Virtual
                            Environments},<br />
                            pages={169--176},<br />
                            year={2017},<br />
                            organization={Eurographics Association}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ir.canterbury.ac.nz/bitstream/handle/10092/15404/icat-egve2017-HaoChen.pdf?sequence=2"
                              target="_blank"
                              >https://ir.canterbury.ac.nz/bitstream/handle/10092/15404/icat-egve2017-HaoChen.pdf?sequence=2</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Previous investigations have shown that pupil
                            dilation can be affected by emotive pictures, audio
                            clips, and videos. In this paper, we explore how
                            emotive Virtual Reality (VR) content can also cause
                            pupil dilation. VR has been shown to be able to
                            evoke negative and positive arousal in users when
                            they are immersed in different virtual scenes. In
                            our research, VR scenes were used as emotional
                            triggers. Five emotional VR scenes were designed in
                            our study and each scene had five emotion segments;
                            happiness, fear, anxiety, sadness, and disgust. When
                            participants experienced the VR scenes, their pupil
                            dilation and the brightness in the headset were
                            captured. We found that both the negative and
                            positive emotion segments produced pupil dilation in
                            the VR environments. We also explored the effect of
                            showing heart beat cues to the users, and if this
                            could cause difference in pupil dilation. In our
                            study, three different heart beat cues were shown to
                            users using a combination of three channels; haptic,
                            audio, and visual. The results showed that the
                            haptic-visual cue caused the most significant pupil
                            dilation change from the baseline.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="222"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon.jpg         982w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon-300x222.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon-768x569.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon-197x146.jpg 197w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Hyungon-101x75.jpg  101w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Collaborative View Configurations for Multi-user
                            Interaction with a Wall-size Display
                          </h5>
                          <small
                            >Kim, H., Kim, Y., Lee, G., Billinghurst, M., &
                            Bartneck, C.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kim, H., Kim, Y., Lee, G., Billinghurst, M., &
                            Bartneck, C. (2017, November). Collaborative view
                            configurations for multi-user interaction with a
                            wall-size display. In Proceedings of the 27th
                            International Conference on Artificial Reality and
                            Telexistence and 22nd Eurographics Symposium on
                            Virtual Environments (pp. 189-196). Eurographics
                            Association.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{kim2017collaborative,<br />
                            title={Collaborative view configurations for
                            multi-user interaction with a wall-size display},<br />
                            author={Kim, Hyungon and Kim, Yeongmi and Lee, Gun
                            and Billinghurst, Mark and Bartneck, Christoph},<br />
                            booktitle={Proceedings of the 27th International
                            Conference on Artificial Reality and Telexistence
                            and 22nd Eurographics Symposium on Virtual
                            Environments},<br />
                            pages={189--196},<br />
                            year={2017},<br />
                            organization={Eurographics Association}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://www.bartneck.de/publications/2017/collaborativeViewConfigurations/kimBartneck2017.pdf"
                              target="_blank"
                              >http://www.bartneck.de/publications/2017/collaborativeViewConfigurations/kimBartneck2017.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper explores the effects of different
                            collaborative view configuration on face-to-face
                            collaboration using a wall-size display and the
                            relationship between view configuration and
                            multi-user interaction. Three different view
                            configurations (shared view, split screen, and split
                            screen with navigation information) for multi-user
                            collaboration with a wall-size display were
                            introduced and evaluated in a user study. From the
                            experiment results, several insights for designing a
                            virtual environment with a wall-size display were
                            discussed. The shared view configuration does not
                            disturb collaboration despite control conflict and
                            can provide an effective collaboration. The split
                            screen view configuration can provide independent
                            collaboration while it can take users’ attention.
                            The navigation information can reduce the
                            interaction required for the navigational task while
                            an overall interaction performance may not increase.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="246"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gesture.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gesture.jpg         468w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gesture-300x246.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gesture-178x146.jpg 178w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gesture-50x41.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gesture-91x75.jpg    91w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Towards Optimization of Mid-air Gestures for
                            In-vehicle Interactions
                          </h5>
                          <small
                            >Hessam, J. F., Zancanaro, M., Kavakli, M., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Hessam, J. F., Zancanaro, M., Kavakli, M., &
                            Billinghurst, M. (2017, November). Towards
                            optimization of mid-air gestures for in-vehicle
                            interactions. In Proceedings of the 29th Australian
                            Conference on Computer-Human Interaction (pp.
                            126-134). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{hessam2017towards,<br />
                            title={Towards optimization of mid-air gestures for
                            in-vehicle interactions},<br />
                            author={Hessam, Jahani F and Zancanaro, Massimo and
                            Kavakli, Manolya and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 29th Australian
                            Conference on Computer-Human Interaction},<br />
                            pages={126--134},<br />
                            year={2017},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3152785"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3152785</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            A mid-air gesture-based interface could provide a
                            less cumbersome in-vehicle interface for a safer
                            driving experience. Despite the recent developments
                            in gesture-driven technologies facilitating the
                            multi-touch and mid-air gestures, interface safety
                            requirements as well as an evaluation of gesture
                            characteristics and functions, need to be explored.
                            This paper describes an optimization study on the
                            previously developed GestDrive gesture vocabulary
                            for in-vehicle secondary tasks. We investigate
                            mid-air gestures and secondary tasks, their
                            correlation, confusions, unintentional inputs and
                            consequential safety risks. Building upon a
                            statistical analysis, the results provide an
                            optimized taxonomy break-down for a user-centered
                            gestural interface design which considers user
                            preferences, requirements, performance, and safety
                            issues.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="204"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli.jpg         722w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli-300x204.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli-215x146.jpg 215w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli-50x34.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli-111x75.jpg  111w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploring Mixed-Scale Gesture Interaction
                          </h5>
                          <small
                            >Ens, B., Quigley, A. J., Yeo, H. S., Irani, P.,
                            Piumsomboon, T., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Ens, B., Quigley, A. J., Yeo, H. S., Irani, P.,
                            Piumsomboon, T., & Billinghurst, M. (2017).
                            Exploring mixed-scale gesture interaction. SA'17
                            SIGGRAPH Asia 2017 Posters.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ens2017exploring,<br />
                            title={Exploring mixed-scale gesture
                            interaction},<br />
                            author={Ens, Barrett and Quigley, Aaron John and
                            Yeo, Hui Shyong and Irani, Pourang and Piumsomboon,
                            Thammathip and Billinghurst, Mark},<br />
                            journal={SA'17 SIGGRAPH Asia 2017 Posters},<br />
                            year={2017},<br />
                            publisher={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/12967/Ens_2017_Exploring_mixed_scale_preprint_SA17_AAM.pdf?sequence=1&isAllowed=y"
                              target="_blank"
                              >https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/12967/Ens_2017_Exploring_mixed_scale_preprint_SA17_AAM.pdf?sequence=1&isAllowed=y</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper presents ongoing work toward a design
                            exploration for combining microgestures with other
                            types of gestures within the greater lexicon of
                            gestures for computer interaction. We describe three
                            prototype applications that show various facets of
                            this multi-dimensional design space. These
                            applications portray various tasks on a Hololens
                            Augmented Reality display, using different
                            combinations of wearable sensors.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="204"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2.jpg         904w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2-300x204.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2-768x523.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2-214x146.jpg 214w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2-50x34.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Soli2-110x75.jpg  110w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Multi-Scale Gestural Interaction for Augmented
                            Reality
                          </h5>
                          <small
                            >Ens, B., Quigley, A., Yeo, H. S., Irani, P., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Ens, B., Quigley, A., Yeo, H. S., Irani, P., &
                            Billinghurst, M. (2017, November). Multi-scale
                            gestural interaction for augmented reality. In
                            SIGGRAPH Asia 2017 Mobile Graphics & Interactive
                            Applications (p. 11). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{ens2017multi,<br />
                            title={Multi-scale gestural interaction for
                            augmented reality},<br />
                            author={Ens, Barrett and Quigley, Aaron and Yeo,
                            Hui-Shyong and Irani, Pourang and Billinghurst,
                            Mark},<br />
                            booktitle={SIGGRAPH Asia 2017 Mobile Graphics \&
                            Interactive Applications},<br />
                            pages={11},<br />
                            year={2017},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/12966/Ens_2017_Multiscale_gestures_preprint_SA17_AAM.pdf?sequence=1"
                              target="_blank"
                              >https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/12966/Ens_2017_Multiscale_gestures_preprint_SA17_AAM.pdf?sequence=1</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <p class="p1">
                              We present a multi-scale gestural interface for
                              augmented reality applications. With virtual
                              objects, gestural interactions such as pointing
                              and grasping can be convenient and intuitive,
                              however they are imprecise, socially awkward, and
                              susceptible to fatigue. Our prototype application
                              uses multiple sensors to detect gestures from both
                              arm and hand motions (macro-scale), and finger
                              gestures (micro-scale). Micro-gestures can provide
                              precise input through a belt-worn sensor
                              configuration, with the hand in a relaxed posture.
                              We present an application that combines direct
                              manipulation with microgestures for precise
                              interaction, beyond the capabilities of direct
                              manipulation alone.
                            </p>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="215"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao.jpg         794w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao-300x215.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao-768x551.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao-203x146.jpg 203w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao-50x36.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Gao-104x75.jpg  104w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Static local environment capturing and sharing for
                            MR remote collaboration
                          </h5>
                          <small
                            >Gao, L., Bai, H., Lindeman, R., & Billinghurst,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gao, L., Bai, H., Lindeman, R., & Billinghurst, M.
                            (2017, November). Static local environment capturing
                            and sharing for MR remote collaboration. In SIGGRAPH
                            Asia 2017 Mobile Graphics & Interactive Applications
                            (p. 17). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gao2017static,<br />
                            title={Static local environment capturing and
                            sharing for MR remote collaboration},<br />
                            author={Gao, Lei and Bai, Huidong and Lindeman, Rob
                            and Billinghurst, Mark},<br />
                            booktitle={SIGGRAPH Asia 2017 Mobile Graphics \&
                            Interactive Applications},<br />
                            pages={17},<br />
                            year={2017},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3139204"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3139204</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present a Mixed Reality (MR) system that supports
                            entire scene capturing of the local physical work
                            environment for remote collaboration in a
                            large-scale workspace. By integrating the key-frames
                            captured with external depth sensor as one single 3D
                            point-cloud data set, our system could reconstruct
                            the entire local physical workspace into the VR
                            world. In this case, the remote helper could observe
                            the local scene independently from the local user's
                            current head and camera position, and provide
                            gesture guiding information even before the local
                            user staring at the target object. We conducted a
                            pilot study to evaluate the usability of the system
                            by comparing it with our previous oriented view
                            system which only sharing the current camera view
                            together with the real-time head orientation data.
                            Our results indicate that this entire scene
                            capturing and sharing system could significantly
                            increase the remote helper's spatial awareness of
                            the local work environment, especially in a
                            large-scale workspace, and gain an overwhelming user
                            preference (80%) than previous system.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="223"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-MGIA1.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-MGIA1.jpg         674w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-MGIA1-300x223.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-MGIA1-197x146.jpg 197w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-MGIA1-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-MGIA1-101x75.jpg  101w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploring enhancements for remote mixed reality
                            collaboration
                          </h5>
                          <small
                            >Piumsomboon, T., Day, A., Ens, B., Lee, Y., Lee,
                            G., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Piumsomboon, T., Day, A., Ens, B., Lee, Y., Lee, G.,
                            & Billinghurst, M. (2017, November). Exploring
                            enhancements for remote mixed reality collaboration.
                            In SIGGRAPH Asia 2017 Mobile Graphics & Interactive
                            Applications (p. 16). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{piumsomboon2017exploring,<br />
                            title={Exploring enhancements for remote mixed
                            reality collaboration},<br />
                            author={Piumsomboon, Thammathip and Day, Arindam and
                            Ens, Barrett and Lee, Youngho and Lee, Gun and
                            Billinghurst, Mark},<br />
                            booktitle={SIGGRAPH Asia 2017 Mobile Graphics \&
                            Interactive Applications},<br />
                            pages={16},<br />
                            year={2017},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/321405854_Exploring_enhancements_for_remote_mixed_reality_collaboration/links/5b0e42a30f7e9b1ed701436a/Exploring-enhancements-for-remote-mixed-reality-collaboration.pdf"
                              target="_blank"
                              >https://www.researchgate.net/profile/Thammathip_Piumsomboon/publication/321405854_Exploring_enhancements_for_remote_mixed_reality_collaboration/links/5b0e42a30f7e9b1ed701436a/Exploring-enhancements-for-remote-mixed-reality-collaboration.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we explore techniques for enhancing
                            remote Mixed Reality (MR) collaboration in terms of
                            communication and interaction. We created CoVAR, a
                            MR system for remote collaboration between an
                            Augmented Reality (AR) and Augmented Virtuality (AV)
                            users. Awareness cues and AV-Snap-to-AR interface
                            were proposed for enhancing communication.
                            Collaborative natural interaction, and
                            AV-User-Body-Scaling were implemented for enhancing
                            interaction. We conducted an exploratory study
                            examining the awareness cues and the collaborative
                            gaze, and the results showed the benefits of the
                            proposed techniques for enhancing communication and
                            interaction.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="241"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social.jpg         834w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social-300x241.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social-768x617.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social-182x146.jpg 182w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social-50x40.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2017-Social-93x75.jpg    93w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            AR social continuum: representing social contacts
                          </h5>
                          <small
                            >Nassani, A., Lee, G., Billinghurst, M., Langlotz,
                            T., & Lindeman, R. W.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Nassani, A., Lee, G., Billinghurst, M., Langlotz,
                            T., & Lindeman, R. W. (2017, November). AR social
                            continuum: representing social contacts. In SIGGRAPH
                            Asia 2017 Mobile Graphics & Interactive Applications
                            (p. 6). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{nassani2017ar,<br />
                            title={AR social continuum: representing social
                            contacts},<br />
                            author={Nassani, Alaeddin and Lee, Gun and
                            Billinghurst, Mark and Langlotz, Tobias and
                            Lindeman, Robert W},<br />
                            booktitle={SIGGRAPH Asia 2017 Mobile Graphics \&
                            Interactive Applications},<br />
                            pages={6},<br />
                            year={2017},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3132812"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3132812</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            One of the key problems with representing social
                            networks in Augmented Reality (AR) is how to
                            differentiate between contacts. In this paper we
                            explore how visual and spatial cues based on social
                            relationships can be used to represent contacts in
                            social AR applications, making it easier to
                            distinguish between them. Previous implementations
                            of social AR have been mostly focusing on location
                            based visualization with no focus on the social
                            relationship to the user. In contrast, we explore
                            how to visualise social relationships in mobile AR
                            environments using proximity and visual fidelity
                            filters. We ran a focus group to explore different
                            options for representing social contacts in a mobile
                            an AR application. We also conducted a user study to
                            test a head-worn AR prototype using proximity and
                            visual fidelity filters. We found out that filtering
                            social contacts on wearable AR is preferred and
                            useful. We discuss the results of focus group and
                            the user study, and provide insights into directions
                            for future work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="200"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen.jpg         928w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen-300x200.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen-768x513.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen-219x146.jpg 219w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen-50x33.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Wen-112x75.jpg  112w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            If Reality Bites, Bite Back Virtually: Simulating
                            Perfection in Augmented Reality Tracking
                          </h5>
                          <small
                            >Wen, J., Helton, W. S., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wen, J., Helton, W. S., & Billinghurst, M. (2015,
                            March). If Reality Bites, Bite Back Virtually:
                            Simulating Perfection in Augmented Reality Tracking.
                            In Proceedings of the 14th Annual ACM SIGCHI_NZ
                            conference on Computer-Human Interaction (p. 3).
                            ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{wen2015if,<br />
                            title={If Reality Bites, Bite Back Virtually:
                            Simulating Perfection in Augmented Reality
                            Tracking},<br />
                            author={Wen, James and Helton, William S and
                            Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 14th Annual ACM
                            SIGCHI\_NZ conference on Computer-Human
                            Interaction},<br />
                            pages={3},<br />
                            year={2015},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=2542246"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=2542246</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) on smart phones can be used
                            to overlay virtual tags in the real world to show
                            points of interest that people may want to visit.
                            However, field tests have failed to validate the
                            belief that AR-based tools would outperform
                            map-based tools for such pedestrian navigation
                            tasks. Assuming this is due to inaccuracies in
                            consumer GPS tracking used in handheld AR, we
                            created a simulated environment that provided
                            perfect tracking for AR and conducted experiments
                            based on real world navigation studies. We measured
                            time-on-task performance for guided traversals on
                            both desktop and head-mounted display systems and
                            found that accurate tracking did validate the
                            superior performance of AR-based navigation tools.
                            We also measured performance for unguided recall
                            traversals of previously traversed paths in order to
                            investigate into how navigation tools impact upon
                            route memory.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="181"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive.jpg         986w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive-300x181.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive-768x463.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive-242x146.jpg 242w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive-50x30.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Adaptive-124x75.jpg  124w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Adaptive Interpupillary Distance Adjustment for
                            Stereoscopic 3D Visualization.
                          </h5>
                          <small>Kim, H., Lee, G., & Billinghurst, M. </small>
                          <p style="font-style: italic; font-size: 12px">
                            Kim, H., Lee, G., & Billinghurst, M. (2015, March).
                            Adaptive Interpupillary Distance Adjustment for
                            Stereoscopic 3D Visualization. In Proceedings of the
                            14th Annual ACM SIGCHI_NZ conference on
                            Computer-Human Interaction (p. 2). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{kim2015adaptive,<br />
                            title={Adaptive Interpupillary Distance Adjustment
                            for Stereoscopic 3D Visualization},<br />
                            author={Kim, Hyungon and Lee, Gun and Billinghurst,
                            Mark},<br />
                            booktitle={Proceedings of the 14th Annual ACM
                            SIGCHI\_NZ conference on Computer-Human
                            Interaction},<br />
                            pages={2},<br />
                            year={2015},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ir.canterbury.ac.nz/bitstream/handle/10092/12239/12648701_chinz2013-kim.pdf?sequence=1"
                              target="_blank"
                              >https://ir.canterbury.ac.nz/bitstream/handle/10092/12239/12648701_chinz2013-kim.pdf?sequence=1</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Stereoscopic visualization creates illusions of
                            depth through disparity between the images shown to
                            left and right eyes of the viewer. While the
                            stereoscopic visualization is widely adopted in
                            immersive visualization systems to improve user
                            experience, it can also cause visual discomfort if
                            the stereoscopic viewing parameters are not adjusted
                            appropriately. These parameters are usually manually
                            adjusted based on human factors and empirical
                            knowledge of the developer or even the user.
                            However, scenes with dynamic change in scale and
                            configuration can lead into continuous adjustment of
                            these parameters while viewing. In this paper, we
                            propose a method to adjust the interpupillary
                            distance adaptively and automatically according to
                            the configuration of the 3D scene, so that the
                            visualized scene can maintain sufficient stereo
                            effect while reducing visual discomfort.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="224"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield.jpg         934w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield-300x224.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield-768x574.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield-195x146.jpg 195w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Westerfield-100x75.jpg  100w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Intelligent Augmented Reality Training for
                            Motherboard Assembly
                          </h5>
                          <small
                            >Westerfield, G., Mitrovic, A., & Billinghurst,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Westerfield, G., Mitrovic, A., & Billinghurst, M.
                            (2015). Intelligent augmented reality training for
                            motherboard assembly. International Journal of
                            Artificial Intelligence in Education, 25(1),
                            157-172.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{westerfield2015intelligent,<br />
                            title={Intelligent augmented reality training for
                            motherboard assembly},<br />
                            author={Westerfield, Giles and Mitrovic, Antonija
                            and Billinghurst, Mark},<br />
                            journal={International Journal of Artificial
                            Intelligence in Education},<br />
                            volume={25},<br />
                            number={1},<br />
                            pages={157--172},<br />
                            year={2015},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s40593-014-0032-x"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s40593-014-0032-x</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We investigate the combination of Augmented Reality
                            (AR) with Intelligent Tutoring Systems (ITS) to
                            assist with training for manual assembly tasks. Our
                            approach combines AR graphics with adaptive guidance
                            from the ITS to provide a more effective learning
                            experience. We have developed a modular software
                            framework for intelligent AR training systems, and a
                            prototype based on this framework that teaches
                            novice users how to assemble a computer motherboard.
                            An evaluation found that our intelligent AR system
                            improved test scores by 25 % and that task
                            performance was 30 % faster compared to the same AR
                            training system without intelligent support. We
                            conclude that using an intelligent AR tutor can
                            significantly improve learning compared to more
                            traditional AR training.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="294"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Gun.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Gun.jpg         734w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Gun-300x294.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Gun-149x146.jpg 149w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Gun-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Gun-76x75.jpg    76w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            User Defined Gestures for Augmented Virtual Mirrors:
                            A Guessability Study
                          </h5>
                          <small
                            >Lee, G. A., Wong, J., Park, H. S., Choi, J. S.,
                            Park, C. J., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Lee, G. A., Wong, J., Park, H. S., Choi, J. S.,
                            Park, C. J., & Billinghurst, M. (2015, April). User
                            defined gestures for augmented virtual mirrors: a
                            guessability study. In Proceedings of the 33rd
                            Annual ACM Conference Extended Abstracts on Human
                            Factors in Computing Systems (pp. 959-964). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{lee2015user,<br />
                            title={User defined gestures for augmented virtual
                            mirrors: a guessability study},<br />
                            author={Lee, Gun A and Wong, Jonathan and Park, Hye
                            Sun and Choi, Jin Sung and Park, Chang Joon and
                            Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 33rd Annual ACM
                            Conference Extended Abstracts on Human Factors in
                            Computing Systems},<br />
                            pages={959--964},<br />
                            year={2015},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=2732747"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=2732747</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Public information displays are evolving from
                            passive screens into more interactive and smarter
                            ubiquitous computing platforms. In this research we
                            investigate applying gesture interaction and
                            Augmented Reality (AR) technologies to make public
                            information displays more intuitive and easy to use.
                            We focus especially on designing intuitive gesture
                            based interaction methods to use in combination with
                            an augmented virtual mirror interface. As an initial
                            step, we conducted a user study to indentify the
                            gestures that users feel are natural for performing
                            common tasks when interacting with augmented virtual
                            mirror displays. We report initial findings from the
                            study, discuss design guidelines, and suggest future
                            research directions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="261"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Freeze.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Freeze.jpg         736w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Freeze-300x261.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Freeze-168x146.jpg 168w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Freeze-50x43.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2015-Freeze-86x75.jpg    86w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Automatically Freezing Live Video for Annotation
                            during Remote Collaboration
                          </h5>
                          <small
                            >Kim, S., Lee, G. A., Ha, S., Sakata, N., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kim, S., Lee, G. A., Ha, S., Sakata, N., &
                            Billinghurst, M. (2015, April). Automatically
                            freezing live video for annotation during remote
                            collaboration. In Proceedings of the 33rd Annual ACM
                            Conference Extended Abstracts on Human Factors in
                            Computing Systems (pp. 1669-1674). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{kim2015automatically,<br />
                            title={Automatically freezing live video for
                            annotation during remote collaboration},<br />
                            author={Kim, Seungwon and Lee, Gun A and Ha, Sangtae
                            and Sakata, Nobuchika and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 33rd Annual ACM
                            Conference Extended Abstracts on Human Factors in
                            Computing Systems},<br />
                            pages={1669--1674},<br />
                            year={2015},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=2732838"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=2732838</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Drawing annotations on shared live video has been
                            investigated as a tool for remote collaboration.
                            However, if a local user changes the viewpoint of a
                            shared live video while a remote user is drawing an
                            annotation, the annotation is projected and drawn at
                            wrong place. Prior work suggested manually freezing
                            the video while annotating to solve the issue, but
                            this needs additional user input. We introduce a
                            solution that automatically freezes the video, and
                            present the results of a user study comparing it
                            with manual freeze and no freeze conditions.
                            Auto-freeze was most preferred by both remote and
                            local participants who felt it best solved the issue
                            of annotations appearing in the wrong place. With
                            auto-freeze, remote users were able to draw
                            annotations quicker, while the local users were able
                            to understand the annotations clearer.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="220"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Richie.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Richie.jpg         758w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Richie-300x220.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Richie-199x146.jpg 199w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Richie-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Richie-102x75.jpg  102w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A comparative study of simulated augmented reality
                            displays for vehicle navigation
                          </h5>
                          <small
                            >Jose, R., Lee, G. A., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Jose, R., Lee, G. A., & Billinghurst, M. (2016,
                            November). A comparative study of simulated
                            augmented reality displays for vehicle navigation.
                            In Proceedings of the 28th Australian conference on
                            computer-human interaction (pp. 40-48). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{jose2016comparative,<br />
                            title={A comparative study of simulated augmented
                            reality displays for vehicle navigation},<br />
                            author={Jose, Richie and Lee, Gun A and
                            Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 28th Australian
                            conference on computer-human interaction},<br />
                            pages={40--48},<br />
                            year={2016},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=3010918"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=3010918</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper we report on a user study in a
                            simulated environment that compares three types of
                            Augmented Reality (AR) displays for assisting with
                            car navigation: Heads Up Display (HUD), Head Mounted
                            Display (HMD) and Heads Down Display (HDD). The
                            virtual cues shown on each of the interface were the
                            same, but there was a significant difference in
                            driver behaviour and preference between interfaces.
                            Overall, users performed better and preferred the
                            HUD over the HDD, and the HMD was ranked lowest.
                            These results have implications for people wanting
                            to use AR cues for car navigation.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Dey-502x500.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Dey-502x500.jpg 502w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Dey-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Dey-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Dey-80x80.jpg    80w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Systematic Review of Usability Studies in
                            Augmented Reality between 2005 and 2014
                          </h5>
                          <small
                            >Dey, A., Billinghurst, M., Lindeman, R. W., & Swan
                            II, J. E.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Dey, A., Billinghurst, M., Lindeman, R. W., & Swan
                            II, J. E. (2016, September). A systematic review of
                            usability studies in augmented reality between 2005
                            and 2014. In 2016 IEEE international symposium on
                            mixed and augmented reality (ISMAR-Adjunct) (pp.
                            49-50). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2016systematic,<br />
                            title={A systematic review of usability studies in
                            augmented reality between 2005 and 2014},<br />
                            author={Dey, Arindam and Billinghurst, Mark and
                            Lindeman, Robert W and Swan II, J Edward},<br />
                            booktitle={2016 IEEE international symposium on
                            mixed and augmented reality (ISMAR-Adjunct)},<br />
                            pages={49--50},<br />
                            year={2016},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://web.cse.msstate.edu/~swan/publications/papers/2016_Dey-etal_Systematic-Review_IEEE-ISMAR-Poster.pdf"
                              target="_blank"
                              >http://web.cse.msstate.edu/~swan/publications/papers/2016_Dey-etal_Systematic-Review_IEEE-ISMAR-Poster.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) interfaces have been studied
                            extensively over the last few decades, with a
                            growing number of user-based experiments. In this
                            paper, we systematically review most AR papers
                            published between 2005 and 2014 that include user
                            studies. A total of 291 papers have been reviewed
                            and classified based on their application areas. The
                            primary contribution of the review is to present the
                            broad landscape of user-based AR research, and to
                            provide a high-level view of how that landscape has
                            changed. We also identify areas where there have
                            been few user studies, and opportunities for future
                            research. This poster describes the methodology of
                            the review and the classifications of AR research
                            that have emerged.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="220"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Alaeddin.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Alaeddin.jpg         716w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Alaeddin-300x220.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Alaeddin-199x146.jpg 199w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Alaeddin-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Alaeddin-102x75.jpg  102w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Augmented Reality Annotation for Social Video
                            Sharing
                          </h5>
                          <small></small>
                          <p style="font-style: italic; font-size: 12px">
                            Nassani, A., Kim, H., Lee, G., Billinghurst, M.,
                            Langlotz, T., & Lindeman, R. W. (2016, November).
                            Augmented reality annotation for social video
                            sharing. In SIGGRAPH ASIA 2016 Mobile Graphics and
                            Interactive Applications (p. 9). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{nassani2016augmented,<br />
                            title={Augmented reality annotation for social video
                            sharing},<br />
                            author={Nassani, Alaeddin and Kim, Hyungon and Lee,
                            Gun and Billinghurst, Mark and Langlotz, Tobias and
                            Lindeman, Robert W},<br />
                            booktitle={SIGGRAPH ASIA 2016 Mobile Graphics and
                            Interactive Applications},<br />
                            pages={9},<br />
                            year={2016},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="http://www.hci.otago.ac.nz/papers/NassaniACMSAMGIA2016.pdf"
                              target="_blank"
                              >http://www.hci.otago.ac.nz/papers/NassaniACMSAMGIA2016.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper explores different visual interfaces for
                            sharing comments on a social live video streaming
                            platforms. So far, comments are displayed separately
                            from the video making it hard to relate the comments
                            to event in the video. In this work we investigate
                            an Augmented Reality (AR) interface displaying
                            comments directly on the streamed live video. Our
                            described prototype allows remote spectators to
                            perceive the streamed live video with different
                            interfaces for displaying the comments. We conducted
                            a user study to compare different ways of
                            visualising comments and found that users prefer
                            having comments in the AR view rather than on a
                            separate list. We discuss the implications of this
                            research and directions for future work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="176"
                              src="https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei.jpg         948w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei-300x176.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei-768x450.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei-249x146.jpg 249w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei-50x29.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2019/11/2016-Lei-128x75.jpg  128w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            An oriented point-cloud view for MR remote
                            collaboration
                          </h5>
                          <small
                            >Gao, L., Bai, H., Lee, G., & Billinghurst,
                            M.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gao, L., Bai, H., Lee, G., & Billinghurst, M. (2016,
                            November). An oriented point-cloud view for MR
                            remote collaboration. In SIGGRAPH ASIA 2016 Mobile
                            Graphics and Interactive Applications (p. 8). ACM.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gao2016oriented,<br />
                            title={An oriented point-cloud view for MR remote
                            collaboration},<br />
                            author={Gao, Lei and Bai, Huidong and Lee, Gun and
                            Billinghurst, Mark},<br />
                            booktitle={SIGGRAPH ASIA 2016 Mobile Graphics and
                            Interactive Applications},<br />
                            pages={8},<br />
                            year={2016},<br />
                            organization={ACM}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/citation.cfm?id=2999531"
                              target="_blank"
                              >https://dl.acm.org/citation.cfm?id=2999531</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present a Mixed Reality system for remote
                            collaboration using Virtual Reality (VR) headsets
                            with external depth cameras attached. By wirelessly
                            sharing a 3D point-cloud data of a local workers'
                            workspace with a remote helper, and sharing the
                            remote helper's hand gestures back to the local
                            worker, the remote helper is able to assist the
                            worker to perform manual tasks.Displaying the
                            point-cloud video in a conventional way, such as a
                            static front view in VR headsets, does not provide
                            helpers with sufficient understanding of the spatial
                            relationships between their hands and the remote
                            surroundings. In contrast, we propose a Mixed
                            Reality (MR) system that shares with the remote
                            helper, not only 3D captured environment data but
                            also real-time orientation info of the worker's
                            viewpoint. We conducted a pilot study to evaluate
                            the usability of the system, and we found that extra
                            synchronized orientation data can make collaborators
                            feel more connected spatially and mentally.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="283"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/empathy.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/empathy.png         350w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/empathy-300x283.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/empathy-155x146.png 155w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/empathy-50x47.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/empathy-80x75.png    80w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Sharing Manipulated Heart Rate Feedback in
                            Collaborative Virtual Environments
                          </h5>
                          <small
                            >Arindam Dey ; Hao Chen ; Ashkan Hayati ; Mark
                            Billinghurst ; Robert W. Lindeman</small
                          >
                          <p style="font-style: italic; font-size: 12px"></p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2019sharing,<br />
                            title={Sharing Manipulated Heart Rate Feedback in
                            Collaborative Virtual Environments},<br />
                            author={Dey, Arindam and Chen, Hao and Hayati,
                            Ashkan and Billinghurst, Mark and Lindeman, Robert
                            W},<br />
                            booktitle={2019 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR)},<br />
                            pages={248--257},<br />
                            year={2019},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8943737"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8943737</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We have explored the effects of sharing manipulated
                            heart rate feedback in collaborative virtual
                            environments. In our study, we created two types of
                            different virtual environments (active and passive)
                            with different levels of interactions and provided
                            three levels of manipulated heart rate feedback
                            (decreased, unchanged, and increased). We measured
                            the effects of manipulated feedback on Social
                            Presence, affect, physical heart rate, and overall
                            experience. We noticed a significant effect of the
                            manipulated heart rate feedback in affecting
                            scariness and nervousness. The perception of the
                            collaborator's valance and arousal was also affected
                            where increased heart rate feedback perceived as a
                            higher valance and lower arousal. Increased heart
                            rate feedback decreased the real heart rate. The
                            type of virtual environments had a significant
                            effect on social presence, heart rate, and affect
                            where the active environment had better performances
                            across these measurements. We discuss the
                            implications of this and directions for future
                            research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="181"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/teo-sharedsphere.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/teo-sharedsphere.png         344w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/teo-sharedsphere-300x181.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/teo-sharedsphere-241x146.png 241w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/teo-sharedsphere-50x30.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/teo-sharedsphere-124x75.png  124w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Technique for Mixed Reality Remote Collaboration
                            using 360 Panoramas in 3D Reconstructed Scenes
                          </h5>
                          <small
                            >Theophilus Teo, Ashkan F. Hayati, Gun A. Lee, Mark
                            Billinghurst, Matt Adcock</small
                          >
                          <p style="font-style: italic; font-size: 12px"></p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{teo2019technique,<br />
                            title={A Technique for Mixed Reality Remote
                            Collaboration using 360 Panoramas in 3D
                            Reconstructed Scenes},<br />
                            author={Teo, Theophilus and F. Hayati, Ashkan and A.
                            Lee, Gun and Billinghurst, Mark and Adcock,
                            Matt},<br />
                            booktitle={25th ACM Symposium on Virtual Reality
                            Software and Technology},<br />
                            pages={1--11},<br />
                            year={2019}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3359996.3364238"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3359996.3364238</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Mixed Reality (MR) remote collaboration provides an
                            enhanced immersive experience where a remote user
                            can provide verbal and nonverbal assistance to a
                            local user to increase the efficiency and
                            performance of the collaboration. This is usually
                            achieved by sharing the local user's environment
                            through live 360 video or a 3D scene, and using
                            visual cues to gesture or point at real objects
                            allowing for better understanding and collaborative
                            task performance. While most of prior work used one
                            of the methods to capture the surrounding
                            environment, there may be situations where users
                            have to choose between using 360 panoramas or 3D
                            scene reconstruction to collaborate, as each have
                            unique benefits and limitations. In this paper we
                            designed a prototype system that combines 360
                            panoramas into a 3D scene to introduce a novel way
                            for users to interact and collaborate with each
                            other. We evaluated the prototype through a user
                            study which compared the usability and performance
                            of our proposed approach to live 360 video
                            collaborative system, and we found that participants
                            enjoyed using different ways to access the local
                            user's environment although it took them longer time
                            to learn to use our system. We also collected
                            subjective feedback for future improvements and
                            provide directions for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="260"
                              height="260"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health.jpg         260w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health-146x146.jpg 146w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health-75x75.jpg    75w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/mental_health-80x80.jpg    80w
                              "
                              sizes="(max-width: 260px) 100vw, 260px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Time to Get Personal: Individualised Virtual Reality
                            for Mental Health
                          </h5>
                          <small
                            >Nilufar Baghaei , Lehan Stemmet , Andrej Hlasnik ,
                            Konstantin Emanov , Sylvia Hach , John A. Naslund ,
                            Mark Billinghurst , Imran Khaliq , Hai-Ning
                            Liang</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Nilufar Baghaei, Lehan Stemmet, Andrej Hlasnik,
                            Konstantin Emanov, Sylvia Hach, John A. Naslund,
                            Mark Billinghurst, Imran Khaliq, and Hai-Ning Liang.
                            2020. Time to Get Personal: Individualised Virtual
                            Reality for Mental Health. In Extended Abstracts of
                            the 2020 CHI Conference on Human Factors in
                            Computing Systems (CHI EA ’20). Association for
                            Computing Machinery, New York, NY, USA, 1–9.
                            DOI:https://doi.org/10.1145/3334480.3382932
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{baghaei2020time,<br />
                            title={Time to Get Personal: Individualised Virtual
                            Reality for Mental Health},<br />
                            author={Baghaei, Nilufar and Stemmet, Lehan and
                            Hlasnik, Andrej and Emanov, Konstantin and Hach,
                            Sylvia and Naslund, John A and Billinghurst, Mark
                            and Khaliq, Imran and Liang, Hai-Ning},<br />
                            booktitle={Extended Abstracts of the 2020 CHI
                            Conference on Human Factors in Computing Systems
                            Extended Abstracts},<br />
                            pages={1--9},<br />
                            year={2020}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3334480.3382932"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3334480.3382932</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Mental health conditions pose a major challenge to
                            healthcare providers and society at large. Early
                            intervention can have significant positive impact on
                            a person's prognosis, particularly important in
                            improving mental health outcomes and functioning for
                            young people. Virtual Reality (VR) in mental health
                            is an emerging and innovative field. Recent studies
                            support the use of VR technology in the treatment of
                            anxiety, phobia, eating disorders, addiction, and
                            pain management. However, there is little research
                            on using VR for supporting, treatment and prevention
                            of depression - a field that is very much emerging.
                            There is also very little work done in offering
                            individualised VR experience to users with mental
                            health issues. This paper proposes iVR, a novel
                            individualised VR for improving users'
                            self-compassion, and in the long run, their positive
                            mental health. We describe the concept, design,
                            architecture and implementation of iVR and outline
                            future work. We believe this contribution will pave
                            the way for large-scale efficacy testing, clinical
                            use, and potentially cost-effective delivery of VR
                            technology for mental health therapy in future.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="251"
                              height="251"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/3376550.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550.jpg         251w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550-146x146.jpg 146w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550-75x75.jpg    75w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/3376550-80x80.jpg    80w
                              "
                              sizes="(max-width: 251px) 100vw, 251px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A User Study on Mixed Reality Remote Collaboration
                            with Eye Gaze and Hand Gesture Sharing
                          </h5>
                          <small
                            >Huidong Bai , Prasanth Sasikumar , Jing Yang , Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Huidong Bai, Prasanth Sasikumar, Jing Yang, and Mark
                            Billinghurst. 2020. A User Study on Mixed Reality
                            Remote Collaboration with Eye Gaze and Hand Gesture
                            Sharing. In Proceedings of the 2020 CHI Conference
                            on Human Factors in Computing Systems (CHI ’20).
                            Association for Computing Machinery, New York, NY,
                            USA, 1–13.
                            DOI:https://doi.org/10.1145/3313831.3376550
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{bai2020user,<br />
                            title={A User Study on Mixed Reality Remote
                            Collaboration with Eye Gaze and Hand Gesture
                            Sharing},<br />
                            author={Bai, Huidong and Sasikumar, Prasanth and
                            Yang, Jing and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 2020 CHI Conference on
                            Human Factors in Computing Systems},<br />
                            pages={1--13},<br />
                            year={2020}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3313831.3376550"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3313831.3376550</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Supporting natural communication cues is critical
                            for people to work together remotely and
                            face-to-face. In this paper we present a Mixed
                            Reality (MR) remote collaboration system that
                            enables a local worker to share a live 3D panorama
                            of his/her surroundings with a remote expert. The
                            remote expert can also share task instructions back
                            to the local worker using visual cues in addition to
                            verbal communication. We conducted a user study to
                            investigate how sharing augmented gaze and gesture
                            cues from the remote expert to the local worker
                            could affect the overall collaboration performance
                            and user experience. We found that by combing gaze
                            and gesture cues, our remote collaboration system
                            could provide a significantly stronger sense of
                            co-presence for both the local and remote users than
                            using the gaze cue alone. The combined cues were
                            also rated significantly higher than the gaze in
                            terms of ease of conveying spatial actions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="251"
                              height="251"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics.jpg         251w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics-146x146.jpg 146w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics-75x75.jpg    75w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Passive-Haptics-80x80.jpg    80w
                              "
                              sizes="(max-width: 251px) 100vw, 251px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Constrained Path Redirection for Passive Haptics
                          </h5>
                          <small>
                            Lili Wang ; Zixiang Zhao ; Xuefeng Yang ; Huidong
                            Bai ; Amit Barde ; Mark Billinghurst
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            L. Wang, Z. Zhao, X. Yang, H. Bai, A. Barde and M.
                            Billinghurst, "A Constrained Path Redirection for
                            Passive Haptics," 2020 IEEE Conference on Virtual
                            Reality and 3D User Interfaces Abstracts and
                            Workshops (VRW), Atlanta, GA, USA, 2020, pp.
                            651-652, doi: 10.1109/VRW50115.2020.00176.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{wang2020constrained,<br />
                            title={A Constrained Path Redirection for Passive
                            Haptics},<br />
                            author={Wang, Lili and Zhao, Zixiang and Yang,
                            Xuefeng and Bai, Huidong and Barde, Amit and
                            Billinghurst, Mark},<br />
                            booktitle={2020 IEEE Conference on Virtual Reality
                            and 3D User Interfaces Abstracts and Workshops
                            (VRW)},<br />
                            pages={651--652},<br />
                            year={2020},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9090521"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9090521</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Navigation with passive haptic feedback can enhance
                            users’ immersion in virtual environments. We propose
                            a constrained path redirection method to provide
                            users with corresponding haptic feedback at the
                            right time and place. We have quantified the VR
                            exploration practicality in a study and the results
                            show advantages over steer-to-center method in terms
                            of presence, and over Steinicke’s method in terms of
                            matching errors and presence.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="251"
                              height="251"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects.jpg         251w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects-146x146.jpg 146w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects-75x75.jpg    75w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Neurophysiological-Effects-80x80.jpg    80w
                              "
                              sizes="(max-width: 251px) 100vw, 251px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Neurophysiological Effects of Presence in Calm
                            Virtual Environments
                          </h5>
                          <small>
                            Arindam Dey ; Jane Phoon ; Shuvodeep Saha ; Chelsea
                            Dobbins ; Mark Billinghurst
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            A. Dey, J. Phoon, S. Saha, C. Dobbins and M.
                            Billinghurst, "Neurophysiological Effects of
                            Presence in Calm Virtual Environments," 2020 IEEE
                            Conference on Virtual Reality and 3D User Interfaces
                            Abstracts and Workshops (VRW), Atlanta, GA, USA,
                            2020, pp. 745-746, doi: 10.1109/VRW50115.2020.00223.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dey2020neurophysiological,<br />
                            title={Neurophysiological Effects of Presence in
                            Calm Virtual Environments},<br />
                            author={Dey, Arindam and Phoon, Jane and Saha,
                            Shuvodeep and Dobbins, Chelsea and Billinghurst,
                            Mark},<br />
                            booktitle={2020 IEEE Conference on Virtual Reality
                            and 3D User Interfaces Abstracts and Workshops
                            (VRW)},<br />
                            pages={745--746},<br />
                            year={2020},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9090463"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9090463</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Presence, the feeling of being there, is an
                            important factor that affects the overall experience
                            of virtual reality. Presence is measured through
                            post-experience subjective questionnaires. While
                            questionnaires are a widely used method in
                            human-based research, they suffer from participant
                            biases, dishonest answers, and fatigue. In this
                            paper, we measured the effects of different levels
                            of presence (high and low) in virtual environments
                            using physiological and neurological signals as an
                            alternative method. Results indicated a significant
                            effect of presence on both physiological and
                            neurological signals.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="250"
                              height="250"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust.jpg         250w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust-150x150.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust-146x146.jpg 146w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust-75x75.jpg    75w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust-85x85.jpg    85w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Human-Trust-80x80.jpg    80w
                              "
                              sizes="(max-width: 250px) 100vw, 250px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Measuring Human Trust in a Virtual Assistant using
                            Physiological Sensing in Virtual Reality
                          </h5>
                          <small>
                            Kunal Gupta, Ryo Hajika, Yun Suen Pai, Andreas
                            Duenser, Martin Lochner, Mark Billinghurst
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            K. Gupta, R. Hajika, Y. S. Pai, A. Duenser, M.
                            Lochner and M. Billinghurst, "Measuring Human Trust
                            in a Virtual Assistant using Physiological Sensing
                            in Virtual Reality," 2020 IEEE Conference on Virtual
                            Reality and 3D User Interfaces (VR), Atlanta, GA,
                            USA, 2020, pp. 756-765, doi:
                            10.1109/VR46266.2020.1581313729558.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gupta2020measuring,<br />
                            title={Measuring Human Trust in a Virtual Assistant
                            using Physiological Sensing in Virtual Reality},<br />
                            author={Gupta, Kunal and Hajika, Ryo and Pai, Yun
                            Suen and Duenser, Andreas and Lochner, Martin and
                            Billinghurst, Mark},<br />
                            booktitle={2020 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={756--765},<br />
                            year={2020},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9089632"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9089632</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            With the advancement of Artificial Intelligence
                            technology to make smart devices, understanding how
                            humans develop trust in virtual agents is emerging
                            as a critical research field. Through our research,
                            we report on a novel methodology to investigate
                            user’s trust in auditory assistance in a Virtual
                            Reality (VR) based search task, under both high and
                            low cognitive load and under varying levels of agent
                            accuracy. We collected physiological sensor data
                            such as electroencephalography (EEG), galvanic skin
                            response (GSR), and heart-rate variability (HRV),
                            subjective data through questionnaire such as System
                            Trust Scale (STS), Subjective Mental Effort
                            Questionnaire (SMEQ) and NASA-TLX. We also collected
                            a behavioral measure of trust (congruency of users’
                            head motion in response to valid/ invalid verbal
                            advice from the agent). Our results indicate that
                            our custom VR environment enables researchers to
                            measure and understand human trust in virtual agents
                            using the matrices, and both cognitive load and
                            agent accuracy play an important role in trust
                            formation. We discuss the implications of the
                            research and directions for future work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="250"
                              height="208"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/Haptic-Feedback.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Haptic-Feedback.jpg         250w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Haptic-Feedback-175x146.jpg 175w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Haptic-Feedback-50x42.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/Haptic-Feedback-90x75.jpg    90w
                              "
                              sizes="(max-width: 250px) 100vw, 250px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Haptic Feedback Helps Me? A VR-SAR Remote
                            Collaborative System with Tangible Interaction
                          </h5>
                          <small
                            >Peng Wang, Xiaoliang Bai, Mark Billinghurst,
                            Shusheng Zhang, Dechuan Han, Mengmeng Sun, Zhuo
                            Wang, Hao Lv, Shu Han</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, Peng, et al. "Haptic Feedback Helps Me? A
                            VR-SAR Remote Collaborative System with Tangible
                            Interaction." International Journal of
                            Human–Computer Interaction (2020): 1-16.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{wang2020haptic,<br />
                            title={Haptic Feedback Helps Me? A VR-SAR Remote
                            Collaborative System with Tangible Interaction},<br />
                            author={Wang, Peng and Bai, Xiaoliang and
                            Billinghurst, Mark and Zhang, Shusheng and Han,
                            Dechuan and Sun, Mengmeng and Wang, Zhuo and Lv, Hao
                            and Han, Shu},<br />
                            journal={International Journal of Human--Computer
                            Interaction},<br />
                            pages={1--16},<br />
                            year={2020},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/full/10.1080/10447318.2020.1732140"
                              target="_blank"
                              >https://www.tandfonline.com/doi/full/10.1080/10447318.2020.1732140</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Research on Augmented Reality (AR)/Mixed Reality
                            (MR) remote collaboration for physical tasks remains
                            a compelling and dynamic area of study. AR systems
                            have been developed which transmit virtual
                            annotations between remote collaborators, but there
                            has been little research on how haptic feedback can
                            also be shared. In this paper, we present a Virtual
                            Reality (VR)-Spatial Augmented Reality (SAR) remote
                            collaborative system that provides haptic feedback
                            with tangible interaction between a local worker and
                            a remote expert helper. Using this system, we
                            conducted a within-subject user study to compare two
                            interfaces for remote collaboration between a local
                            worker and expert helper, one with mid-air free
                            drawing (MFD) and one with tangible physical drawing
                            (TPD). The results showed that there were no
                            significant differences with respect to performance
                            time and operation errors. However, users felt that
                            the TPD interface supporting passive haptic feedback
                            could significantly improve the remote experts’ user
                            experience in VR. Our research provides useful
                            information on the way for gesture- and gaze-based
                            multimodal interaction supporting haptic feedback in
                            AR/MR remote collaboration on physical tasks.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="250"
                              height="219"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/firefighter.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/firefighter.jpg         250w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/firefighter-167x146.jpg 167w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/firefighter-50x44.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/firefighter-86x75.jpg    86w
                              "
                              sizes="(max-width: 250px) 100vw, 250px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Aerial firefighter radio communication performance
                            in a virtual training system: radio communication
                            disruptions simulated in VR for Air Attack
                            Supervision
                          </h5>
                          <small
                            >Rory M. S. Clifford, Hendrik Engelbrecht, Sungchul
                            Jung, Hamish Oliver, Mark Billinghurst, Robert W.
                            Lindeman & Simon Hoermann
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Clifford, Rory MS, et al. "Aerial firefighter radio
                            communication performance in a virtual training
                            system: radio communication disruptions simulated in
                            VR for Air Attack Supervision." The Visual Computer
                            (2020): 1-14.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{clifford2020aerial,<br />
                            title={Aerial firefighter radio communication
                            performance in a virtual training system: radio
                            communication disruptions simulated in VR for Air
                            Attack Supervision},<br />
                            author={Clifford, Rory MS and Engelbrecht, Hendrik
                            and Jung, Sungchul and Oliver, Hamish and
                            Billinghurst, Mark and Lindeman, Robert W and
                            Hoermann, Simon},<br />
                            journal={The Visual Computer},<br />
                            pages={1--14},<br />
                            year={2020},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s00371-020-01816-6"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s00371-020-01816-6</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Communication disruptions are frequent in aerial
                            firefighting. Information is more easily lost over
                            multiple radio channels, busy with simultaneous
                            conversations. Such a high bandwidth of information
                            throughput creates mental overload. Further problems
                            with hardware or radio signals being disrupted over
                            long distances or by mountainous terrain make it
                            difficult to coordinate firefighting efforts. This
                            creates stressful conditions and requires certain
                            expertise to manage effectively. An experiment was
                            conducted which tested the effects of disrupting
                            users communications equipment and measured their
                            stress levels as well as communication performance.
                            This research investigated how realistic
                            communication disruptions have an effect on
                            behavioural changes in communication frequency, as
                            well as physiological stress by means of measuring
                            heart rate variability (HRV). Broken radio
                            transmissions created a greater degree of stress
                            than background chatter alone. Experts could
                            maintain a more stable HRV during disruptions than
                            novices, which was calculated on the change in HRV
                            during the experiment. From this, we deduce that
                            experts have a better ability to manage stress. We
                            also noted strategies employed by experts such as
                            relaying to overcome the radio challenges, as
                            opposed to novices who would not find a solution,
                            effectively giving up.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="250"
                              height="219"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/geometry-teaching.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/geometry-teaching.jpg         250w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/geometry-teaching-167x146.jpg 167w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/geometry-teaching-50x44.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/geometry-teaching-86x75.jpg    86w
                              "
                              sizes="(max-width: 250px) 100vw, 250px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            An assessment of geometry teaching supported with
                            augmented reality teaching materials to enhance
                            students’ 3D geometry thinking skills
                          </h5>
                          <small
                            >Emin İbili, Mevlüt Çat, Dmitry Resnyansky, Sami
                            Şahin & Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            İbili, Emin, et al. "An assessment of geometry
                            teaching supported with augmented reality teaching
                            materials to enhance students’ 3D geometry thinking
                            skills." International Journal of Mathematical
                            Education in Science and Technology 51.2 (2020):
                            224-246.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ibili2020assessment,<br />
                            title={An assessment of geometry teaching supported
                            with augmented reality teaching materials to enhance
                            students’ 3D geometry thinking skills},<br />
                            author={{\.I}bili, Emin and {\c{C}}at, Mevl{\"u}t
                            and Resnyansky, Dmitry and {\c{S}}ahin, Sami and
                            Billinghurst, Mark},<br />
                            journal={International Journal of Mathematical
                            Education in Science and Technology},<br />
                            volume={51},<br />
                            number={2},<br />
                            pages={224--246},<br />
                            year={2020},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/full/10.1080/0020739X.2019.1583382"
                              target="_blank"
                              >https://www.tandfonline.com/doi/full/10.1080/0020739X.2019.1583382</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The aim of this research was to examine the effect
                            of Augmented Reality (AR) supported geometry
                            teaching on students’ 3D thinking skills. This
                            research consisted of 3 steps: (1) developing a 3D
                            thinking ability scale, (ii) design and development
                            of an AR Geometry Tutorial System (ARGTS) and (iii)
                            implementation and assessment of geometry teaching
                            supported with ARGTS. A 3D thinking ability scale
                            was developed and tested with experimental and
                            control groups as a pre- and post-test evaluation.
                            An AR Geometry Tutorial System (ARGTS) and AR
                            teaching materials and environments were developed
                            to enhance 3D thinking skills. A user study with
                            these materials found that geometry teaching
                            supported by ARGTS significantly increased the
                            students’ 3D thinking skills. The increase in
                            average scores of Structuring 3D arrays of cubes and
                            Calculation of the volume and the area of solids
                            thinking skills was not statistically significant
                            (<i>p</i> &gt; 0.05). In terms of other 3D geometric
                            thinking skills’ subfactors of the scale a
                            statistically significant difference was found in
                            favour of the experimental group in pre-test and
                            post-test scores (<i>p</i> &lt; 0.05). The biggest
                            difference was found on ability to recognize and
                            create 3D shapes (<i>p</i> &lt; 0.01).The results of
                            this research are particularly important for
                            identifying individual differences in 3D thinking
                            skills of secondary school students and creating
                            personalized dynamic intelligent learning
                            environments.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="220"
                              src="https://empathiccomputing.org/wp-content/uploads/2020/05/ar-language-learning.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2020/05/ar-language-learning.jpg         377w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/ar-language-learning-300x220.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/ar-language-learning-199x146.jpg 199w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/ar-language-learning-50x37.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2020/05/ar-language-learning-102x75.jpg  102w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Using augmented reality with speech input for
                            non-native children's language learning
                          </h5>
                          <small
                            >Che Samihah Che Dalim, Mohd Shahrizal, Sunar,
                            Arindam Dey, MarkBillinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dalim, Che Samihah Che, et al. "Using augmented
                            reality with speech input for non-native children's
                            language learning." International Journal of
                            Human-Computer Studies 134 (2020): 44-64.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{dalim2020using,<br />
                            title={Using augmented reality with speech input for
                            non-native children's language learning},<br />
                            author={Dalim, Che Samihah Che and Sunar, Mohd
                            Shahrizal and Dey, Arindam and Billinghurst,
                            Mark},<br />
                            journal={International Journal of Human-Computer
                            Studies},<br />
                            volume={134},<br />
                            pages={44--64},<br />
                            year={2020},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S1071581918303161"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S1071581918303161</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) offers an enhanced learning
                            environment which could potentially influence
                            children's experience and knowledge gain during the
                            language learning process. Teaching English or other
                            foreign languages to children with different native
                            language can be difficult and requires an effective
                            strategy to avoid boredom and detachment from the
                            learning activities. With the growing numbers of AR
                            education applications and the increasing
                            pervasiveness of speech recognition, we are keen to
                            understand how these technologies benefit non-native
                            young children in learning English. In this paper,
                            we explore children's experience in terms of
                            knowledge gain and enjoyment when learning through a
                            combination of AR and speech recognition
                            technologies. We developed a prototype AR interface
                            called TeachAR, and ran two experiments to
                            investigate how effective the combination of AR and
                            speech recognition was towards the learning of 1)
                            English terms for color and shapes, and 2) English
                            words for spatial relationships. We found
                            encouraging results by creating a novel teaching
                            strategy using these two technologies, not only in
                            terms of increase in knowledge gain and enjoyment
                            when compared with traditional strategy but also
                            enables young children to finish the certain task
                            faster and easier.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="194"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM.jpg          1342w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM-300x194.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM-768x496.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM-1024x661.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM-226x146.jpg   226w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM-50x32.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Screen-Shot-2021-07-24-at-1.13.33-PM-116x75.jpg    116w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A comparative study on inter-brain synchrony in real
                            and virtual environments using hyperscanning
                          </h5>
                          <small
                            >Ihshan Gumilar, Ekansh Sareen, Reed Bell, Augustus
                            Stone, Ashkan Hayati, Jingwen Mao, Amit Barde,
                            Anubha Gupta, Arindam Dey, Gun Lee, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gumilar, I., Sareen, E., Bell, R., Stone, A.,
                            Hayati, A., Mao, J., ... & Billinghurst, M. (2021).
                            A comparative study on inter-brain synchrony in real
                            and virtual environments using hyperscanning.
                            Computers & Graphics, 94, 62-75.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{gumilar2021comparative,<br />
                            title={A comparative study on inter-brain synchrony
                            in real and virtual environments using
                            hyperscanning},<br />
                            author={Gumilar, Ihshan and Sareen, Ekansh and Bell,
                            Reed and Stone, Augustus and Hayati, Ashkan and Mao,
                            Jingwen and Barde, Amit and Gupta, Anubha and Dey,
                            Arindam and Lee, Gun and others},<br />
                            journal={Computers \& Graphics},<br />
                            volume={94},<br />
                            pages={62--75},<br />
                            year={2021},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/abs/pii/S0097849320301540"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/abs/pii/S0097849320301540</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Researchers have employed hyperscanning, a technique
                            used to simultaneously record neural activity from
                            multiple participants, in real-world collaborations.
                            However, to the best of our knowledge, there is no
                            study that has used hyperscanning in Virtual Reality
                            (VR). The aims of this study were; firstly, to
                            replicate results of inter-brain synchrony reported
                            in existing literature for a real world task and
                            secondly, to explore whether the inter-brain
                            synchrony could be elicited in a Virtual Environment
                            (VE). This paper reports on three pilot-studies in
                            two different settings (real-world and VR). Paired
                            participants performed two sessions of a
                            finger-pointing exercise separated by a
                            finger-tracking exercise during which their neural
                            activity was simultaneously recorded by
                            electroencephalography (EEG) hardware. By using
                            Phase Locking Value (PLV) analysis, VR was found to
                            induce similar inter-brain synchrony as seen in the
                            real-world. Further, it was observed that the
                            finger-pointing exercise shared the same neurally
                            activated area in both the real-world and VR. Based
                            on these results, we infer that VR can be used to
                            enhance inter-brain synchrony in collaborative tasks
                            carried out in a VE. In particular, we have been
                            able to demonstrate that changing visual perspective
                            in VR is capable of eliciting inter-brain synchrony.
                            This demonstrates that VR could be an exciting
                            platform to explore the phenomena of inter-brain
                            synchrony further and provide a deeper understanding
                            of the neuroscience of human communication.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="193"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2.jpg         946w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2-300x193.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2-768x494.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2-227x146.jpg 227w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2-50x32.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper2-117x75.jpg  117w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Grand Challenges for Augmented Reality
                          </h5>
                          <small>Mark Billinghurst</small>
                          <p style="font-style: italic; font-size: 12px">
                            Billinghurst, M. (2021). Grand Challenges for
                            Augmented Reality. Frontiers in Virtual Reality, 2,
                            12.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{billinghurst2021grand,<br />
                            title={Grand Challenges for Augmented Reality},<br />
                            author={Billinghurst, Mark},<br />
                            journal={Frontiers in Virtual Reality},<br />
                            volume={2},<br />
                            pages={12},<br />
                            year={2021},<br />
                            publisher={Frontiers}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.frontiersin.org/articles/10.3389/frvir.2021.578080/full"
                              target="_blank"
                              >https://www.frontiersin.org/articles/10.3389/frvir.2021.578080/full</a
                            ><br />
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="196"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3.jpg          1234w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3-300x196.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3-768x503.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3-1024x670.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3-223x146.jpg   223w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3-50x33.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper3-115x75.jpg    115w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Bringing full-featured mobile phone interaction into
                            virtual reality
                          </h5>
                          <small>H Bai, L Zhang, J Yang, M Billinghurst</small>
                          <p style="font-style: italic; font-size: 12px">
                            Bai, H., Zhang, L., Yang, J., & Billinghurst, M.
                            (2021). Bringing full-featured mobile phone
                            interaction into virtual reality. Computers &
                            Graphics, 97, 42-53.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{bai2021bringing,<br />
                            title={Bringing full-featured mobile phone
                            interaction into virtual reality},<br />
                            author={Bai, Huidong and Zhang, Li and Yang, Jing
                            and Billinghurst, Mark},<br />
                            journal={Computers \& Graphics},<br />
                            volume={97},<br />
                            pages={42--53},<br />
                            year={2021},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1016/j.cag.2021.04.004"
                              target="_blank"
                              >https://doi.org/10.1016/j.cag.2021.04.004</a
                            ><br />Video:
                            <a
                              href="https://www.youtube.com/watch?v=pBDRl4WRnxk"
                              target="_blank"
                              >https://www.youtube.com/watch?v=pBDRl4WRnxk</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <div id="abs0001" class="abstract author">
                              <div id="abssec0003">
                                <p id="sp0003">
                                  Virtual Reality (VR) Head-Mounted Display
                                  (HMD) technology immerses a user in a computer
                                  generated virtual environment. However, a VR
                                  HMD also blocks the users’ view of their
                                  physical surroundings, and so prevents them
                                  from using their mobile phones in a natural
                                  manner. In this paper, we present a novel
                                  Augmented Virtuality (AV) interface that
                                  enables people to naturally interact with a
                                  mobile phone in real time in a virtual
                                  environment. The system allows the user to
                                  wear a VR HMD while seeing his/her 3D hands
                                  captured by a depth sensor and rendered in
                                  different styles, and enables the user to
                                  operate a virtual mobile phone aligned with
                                  their real phone. We conducted a formal user
                                  study to compare the AV interface with
                                  physical touch interaction on user experience
                                  in five mobile applications. Participants
                                  reported that our system brought the real
                                  mobile phone into the virtual world.
                                  Unfortunately, the experiment results
                                  indicated that using a phone with our AV
                                  interfaces in VR was more difficult than the
                                  regular smartphone touch interaction, with
                                  increased workload and lower
                                  <a
                                    class="topic-link"
                                    title="Learn more about system usability from ScienceDirect's AI-generated Topic Pages"
                                    href="https://www.sciencedirect.com/topics/computer-science/usability-systems"
                                    >system usability</a
                                  >, especially for a typing task. We ran a
                                  follow-up study to compare different hand
                                  visualizations for text typing using the AV
                                  interface. Participants felt that a
                                  skin-colored hand visualization method
                                  provided better usability and immersiveness
                                  than other hand rendering styles.
                                </p>
                              </div>
                            </div>
                            <div id="absh0002" class="abstract graphical"></div>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="217"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4.jpg          1250w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4-300x217.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4-768x555.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4-1024x741.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4-202x146.jpg   202w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4-50x36.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper4-104x75.jpg    104w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            SecondSight: A Framework for Cross-Device Augmented
                            Reality Interfaces
                          </h5>
                          <small
                            >Reichherzer, Carolin, Jack Fraser, Damien
                            Constantine Rompapas, Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Reichherzer, C., Fraser, J., Rompapas, D. C., &
                            Billinghurst, M. (2021, May). SecondSight: A
                            Framework for Cross-Device Augmented Reality
                            Interfaces. In Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing Systems
                            (pp. 1-6).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{reichherzer2021secondsight,<br />
                            title={SecondSight: A Framework for Cross-Device
                            Augmented Reality Interfaces},<br />
                            author={Reichherzer, Carolin and Fraser, Jack and
                            Rompapas, Damien Constantine and Billinghurst,
                            Mark},<br />
                            booktitle={Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            pages={1--6},<br />
                            year={2021}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1145/3411763.3451839"
                              target="_blank"
                              >https://doi.org/10.1145/3411763.3451839</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper describes a modular framework developed
                            to facilitate the design space exploration of
                            cross-device Augmented Reality (AR) interfaces that
                            combine an AR head-mounted display (HMD) with a
                            smartphone. Currently, there is a growing interest
                            in how AR HMDs can be used with smartphones to
                            improve the user’s AR experience. In this work, we
                            describe a framework that enables rapid prototyping
                            and evaluation of an interface. Our system enables
                            different modes of interaction, content placement,
                            and simulated AR HMD field of view to assess which
                            combination is best suited to inform future
                            researchers on design recommendations. We provide
                            examples of how the framework could be used to
                            create sample applications, the types of the studies
                            which could be supported, and example results from a
                            simple pilot study.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="208"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5.jpg          1454w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5-300x208.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5-768x533.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5-1024x711.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5-210x146.jpg   210w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5-50x35.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper5-108x75.jpg    108w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Eye See What You See: Exploring How Bi-Directional
                            Augmented Reality Gaze Visualisation Influences
                            Co-Located Symmetric Collaboration
                          </h5>
                          <small
                            >Allison Jing, Kieran May, Gun Lee, Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Jing, A., May, K., Lee, G., & Billinghurst, M.
                            (2021). Eye See What You See: Exploring How
                            Bi-Directional Augmented Reality Gaze Visualisation
                            Influences Co-Located Symmetric Collaboration.
                            Frontiers in Virtual Reality, 2, 79.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{jing2021eye,<br />
                            title={Eye See What You See: Exploring How
                            Bi-Directional Augmented Reality Gaze Visualisation
                            Influences Co-Located Symmetric Collaboration},<br />
                            author={Jing, Allison and May, Kieran and Lee, Gun
                            and Billinghurst, Mark},<br />
                            journal={Frontiers in Virtual Reality},<br />
                            volume={2},<br />
                            pages={79},<br />
                            year={2021},<br />
                            publisher={Frontiers}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.3389/frvir.2021.697367"
                              target="_blank"
                              >https://doi.org/10.3389/frvir.2021.697367</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Gaze is one of the predominant communication cues
                            and can provide valuable implicit information such
                            as intention or focus when performing collaborative
                            tasks. However, little research has been done on how
                            virtual gaze cues combining spatial and temporal
                            characteristics impact real-life physical tasks
                            during face to face collaboration. In this study, we
                            explore the effect of showing joint gaze interaction
                            in an Augmented Reality (AR) interface by evaluating
                            three bi-directional collaborative (BDC) gaze
                            visualisations with three levels of gaze behaviours.
                            Using three independent tasks, we found that all
                            bi-directional collaborative BDC visualisations are
                            rated significantly better at representing joint
                            attention and user intention compared to a
                            non-collaborative (NC) condition, and hence are
                            considered more engaging. The Laser Eye condition,
                            spatially embodied with gaze direction, is perceived
                            significantly more effective as it encourages mutual
                            gaze awareness with a relatively low mental effort
                            in a less constrained workspace. In addition, by
                            offering additional virtual representation that
                            compensates for verbal descriptions and hand
                            pointing, BDC gaze visualisations can encourage more
                            conscious use of gaze cues coupled with deictic
                            references during co-located symmetric
                            collaboration. We provide a summary of the lessons
                            learned, limitations of the study, and directions
                            for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="196"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6.jpg         898w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6-300x196.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6-768x503.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6-223x146.jpg 223w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6-50x33.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper6-115x75.jpg  115w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            First Contact‐Take 2: Using XR technology as a
                            bridge between Māori, Pākehā and people from other
                            cultures in Aotearoa, New Zealand
                          </h5>
                          <small
                            >Mairi Gunn, Mark Billinghurst, Huidong Bai,
                            Prasanth Sasikumar.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gunn, M., Billinghurst, M., Bai, H., & Sasikumar, P.
                            (2021). First Contact‐Take 2: Using XR technology as
                            a bridge between Māori, Pākehā and people from other
                            cultures in Aotearoa, New Zealand. Virtual
                            Creativity, 11(1), 67-90.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{gunn2021first,<br />
                            title={First Contact-Take 2: Using XR technology as
                            a bridge between M{\=a}ori, P{\=a}keh{\=a} and
                            people from other cultures in Aotearoa, New
                            Zealand},<br />
                            author={Gunn, Mairi and Billinghurst, Mark and Bai,
                            Huidong and Sasikumar, Prasanth},<br />
                            journal={Virtual Creativity},<br />
                            volume={11},<br />
                            number={1},<br />
                            pages={67--90},<br />
                            year={2021},<br />
                            publisher={Intellect}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1386/vcr_00043_1"
                              target="_blank"
                              >https://doi.org/10.1386/vcr_00043_1</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The art installation <i>common/room</i> explores
                            human‐digital‐human encounter across cultural
                            differences. It comprises a suite of extended
                            reality (XR) experiences that use technology as a
                            bridge to help support human connections with a view
                            to overcoming intercultural discomfort (racism). The
                            installations are exhibited as an informal dining
                            room, where each table hosts a distinct experience
                            designed to bring people together in a playful yet
                            meaningful way. Each experience uses different
                            technologies, including 360° 3D virtual reality (VR)
                            in a headset (<i>common/place</i>), 180° 3D
                            projection (<i>Common Sense</i>) and augmented
                            reality (AR) (<i>Come to the Table!</i> and
                            <i>First Contact ‐ Take 2</i>). This article focuses
                            on the latter, <i>First Contact ‐ Take 2</i>, in
                            which visitors are invited to sit at a dining table,
                            wear an AR head-mounted display and encounter a
                            recorded volumetric representation of an Indigenous
                            Māori woman seated opposite them. She speaks
                            directly to the visitor out of a culture that has
                            refined collective endeavour and relational
                            psychology over millennia. The contextual and
                            methodological framework for this research is
                            international commons scholarship and practice that
                            sits within a set of relationships outlined by the
                            Mātike Mai Report on constitutional transformation
                            for Aotearoa, New Zealand. The goal is to practise
                            and build new relationships between Māori and
                            Tauiwi, including Pākehā.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="197"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7.jpg          1580w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7-300x197.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7-768x505.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7-1024x673.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7-222x146.jpg   222w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7-50x33.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper7-114x75.jpg    114w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            ShowMeAround: Giving Virtual Tours Using Live 360
                            Video
                          </h5>
                          <small
                            >Alaeddin Nassani, Li Zhang, Huidong Bai, Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Nassani, A., Zhang, L., Bai, H., & Billinghurst, M.
                            (2021, May). ShowMeAround: Giving Virtual Tours
                            Using Live 360 Video. In Extended Abstracts of the
                            2021 CHI Conference on Human Factors in Computing
                            Systems (pp. 1-4).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{nassani2021showmearound,<br />
                            title={ShowMeAround: Giving Virtual Tours Using Live
                            360 Video},<br />
                            author={Nassani, Alaeddin and Zhang, Li and Bai,
                            Huidong and Billinghurst, Mark},<br />
                            booktitle={Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            pages={1--4},<br />
                            year={2021}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1145/3411763.3451555"
                              target="_blank"
                              >https://doi.org/10.1145/3411763.3451555</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This demonstration presents ShowMeAround, a video
                            conferencing system designed to allow people to give
                            virtual tours over live 360-video. Using
                            ShowMeAround a host presenter walks through a real
                            space and can live stream a 360-video view to a
                            small group of remote viewers. The ShowMeAround
                            interface has features such as remote pointing and
                            viewpoint awareness to support natural collaboration
                            between the viewers and host presenter. The system
                            also enables sharing of pre-recorded high resolution
                            360 video and still images to further enhance the
                            virtual tour experience.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="198"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8.jpg          1302w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8-300x198.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8-768x507.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8-1024x676.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8-221x146.jpg   221w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8-50x33.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper8-114x75.jpg    114w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Manipulating Avatars for Enhanced Communication in
                            Extended Reality
                          </h5>
                          <small
                            >Jonathon Hart, Thammathip Piumsomboon, Gun A. Lee,
                            Ross T. Smith, Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Hart, J. D., Piumsomboon, T., Lee, G. A., Smith, R.
                            T., & Billinghurst, M. (2021, May). Manipulating
                            Avatars for Enhanced Communication in Extended
                            Reality. In 2021 IEEE International Conference on
                            Intelligent Reality (ICIR) (pp. 9-16). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{hart2021manipulating,<br />
                            title={Manipulating Avatars for Enhanced
                            Communication in Extended Reality},<br />
                            author={Hart, Jonathon Derek and Piumsomboon,
                            Thammathip and Lee, Gun A and Smith, Ross T and
                            Billinghurst, Mark},<br />
                            booktitle={2021 IEEE International Conference on
                            Intelligent Reality (ICIR)},<br />
                            pages={9--16},<br />
                            year={2021},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9480967"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9480967</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Avatars are common virtual representations used in
                            Extended Reality (XR) to support interaction and
                            communication between remote collaborators. Recent
                            advancements in wearable displays provide features
                            such as eye and face-tracking, to enable avatars to
                            express non-verbal cues in XR. The research in this
                            paper investigates the impact of avatar
                            visualization on Social Presence and user’s
                            preference by simulating face tracking in an
                            asymmetric XR remote collaboration between a desktop
                            user and a Virtual Reality (VR) user. Our study was
                            conducted between pairs of participants, one on a
                            laptop computer supporting face tracking and the
                            other being immersed in VR, experiencing different
                            visualization conditions. They worked together to
                            complete an island survival task. We found that the
                            users preferred 3D avatars with facial expressions
                            placed in the scene, compared to 2D screen attached
                            avatars without facial expressions. Participants
                            felt that the presence of the collaborator’s avatar
                            improved overall communication, yet Social Presence
                            was not significantly different between conditions
                            as they mainly relied on audio for communication.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="233"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9.jpg         1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9-300x233.jpg  300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9-768x597.jpg  768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9-188x146.jpg  188w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9-50x39.jpg     50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9-96x75.jpg     96w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper9-960x750.jpg  960w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Adapting Fitts’ Law and N-Back to Assess Hand
                            Proprioception
                          </h5>
                          <small
                            >Tamil Gunasekaran, Ryo Hajika, Chloe Dolma Si Ying
                            Haigh, Yun Suen Pai, Danielle Lottridge, Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gunasekaran, T. S., Hajika, R., Haigh, C. D. S. Y.,
                            Pai, Y. S., Lottridge, D., & Billinghurst, M. (2021,
                            May). Adapting Fitts’ Law and N-Back to Assess Hand
                            Proprioception. In Extended Abstracts of the 2021
                            CHI Conference on Human Factors in Computing Systems
                            (pp. 1-7).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gunasekaran2021adapting,<br />
                            title={Adapting Fitts’ Law and N-Back to Assess Hand
                            Proprioception},<br />
                            author={Gunasekaran, Tamil Selvan and Hajika, Ryo
                            and Haigh, Chloe Dolma Si Ying and Pai, Yun Suen and
                            Lottridge, Danielle and Billinghurst, Mark},<br />
                            booktitle={Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            pages={1--7},<br />
                            year={2021}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1145/3411763.3451699"
                              target="_blank"
                              >https://doi.org/10.1145/3411763.3451699</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Proprioception is the body’s ability to sense the
                            position and movement of each limb, as well as the
                            amount of effort exerted onto or by them. Methods to
                            assess proprioception have been introduced before,
                            yet there is little to no study on assessing the
                            degree of proprioception on body parts for use cases
                            like gesture recognition wearable computing. We
                            propose the use of Fitts’ law coupled with the
                            N-Back task to evaluate proprioception of the hand.
                            We evaluate 15 distinct points at the back of the
                            hand and assess the musing extended 3D Fitts’ law.
                            Our results show that the index of difficulty of
                            tapping point from thumb to pinky increases
                            gradually with a linear regression factor of 0.1144.
                            Additionally, participants perform the tap before
                            performing the N-Back task. From these results, we
                            discuss the fundamental limitations and suggest how
                            Fitts’ law can be further extended to assess
                            proprioception
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="168"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10.jpg          1744w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10-300x168.jpg   300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10-768x431.jpg   768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10-1024x574.jpg 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10-260x146.jpg   260w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10-50x28.jpg      50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/07/Paper10-134x75.jpg    134w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            XRTB: A Cross Reality Teleconference Bridge to
                            incorporate 3D interactivity to 2D Teleconferencing
                          </h5>
                          <small
                            >Prasanth Sasikumar, Max Collins, Huidong Bai, Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Sasikumar, P., Collins, M., Bai, H., & Billinghurst,
                            M. (2021, May). XRTB: A Cross Reality Teleconference
                            Bridge to incorporate 3D interactivity to 2D
                            Teleconferencing. In Extended Abstracts of the 2021
                            CHI Conference on Human Factors in Computing Systems
                            (pp. 1-4).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{sasikumar2021xrtb,<br />
                            title={XRTB: A Cross Reality Teleconference Bridge
                            to incorporate 3D interactivity to 2D
                            Teleconferencing},<br />
                            author={Sasikumar, Prasanth and Collins, Max and
                            Bai, Huidong and Billinghurst, Mark},<br />
                            booktitle={Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            pages={1--4},<br />
                            year={2021}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://doi.org/10.1145/3411763.3451546"
                              target="_blank"
                              >https://doi.org/10.1145/3411763.3451546</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present XRTeleBridge (XRTB), an application that
                            integrates a Mixed Reality (MR) interface into
                            existing teleconferencing solutions like Zoom.
                            Unlike conventional webcam, XRTB provides a window
                            into the virtual world to demonstrate and visualize
                            content. Participants can join via webcam or via
                            head mounted display (HMD) in a Virtual Reality (VR)
                            environment. It enables users to embody 3D avatars
                            with natural gestures and eye gaze. A camera in the
                            virtual environment operates as a video feed to the
                            teleconferencing software. An interface resembling a
                            tablet mirrors the teleconferencing window inside
                            the virtual environment, thus enabling the
                            participant in the VR environment to see the webcam
                            participants in real-time. This allows the presenter
                            to view and interact with other participants
                            seamlessly. To demonstrate the system’s
                            functionalities, we created a virtual chemistry lab
                            environment and presented an example lesson using
                            the virtual space and virtual objects and effects.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="186"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/09/Faces.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Faces.jpg         378w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Faces-300x186.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Faces-236x146.jpg 236w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Faces-50x31.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Faces-121x75.jpg  121w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Connecting the Brains via Virtual Eyes: Eye-Gaze
                            Directions and Inter-brain Synchrony in VR
                          </h5>
                          <small
                            >Ihshan Gumilar, Amit Barde, Ashkan Hayati, Mark
                            Billinghurst, Gun Lee, Abdul Momin, Charles Averill,
                            Arindam Dey.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Gumilar, I., Barde, A., Hayati, A. F., Billinghurst,
                            M., Lee, G., Momin, A., ... & Dey, A. (2021, May).
                            Connecting the Brains via Virtual Eyes: Eye-Gaze
                            Directions and Inter-brain Synchrony in VR. In
                            Extended Abstracts of the 2021 CHI Conference on
                            Human Factors in Computing Systems (pp. 1-7).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gumilar2021connecting,<br />
                            title={Connecting the Brains via Virtual Eyes:
                            Eye-Gaze Directions and Inter-brain Synchrony in
                            VR},<br />
                            author={Gumilar, Ihshan and Barde, Amit and Hayati,
                            Ashkan F and Billinghurst, Mark and Lee, Gun and
                            Momin, Abdul and Averill, Charles and Dey,
                            Arindam},<br />
                            booktitle={Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            pages={1--7},<br />
                            year={2021}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/pdf/10.1145/3411763.3451583"
                              target="_blank"
                              >https://dl.acm.org/doi/pdf/10.1145/3411763.3451583</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Hyperscanning is an emerging method for measuring
                            two or more brains simultaneously. This method
                            allows researchers to simultaneously record neural
                            activity from two or more people. While this method
                            has been extensively implemented over the last five
                            years in the real-world to study inter-brain
                            synchrony, there is little work that has been
                            undertaken in the use of hyperscanning in virtual
                            environments. Preliminary research in the area
                            demonstrates that inter-brain synchrony in virtual
                            environments can be achieved in a mannersimilar to
                            thatseen in the real world. The study described in
                            this paper proposes to further research in the area
                            by studying how non-verbal communication cues in
                            social interactions in virtual environments can
                            afect inter-brain synchrony. In particular, we
                            concentrate on the role eye gaze playsin inter-brain
                            synchrony. The aim of this research is to explore
                            how eye gaze afects inter-brain synchrony between
                            users in a collaborative virtual environment
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="244"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/09/BrainSync.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/09/BrainSync.jpg         465w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/BrainSync-300x244.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/BrainSync-180x146.jpg 180w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/BrainSync-50x41.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/BrainSync-92x75.jpg    92w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Review of Hyperscanning and Its Use in Virtual
                            Environments
                          </h5>
                          <small
                            >Ihshan Gumilar, Ekansh Sareen, Reed Bell, Augustus
                            Stone, Ashkan Hayati, Jingwen Mao, Amit Barde,
                            Anubha Gupta, Arindam Dey, Gun Lee, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Barde, A., Gumilar, I., Hayati, A. F., Dey, A., Lee,
                            G., & Billinghurst, M. (2020, December). A Review of
                            Hyperscanning and Its Use in Virtual Environments.
                            In Informatics (Vol. 7, No. 4, p. 55).
                            Multidisciplinary Digital Publishing Institute.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{barde2020review,<br />
                            title={A Review of Hyperscanning and Its Use in
                            Virtual Environments},<br />
                            author={Barde, Amit and Gumilar, Ihshan and Hayati,
                            Ashkan F and Dey, Arindam and Lee, Gun and
                            Billinghurst, Mark},<br />
                            booktitle={Informatics},<br />
                            volume={7},<br />
                            number={4},<br />
                            pages={55},<br />
                            year={2020},<br />
                            organization={Multidisciplinary Digital Publishing
                            Institute}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.mdpi.com/2227-9709/7/4/55"
                              target="_blank"
                              >https://www.mdpi.com/2227-9709/7/4/55</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Researchers have employed hyperscanning, a technique
                            used to simultaneously record neural activity from
                            multiple participants, in real-world collaborations.
                            However, to the best of our knowledge, there is no
                            study that has used hyperscanning in Virtual Reality
                            (VR). The aims of this study were; firstly, to
                            replicate results of inter-brain synchrony reported
                            in existing literature for a real world task and
                            secondly, to explore whether the inter-brain
                            synchrony could be elicited in a Virtual Environment
                            (VE). This paper reports on three pilot-studies in
                            two different settings (real-world and VR). Paired
                            participants performed two sessions of a
                            finger-pointing exercise separated by a
                            finger-tracking exercise during which their neural
                            activity was simultaneously recorded by
                            electroencephalography (EEG) hardware. By using
                            Phase Locking Value (PLV) analysis, VR was found to
                            induce similar inter-brain synchrony as seen in the
                            real-world. Further, it was observed that the
                            finger-pointing exercise shared the same neurally
                            activated area in both the real-world and VR. Based
                            on these results, we infer that VR can be used to
                            enhance inter-brain synchrony in collaborative tasks
                            carried out in a VE. In particular, we have been
                            able to demonstrate that changing visual perspective
                            in VR is capable of eliciting inter-brain synchrony.
                            This demonstrates that VR could be an exciting
                            platform to explore the phenomena of inter-brain
                            synchrony further and provide a deeper understanding
                            of the neuroscience of human communication.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="126"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning.jpg         818w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning-300x126.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning-768x323.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning-260x109.jpg 260w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning-50x21.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/Hyperscanning-150x63.jpg  150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Inter-brain connectivity: Comparisons between real
                            and virtual environments using hyperscanning
                          </h5>
                          <small
                            >Amit Barde, Nastaran Saffaryazdi, P. Withana, N.
                            Patel, Prasanth Sasikumar, Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Barde, A., Saffaryazdi, N., Withana, P., Patel, N.,
                            Sasikumar, P., & Billinghurst, M. (2019, October).
                            Inter-brain connectivity: Comparisons between real
                            and virtual environments using hyperscanning. In
                            2019 IEEE International Symposium on Mixed and
                            Augmented Reality Adjunct (ISMAR-Adjunct) (pp.
                            338-339). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{barde2019inter,<br />
                            title={Inter-brain connectivity: Comparisons between
                            real and virtual environments using
                            hyperscanning},<br />
                            author={Barde, Amit and Saffaryazdi, Nastaran and
                            Withana, Pawan and Patel, Nakul and Sasikumar,
                            Prasanth and Billinghurst, Mark},<br />
                            booktitle={2019 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={338--339},<br />
                            year={2019},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8951947"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8951947</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Inter-brain connectivity between pairs of people was
                            explored during a finger tracking task in the
                            real-world and in Virtual Reality (VR). This was
                            facilitated by the use of a dual EEG set-up that
                            allowed us to use hyperscanning to simultaneously
                            record the neural activity of both participants. We
                            found that similar levels of inter-brain synchrony
                            can be elicited in the real-world and VR for the
                            same task. This is the first time that hyperscanning
                            has been used to compare brain activity for the same
                            task performed in real and virtual environments.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="207"
                              src="https://empathiccomputing.org/wp-content/uploads/2021/09/NeuroDrum.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2021/09/NeuroDrum.jpg         614w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/NeuroDrum-300x207.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/NeuroDrum-211x146.jpg 211w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/NeuroDrum-50x35.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2021/09/NeuroDrum-109x75.jpg  109w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            NeuralDrum: Perceiving Brain Synchronicity in XR
                            Drumming
                          </h5>
                          <small
                            >Y. S. Pai, Ryo Hajika, Kunla Gupta, Prasnth
                            Sasikumar, Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Pai, Y. S., Hajika, R., Gupta, K., Sasikumar, P., &
                            Billinghurst, M. (2020). NeuralDrum: Perceiving
                            Brain Synchronicity in XR Drumming. In SIGGRAPH Asia
                            2020 Technical Communications (pp. 1-4).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @incollection{pai2020neuraldrum,<br />
                            title={NeuralDrum: Perceiving Brain Synchronicity in
                            XR Drumming},<br />
                            author={Pai, Yun Suen and Hajika, Ryo and Gupta,
                            Kunal and Sasikumar, Prasanth and Billinghurst,
                            Mark},<br />
                            booktitle={SIGGRAPH Asia 2020 Technical
                            Communications},<br />
                            pages={1--4},<br />
                            year={2020}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3410700.3425434"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3410700.3425434</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Brain synchronicity is a neurological phenomena
                            where two or more individuals have their brain
                            activation in phase when performing a shared
                            activity. We present NeuralDrum, an extended reality
                            (XR) drumming experience that allows two players to
                            drum together while their brain signals are
                            simultaneously measured. We calculate the Phase
                            Locking Value (PLV) to determine their brain
                            synchronicity and use this to directly affect their
                            visual and auditory experience in the game, creating
                            a closed feedback loop. In a pilot study, we logged
                            and analysed the users’ brain signals as well as had
                            them answer a subjective questionnaire regarding
                            their perception of synchronicity with their partner
                            and the overall experience. From the results, we
                            discuss design implications to further improve
                            NeuralDrum and propose methods to integrate brain
                            synchronicity into interactive experiences.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="227"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/06/Screenshot-2022-06-26-at-15-51-35-NapWell-An-EOG-based-Sleep-Assistant-Exploring-the-Effects-of-Virtual-Reality-on-Sleep-Onset-Virtual-Reality.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Screenshot-2022-06-26-at-15-51-35-NapWell-An-EOG-based-Sleep-Assistant-Exploring-the-Effects-of-Virtual-Reality-on-Sleep-Onset-Virtual-Reality.png         515w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Screenshot-2022-06-26-at-15-51-35-NapWell-An-EOG-based-Sleep-Assistant-Exploring-the-Effects-of-Virtual-Reality-on-Sleep-Onset-Virtual-Reality-300x227.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Screenshot-2022-06-26-at-15-51-35-NapWell-An-EOG-based-Sleep-Assistant-Exploring-the-Effects-of-Virtual-Reality-on-Sleep-Onset-Virtual-Reality-193x146.png 193w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Screenshot-2022-06-26-at-15-51-35-NapWell-An-EOG-based-Sleep-Assistant-Exploring-the-Effects-of-Virtual-Reality-on-Sleep-Onset-Virtual-Reality-50x38.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Screenshot-2022-06-26-at-15-51-35-NapWell-An-EOG-based-Sleep-Assistant-Exploring-the-Effects-of-Virtual-Reality-on-Sleep-Onset-Virtual-Reality-99x75.png    99w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            NapWell: An EOG-based Sleep Assistant Exploring the
                            Effects of Virtual Reality on Sleep Onset
                          </h5>
                          <small
                            >Yun Suen Pai, Marsel L. Bait, Juyoung Lee, Jingjing
                            Xu, Roshan L Peiris, Woontack Woo, Mark Billinghurst
                            & Kai Kunze</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Pai, Y. S., Bait, M. L., Lee, J., Xu, J., Peiris, R.
                            L., Woo, W., ... & Kunze, K. (2022). NapWell: an
                            EOG-based sleep assistant exploring the effects of
                            virtual reality on sleep onset. Virtual Reality,
                            26(2), 437-451.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{pai2022napwell,<br />
                            title={NapWell: an EOG-based sleep assistant
                            exploring the effects of virtual reality on sleep
                            onset},<br />
                            author={Pai, Yun Suen and Bait, Marsel L and Lee,
                            Juyoung and Xu, Jingjing and Peiris, Roshan L and
                            Woo, Woontack and Billinghurst, Mark and Kunze,
                            Kai},<br />
                            journal={Virtual Reality},<br />
                            volume={26},<br />
                            number={2},<br />
                            pages={437--451},<br />
                            year={2022},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-021-00571-w"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-021-00571-w</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We present NapWell, a Sleep Assistant using virtual
                            reality (VR) to decrease sleep onset latency by
                            providing a realistic imagery distraction prior to
                            sleep onset. Our proposed prototype was built using
                            commercial hardware and with relatively low cost,
                            making it replicable for future works as well as
                            paving the way for more low cost EOG-VR devices for
                            sleep assistance. We conducted a user study (<span
                              class="mathjax-tex"
                              ><span
                                id="MathJax-Element-1-Frame"
                                class="MathJax"
                                tabindex="0"
                                role="presentation"
                                data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;/math&gt;'
                                ><span id="MathJax-Span-1" class="math"
                                  ><span id="MathJax-Span-2" class="mrow"
                                    ><span id="MathJax-Span-3" class="mi"
                                      >n</span
                                    ><span id="MathJax-Span-4" class="mo"
                                      >=</span
                                    ><span id="MathJax-Span-5" class="mn"
                                      >20</span
                                    ></span
                                  ></span
                                ></span
                              ></span
                            >) by comparing different sleep conditions; no
                            devices, sleeping mask, VR environment of the study
                            room and preferred VR environment by the
                            participant. During this period, we recorded the
                            electrooculography (EOG) signal and sleep onset time
                            using a finger tapping task (FTT). We found that VR
                            was able to significantly decrease sleep onset
                            latency. We also developed a machine learning model
                            based on EOG signals that can predict sleep onset
                            with a cross-validated accuracy of 70.03%. The
                            presented study demonstrates the feasibility of VR
                            to be used as a tool to decrease sleep onset
                            latency, as well as the use of embedded EOG sensors
                            with VR for automatic sleep detection.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="180"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/06/RaITIn.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/06/RaITIn.png         336w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/RaITIn-300x180.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/RaITIn-243x146.png 243w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/RaITIn-50x30.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/RaITIn-125x75.png  125w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            RaITIn: Radar-Based Identification for Tangible
                            Interactions
                          </h5>
                          <small
                            >Tamil Selvan Gunasekaran , Ryo Hajika , Yun Suen
                            Pai , Eiji Hayashi , Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gunasekaran, T. S., Hajika, R., Pai, Y. S., Hayashi,
                            E., & Billinghurst, M. (2022, April). RaITIn:
                            Radar-Based Identification for Tangible
                            Interactions. In CHI Conference on Human Factors in
                            Computing Systems Extended Abstracts (pp. 1-7).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gunasekaran2022raitin,<br />
                            title={RaITIn: Radar-Based Identification for
                            Tangible Interactions},<br />
                            author={Gunasekaran, Tamil Selvan and Hajika, Ryo
                            and Pai, Yun Suen and Hayashi, Eiji and
                            Billinghurst, Mark},<br />
                            booktitle={CHI Conference on Human Factors in
                            Computing Systems Extended Abstracts},<br />
                            pages={1--7},<br />
                            year={2022}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/pdf/10.1145/3491101.3519808"
                              target="_blank"
                              >https://dl.acm.org/doi/pdf/10.1145/3491101.3519808</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Radar is primarily used for applications like
                            tracking and large-scale ranging, and its use for
                            object identification has been rarely explored. This
                            paper introduces RaITIn, a radar-based
                            identification (ID) method for tangible
                            interactions. Unlike conventional radar solutions,
                            RaITIn can track and identify objects on a tabletop
                            scale. We use frequency modulated continuous wave
                            (FMCW) radar sensors to classify different objects
                            embedded with low-cost radar reflectors of varying
                            sizes on a tabletop setup. We also introduce
                            Stackable IDs, where different objects can be
                            stacked and combined to produce unique IDs. The
                            result allows RaITIn to accurately identify visually
                            identical objects embedded with different low-cost
                            reflector configurations. When combined with a
                            radar’s ability for tracking, it creates novel
                            tabletop interaction modalities. We discuss possible
                            applications and areas for future work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="182"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/06/Eye-Gaze-Direction.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Eye-Gaze-Direction.png         457w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Eye-Gaze-Direction-300x182.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Eye-Gaze-Direction-240x146.png 240w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Eye-Gaze-Direction-50x30.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Eye-Gaze-Direction-123x75.png  123w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Inter-brain Synchrony and Eye Gaze Direction During
                            Collaboration in VR
                          </h5>
                          <small
                            >Ihshan Gumilar , Amit Barde , Prasanth Sasikumar ,
                            Mark Billinghurst , Ashkan F. Hayati , Gun Lee ,
                            Yuda Munarko , Sanjit Singh , Abdul Momin</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gumilar, I., Barde, A., Sasikumar, P., Billinghurst,
                            M., Hayati, A. F., Lee, G., ... & Momin, A. (2022,
                            April). Inter-brain Synchrony and Eye Gaze Direction
                            During Collaboration in VR. In CHI Conference on
                            Human Factors in Computing Systems Extended
                            Abstracts (pp. 1-7).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{gumilar2022inter,<br />
                            title={Inter-brain Synchrony and Eye Gaze Direction
                            During Collaboration in VR},<br />
                            author={Gumilar, Ihshan and Barde, Amit and
                            Sasikumar, Prasanth and Billinghurst, Mark and
                            Hayati, Ashkan F and Lee, Gun and Munarko, Yuda and
                            Singh, Sanjit and Momin, Abdul},<br />
                            booktitle={CHI Conference on Human Factors in
                            Computing Systems Extended Abstracts},<br />
                            pages={1--7},<br />
                            year={2022}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/pdf/10.1145/3491101.3519746"
                              target="_blank"
                              >https://dl.acm.org/doi/pdf/10.1145/3491101.3519746</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Brain activity sometimes synchronises when people
                            collaborate together on real world tasks.
                            Understanding this process could to lead to
                            improvements in face to face and remote
                            collaboration. In this paper we report on an
                            experiment exploring the relationship between eye
                            gaze and inter-brain synchrony in Virtual Reality
                            (VR). The experiment recruited pairs who were asked
                            to perform finger-tracking exercises in VR with
                            three different gaze conditions: averted, direct,
                            and natural, while their brain activity was
                            recorded. We found that gaze direction has a
                            significant effect on inter-brain synchrony during
                            collaboration for this task in VR. This shows that
                            representing natural gaze could influence
                            inter-brain synchrony in VR, which may have
                            implications for avatar design for social VR. We
                            discuss implications of our research and possible
                            directions for future work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="209"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/06/communication-cues-for-augmented-reality.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/06/communication-cues-for-augmented-reality.png         412w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/communication-cues-for-augmented-reality-300x209.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/communication-cues-for-augmented-reality-210x146.png 210w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/communication-cues-for-augmented-reality-50x35.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/communication-cues-for-augmented-reality-108x75.png  108w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A review on communication cues for augmented reality
                            based remote guidance
                          </h5>
                          <small
                            >Weidong Huang, Mathew Wakefield, Troels Ammitsbøl
                            Rasmussen, Seungwon Kim & Mark Billinghurst
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Huang, W., Wakefield, M., Rasmussen, T. A., Kim, S.,
                            & Billinghurst, M. (2022). A review on communication
                            cues for augmented reality based remote guidance.
                            Journal on Multimodal User Interfaces, 1-18.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{huang2022review,<br />
                            title={A review on communication cues for augmented
                            reality based remote guidance},<br />
                            author={Huang, Weidong and Wakefield, Mathew and
                            Rasmussen, Troels Ammitsb{\o}l and Kim, Seungwon and
                            Billinghurst, Mark},<br />
                            journal={Journal on Multimodal User Interfaces},<br />
                            pages={1--18},<br />
                            year={2022},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/content/pdf/10.1007/s12193-022-00387-1.pdf"
                              target="_blank"
                              >https://link.springer.com/content/pdf/10.1007/s12193-022-00387-1.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Remote guidance on physical tasks is a type of
                            collaboration in which a local worker is guided by a
                            remote helper to operate on a set of physical
                            objects. It has many applications in industrial
                            sections such as remote maintenance and how to
                            support this type of remote collaboration has been
                            researched for almost three decades. Although a
                            range of different modern computing tools and
                            systems have been proposed, developed and used to
                            support remote guidance in different application
                            scenarios, it is essential to provide communication
                            cues in a shared visual space to achieve common
                            ground for effective communication and
                            collaboration. In this paper, we conduct a selective
                            review to summarize communication cues, approaches
                            that implement the cues and their effects on
                            augmented reality based remote guidance. We also
                            discuss challenges and propose possible future
                            research and development directions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="201"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/06/Seeing.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Seeing.png         468w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Seeing-300x201.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Seeing-218x146.png 218w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Seeing-50x33.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/Seeing-112x75.png  112w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Seeing is believing: AR-assisted blind area assembly
                            to support hand–eye coordination
                          </h5>
                          <small
                            >Shuo Feng, Weiping He, Shaohua Zhang & Mark
                            Billinghurst
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Feng, S., He, W., Zhang, S., & Billinghurst, M.
                            (2022). Seeing is believing: AR-assisted blind area
                            assembly to support hand–eye coordination. The
                            International Journal of Advanced Manufacturing
                            Technology, 119(11), 8149-8158.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{feng2022seeing,<br />
                            title={Seeing is believing: AR-assisted blind area
                            assembly to support hand--eye coordination},<br />
                            author={Feng, Shuo and He, Weiping and Zhang,
                            Shaohua and Billinghurst, Mark},<br />
                            journal={The International Journal of Advanced
                            Manufacturing Technology},<br />
                            volume={119},<br />
                            number={11},<br />
                            pages={8149--8158},<br />
                            year={2022},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s00170-021-08546-6"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s00170-021-08546-6</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The assembly stage is a vital phase in the
                            production process and currently, there are still
                            many manual tasks in the assembly operation. One of
                            the challenges of manual assembly is the issue of
                            blind area assembly since the visual obstruction of
                            the hands or a part can lead to more errors and
                            lower assembly efficiency. In this study, we
                            developed an AR-assisted assembly system that solves
                            the occlusion problem. Assembly workers can use the
                            system to achieve comprehensive and precise hand–eye
                            coordination (HEC). Additionally, we designed and
                            conducted a user evaluation experiment to measure
                            the learnability, usability, and mental effort
                            required for the system for other HEC modes. Results
                            indicate that hand position is the first visual
                            information that should be considered in blind
                            areas. Besides, the Intact HEC mode can effectively
                            reduce the difficulty of learning and mental burden
                            in operation, while at the same time improving
                            efficiency.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="172"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/06/facial-expressions.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/06/facial-expressions.png         450w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/facial-expressions-300x172.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/facial-expressions-255x146.png 255w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/facial-expressions-50x29.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/06/facial-expressions-131x75.png  131w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Effects of interacting with facial expressions and
                            controllers in different virtual environments on
                            presence, usability, affect, and neurophysiological
                            signals
                          </h5>
                          <small
                            >Arindam Dey, Amit Barde, Bowen Yuan, Ekansh Sareen,
                            Chelsea Dobbins, Aaron Goh, Gaurav Gupta, Anubha
                            Gupta, MarkBillinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dey, A., Barde, A., Yuan, B., Sareen, E., Dobbins,
                            C., Goh, A., ... & Billinghurst, M. (2022). Effects
                            of interacting with facial expressions and
                            controllers in different virtual environments on
                            presence, usability, affect, and neurophysiological
                            signals. International Journal of Human-Computer
                            Studies, 160, 102762.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{dey2022effects,<br />
                            title={Effects of interacting with facial
                            expressions and controllers in different virtual
                            environments on presence, usability, affect, and
                            neurophysiological signals},<br />
                            author={Dey, Arindam and Barde, Amit and Yuan, Bowen
                            and Sareen, Ekansh and Dobbins, Chelsea and Goh,
                            Aaron and Gupta, Gaurav and Gupta, Anubha and
                            Billinghurst, Mark},<br />
                            journal={International Journal of Human-Computer
                            Studies},<br />
                            volume={160},<br />
                            pages={102762},<br />
                            year={2022},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S1071581921001804"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S1071581921001804</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual Reality (VR) interfaces provide an immersive
                            medium to interact with the digital world. Most VR
                            interfaces require physical interactions using
                            handheld controllers, but there are other
                            alternative interaction methods that can support
                            different use cases and users. Interaction methods
                            in VR are primarily evaluated based on their
                            usability, however, their differences in
                            neurological and physiological effects remains less
                            investigated. In this paper—along with other
                            traditional qualitative matrices such as presence,
                            affect, and system usability—we explore the
                            neurophysiological effects—brain signals and
                            electrodermal activity—of using an alternative
                            facial expression interaction method to interact
                            with VR interfaces. This form of interaction was
                            also compared with traditional handheld controllers.
                            Three different environments, with different
                            experiences to interact with were used—happy
                            (butterfly catching), neutral (object picking), and
                            scary (zombie shooting). Overall, we noticed an
                            effect of interaction methods on the gamma
                            activities in the brain and on skin conductance. For
                            some aspects of presence, facial expression
                            outperformed controllers but controllers were found
                            to be better than facial expressions in terms of
                            usability.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="197"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/07/HapticProxy.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/07/HapticProxy.png         611w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/HapticProxy-300x197.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/HapticProxy-222x146.png 222w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/HapticProxy-50x33.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/HapticProxy-114x75.png  114w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            HapticProxy: Providing Positional Vibrotactile
                            Feedback on a Physical Proxy for Virtual-Real
                            Interaction in Augmented Reality
                          </h5>
                          <small
                            >Zhang, L., He, W., Cao, Z., Wang, S., Bai, H., &
                            Billinghurst, M.
                          </small>
                          <p style="font-style: italic; font-size: 12px">
                            Zhang, L., He, W., Cao, Z., Wang, S., Bai, H., &
                            Billinghurst, M. (2022). HapticProxy: Providing
                            Positional Vibrotactile Feedback on a Physical Proxy
                            for Virtual-Real Interaction in Augmented Reality.
                            International Journal of Human–Computer Interaction,
                            1-15.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{zhang2022hapticproxy,<br />
                            title={HapticProxy: Providing Positional
                            Vibrotactile Feedback on a Physical Proxy for
                            Virtual-Real Interaction in Augmented Reality},<br />
                            author={Zhang, Li and He, Weiping and Cao, Zhiwei
                            and Wang, Shuxia and Bai, Huidong and Billinghurst,
                            Mark},<br />
                            journal={International Journal of Human--Computer
                            Interaction},<br />
                            pages={1--15},<br />
                            year={2022},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/epub/10.1080/10447318.2022.2041895"
                              target="_blank"
                              >https://www.tandfonline.com/doi/epub/10.1080/10447318.2022.2041895</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Consistent visual and haptic feedback is an
                            important way to improve the user experience when
                            interacting with virtual objects. However, the
                            perception provided in Augmented Reality (AR) mainly
                            comes from visual cues and amorphous tactile
                            feedback. This work explores how to simulate
                            positional vibrotactile feedback (PVF) with multiple
                            vibration motors when colliding with virtual objects
                            in AR. By attaching spatially distributed vibration
                            motors on a physical haptic proxy, users can obtain
                            an augmented collision experience with positional
                            vibration sensations from the contact point with
                            virtual objects. We first developed a prototype
                            system and conducted a user study to optimize the
                            design parameters. Then we investigated the effect
                            of PVF on user performance and experience in a
                            virtual and real object alignment task in the AR
                            environment. We found that this approach could
                            significantly reduce the alignment offset between
                            virtual and physical objects with tolerable task
                            completion time increments. With the PVF cue,
                            participants obtained a more comprehensive
                            perception of the offset direction, more useful
                            information, and a more authentic AR experience.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="190"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/07/Octopus-Sensing.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Octopus-Sensing.jpg         301w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Octopus-Sensing-300x190.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Octopus-Sensing-230x146.jpg 230w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Octopus-Sensing-50x32.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Octopus-Sensing-118x75.jpg  118w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Octopus Sensing: A Python library for human behavior
                            studies
                          </h5>
                          <small
                            >Nastaran Saffaryazdi, Aidin Gharibnavaz, Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Saffaryazdi, N., Gharibnavaz, A., & Billinghurst, M.
                            (2022). Octopus Sensing: A Python library for human
                            behavior studies. Journal of Open Source Software,
                            7(71), 4045.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{saffaryazdi2022octopus,<br />
                            title={Octopus Sensing: A Python library for human
                            behavior studies},<br />
                            author={Saffaryazdi, Nastaran and Gharibnavaz, Aidin
                            and Billinghurst, Mark},<br />
                            journal={Journal of Open Source Software},<br />
                            volume={7},<br />
                            number={71},<br />
                            pages={4045},<br />
                            year={2022}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://joss.theoj.org/papers/10.21105/joss.04045.pdf"
                              target="_blank"
                              >https://joss.theoj.org/papers/10.21105/joss.04045.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <span dir="ltr" role="presentation"
                              >Designing user studies and collecting data is
                              critical to exploring and automatically
                              recognizing </span
                            ><span dir="ltr" role="presentation"
                              >human behavior.</span
                            >
                            <span dir="ltr" role="presentation"
                              >It is currently possible to use a range of
                              sensors to capture heart rate,</span
                            >
                            <span dir="ltr" role="presentation"
                              >brain activity, skin conductance, and a variety
                              of different physiological cues</span
                            >.
                            <span dir="ltr" role="presentation"
                              >These data can be combined to provide information
                              about a user’s emotional</span
                            >
                            <span dir="ltr" role="presentation"
                              >state, cognitive load</span
                            ><span dir="ltr" role="presentation"
                              >, or other factors.</span
                            >
                            <span dir="ltr" role="presentation"
                              >However, even when data are collected
                              correctly,</span
                            >
                            <span dir="ltr" role="presentation"
                              >synchronizing data from multiple sensors is
                              time-consuming and prone to errors. Failure
                              to</span
                            >
                            <span dir="ltr" role="presentation"
                              >record and synchronize data is likely to result
                              in errors in analysis and results, as well as
                              the</span
                            >
                            <span dir="ltr" role="presentation"
                              >need to repeat the time-consuming experiments
                              several times. To overcome these challenges, </span
                            ><span dir="ltr" role="presentation">Octopus</span>
                            <span dir="ltr" role="presentation">Sensing</span>
                            <span dir="ltr" role="presentation"
                              >facilitates synchronous data acquisition from
                              various sources and provides</span
                            >
                            <span dir="ltr" role="presentation"
                              >some utilities for designing user studies,
                              real-time monitoring, and offline data
                              visualization.</span
                            ><br role="presentation" /><span
                              dir="ltr"
                              role="presentation"
                              >The primary aim of</span
                            >
                            <span dir="ltr" role="presentation">Octopus</span>
                            <span dir="ltr" role="presentation">Sensing</span>
                            <span dir="ltr" role="presentation"
                              >is to provide a simple scripting interface so
                              that people</span
                            >
                            <span dir="ltr" role="presentation"
                              >with basic or no software development skills can
                              define sensor-based experiment scenarios with </span
                            ><span dir="ltr" role="presentation"
                              >less effort</span
                            >
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="200"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/07/Emotion-Recognition.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Emotion-Recognition.png         490w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Emotion-Recognition-300x200.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Emotion-Recognition-219x146.png 219w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Emotion-Recognition-50x33.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/Emotion-Recognition-113x75.png  113w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Emotion Recognition in Conversations Using Brain and
                            Physiological Signals
                          </h5>
                          <small
                            >Nastaran Saffaryazdi , Yenushka Goonesekera ,
                            Nafiseh Saffaryazdi , Nebiyou Daniel Hailemariam ,
                            Ebasa Girma Temesgen , Suranga Nanayakkara ,
                            Elizabeth Broadbent , Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Saffaryazdi, N., Goonesekera, Y., Saffaryazdi, N.,
                            Hailemariam, N. D., Temesgen, E. G., Nanayakkara,
                            S., ... & Billinghurst, M. (2022, March). Emotion
                            Recognition in Conversations Using Brain and
                            Physiological Signals. In 27th International
                            Conference on Intelligent User Interfaces (pp.
                            229-242).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{saffaryazdi2022emotion,<br />
                            title={Emotion recognition in conversations using
                            brain and physiological signals},<br />
                            author={Saffaryazdi, Nastaran and Goonesekera,
                            Yenushka and Saffaryazdi, Nafiseh and Hailemariam,
                            Nebiyou Daniel and Temesgen, Ebasa Girma and
                            Nanayakkara, Suranga and Broadbent, Elizabeth and
                            Billinghurst, Mark},<br />
                            booktitle={27th International Conference on
                            Intelligent User Interfaces},<br />
                            pages={229--242},<br />
                            year={2022}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/pdf/10.1145/3490099.3511148"
                              target="_blank"
                              >https://dl.acm.org/doi/pdf/10.1145/3490099.3511148</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Emotions are complicated psycho-physiological
                            processes that are related to numerous external and
                            internal changes in the body. They play an essential
                            role in human-human interaction and can be important
                            for human-machine interfaces. Automatically
                            recognizing emotions in conversation could be
                            applied in many application domains like
                            health-care, education, social interactions,
                            entertainment, and more. Facial expressions, speech,
                            and body gestures are primary cues that have been
                            widely used for recognizing emotions in
                            conversation. However, these cues can be ineffective
                            as they cannot reveal underlying emotions when
                            people involuntarily or deliberately conceal their
                            emotions. Researchers have shown that analyzing
                            brain activity and physiological signals can lead to
                            more reliable emotion recognition since they
                            generally cannot be controlled. However, these body
                            responses in emotional situations have been rarely
                            explored in interactive tasks like conversations.
                            This paper explores and discusses the performance
                            and challenges of using brain activity and other
                            physiological signals in recognizing emotions in a
                            face-to-face conversation. We present an
                            experimental setup for stimulating spontaneous
                            emotions using a face-to-face conversation and
                            creating a dataset of the brain and physiological
                            activity. We then describe our analysis strategies
                            for recognizing emotions using
                            Electroencephalography (EEG), Photoplethysmography
                            (PPG), and Galvanic Skin Response (GSR) signals in
                            subject-dependent and subject-independent
                            approaches. Finally, we describe new directions for
                            future research in conversational emotion
                            recognition and the limitations and challenges of
                            our approach.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="197"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching.png         756w,
                                https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-300x197.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-223x146.png 223w,
                                https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-50x33.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/04/VR-Sketching-114x75.png  114w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Asymmetric interfaces with stylus and gesture for VR
                            sketching
                          </h5>
                          <small
                            >Qianyuan Zou; Huidong Bai; Lei Gao; Allan Fowler;
                            Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zou, Q., Bai, H., Gao, L., Fowler, A., &
                            Billinghurst, M. (2022, March). Asymmetric
                            interfaces with stylus and gesture for VR sketching.
                            In 2022 IEEE Conference on Virtual Reality and 3D
                            User Interfaces Abstracts and Workshops (VRW) (pp.
                            968-969). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{zou2022asymmetric,<br />
                            title={Asymmetric interfaces with stylus and gesture
                            for VR sketching},<br />
                            author={Zou, Qianyuan and Bai, Huidong and Gao, Lei
                            and Fowler, Allan and Billinghurst, Mark},<br />
                            booktitle={2022 IEEE Conference on Virtual Reality
                            and 3D User Interfaces Abstracts and Workshops
                            (VRW)},<br />
                            pages={968--969},<br />
                            year={2022},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9757444/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9757444/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual Reality (VR) can be used for design and
                            artistic applications. However, traditional
                            symmetrical input devices are not specifically
                            designed as creative tools and may not fully meet
                            artist needs. In this demonstration, we present a
                            variety of tool-based asymmetric VR interfaces to
                            assist artists to create artwork with better
                            performance and easier effort. These interaction
                            methods allow artists to hold different tools in
                            their hands, such as wearing a data glove on the
                            left hand and holding a stylus in the right-hand. We
                            demonstrate this by showing a stylus and glove based
                            sketching interface. We conducted a pilot study
                            showing that most users prefer to create art with
                            different tools in both hands.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="225"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="eyemR-Talk system overview: Illustration and demonstration of the system setup, gaze states, and shared gaze indicator interface designs"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk.png         921w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-300x225.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-768x575.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-195x146.png 195w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-50x37.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-100x75.png  100w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Using Speech to Visualise Shared Gaze Cues in MR
                            Remote Collaboration
                          </h5>
                          <small
                            >Allison Jing; Gun Lee; Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Jing, A., Lee, G., & Billinghurst, M. (2022, March).
                            Using Speech to Visualise Shared Gaze Cues in MR
                            Remote Collaboration. In 2022 IEEE Conference on
                            Virtual Reality and 3D User Interfaces (VR) (pp.
                            250-259). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{jing2022using,<br />
                            title={Using Speech to Visualise Shared Gaze Cues in
                            MR Remote Collaboration},<br />
                            author={Jing, Allison and Lee, Gun and Billinghurst,
                            Mark},<br />
                            booktitle={2022 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={250--259},<br />
                            year={2022},<br />
                            organization={IEEE}<br />
                            }<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9756787"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9756787</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we present a 360° panoramic Mixed
                            Reality (MR) sys-tem that visualises shared gaze
                            cues using contextual speech input to improve task
                            coordination. We conducted two studies to evaluate
                            the design of the MR gaze-speech interface exploring
                            the combinations of visualisation style and context
                            control level. Findings from the first study suggest
                            that an explicit visual form that directly connects
                            the collaborators’ shared gaze to the contextual
                            conversation is preferred. The second study
                            indicates that the gaze-speech modality shortens the
                            coordination time to attend to the shared interest,
                            making the communication more natural and the
                            collaboration more effective. Qualitative feedback
                            also suggest that having a constant joint gaze
                            indicator provides a consistent bi-directional view
                            while establishing a sense of co-presence during
                            task collaboration. We discuss the implications for
                            the design of collaborative MR systems and
                            directions for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="190"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/07/noimage.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage.jpg         301w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-300x190.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-230x146.jpg 230w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-50x32.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-118x75.jpg  118w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Jamming in MR: Towards Real-Time Music Collaboration
                            in Mixed Reality
                          </h5>
                          <small
                            >Ruben Schlagowski; Kunal Gupta; Silvan Mertes; Mark
                            Billinghurst; Susanne Metzner; Elisabeth
                            André</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Schlagowski, R., Gupta, K., Mertes, S.,
                            Billinghurst, M., Metzner, S., & André, E. (2022,
                            March). Jamming in MR: Towards Real-Time Music
                            Collaboration in Mixed Reality. In 2022 IEEE
                            Conference on Virtual Reality and 3D User Interfaces
                            Abstracts and Workshops (VRW) (pp. 854-855). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{schlagowski2022jamming,<br />
                            title={Jamming in MR: towards real-time music
                            collaboration in mixed reality},<br />
                            author={Schlagowski, Ruben and Gupta, Kunal and
                            Mertes, Silvan and Billinghurst, Mark and Metzner,
                            Susanne and Andr{\'e}, Elisabeth},<br />
                            booktitle={2022 IEEE Conference on Virtual Reality
                            and 3D User Interfaces Abstracts and Workshops
                            (VRW)},<br />
                            pages={854--855},<br />
                            year={2022},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9757686/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9757686/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Recent pandemic-related contact restrictions have
                            made it difficult for musicians to meet in person to
                            make music. As a result, there has been an increased
                            demand for applications that enable remote and
                            real-time music collaboration. One desirable goal
                            here is to give musicians a sense of social
                            presence, to make them feel that they are “on site”
                            with their musical partners. We conducted a focus
                            group study to investigate the impact of remote
                            jamming on users' affect. Further, we gathered user
                            requirements for a Mixed Reality system that enables
                            real-time jamming and developed a prototype based on
                            these findings.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="190"
                              src="https://empathiccomputing.org/wp-content/uploads/2022/07/noimage.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage.jpg         301w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-300x190.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-230x146.jpg 230w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-50x32.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2022/07/noimage-118x75.jpg  118w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Supporting Jury Understanding of Expert Evidence in
                            a Virtual Environment
                          </h5>
                          <small
                            >Carolin Reichherzer; Andrew Cunningham; Jason Barr;
                            Tracey Coleman; Kurt McManus; Dion Sheppard; Scott
                            Coussens; Mark Kohler; Mark Billinghurst; Bruce H.
                            Thomas</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Reichherzer, C., Cunningham, A., Barr, J., Coleman,
                            T., McManus, K., Sheppard, D., ... & Thomas, B. H.
                            (2022, March). Supporting Jury Understanding of
                            Expert Evidence in a Virtual Environment. In 2022
                            IEEE Conference on Virtual Reality and 3D User
                            Interfaces (VR) (pp. 615-624). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{reichherzer2022supporting,<br />
                            title={Supporting Jury Understanding of Expert
                            Evidence in a Virtual Environment},<br />
                            author={Reichherzer, Carolin and Cunningham, Andrew
                            and Barr, Jason and Coleman, Tracey and McManus,
                            Kurt and Sheppard, Dion and Coussens, Scott and
                            Kohler, Mark and Billinghurst, Mark and Thomas,
                            Bruce H},<br />
                            booktitle={2022 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)},<br />
                            pages={615--624},<br />
                            year={2022},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9756754"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9756754</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This work investigates the use of Virtual Reality
                            (VR) to present forensic evidence to the jury in a
                            courtroom trial. The findings of a
                            between-participant user study on comprehension of
                            an expert statement are presented, examining the
                            benefits and issues of using VR compared to
                            traditional courtroom presentation (being still
                            images). Participants listened to a forensic
                            scientist explain bloodstain spatter patterns while
                            viewing a mock crime scene in either VR or as still
                            images in video format. Under these conditions, we
                            compared understanding of the expert domain, mental
                            effort and content recall. We found that VR
                            significantly improves the understanding of spatial
                            information and knowledge acquisition. We also
                            identify different patterns of user behaviour
                            depending on the display method. We conclude with
                            suggestions on how to best adapt evidence
                            presentation to VR.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="155"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/05/Sketch-2.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/05/Sketch-2.jpg         440w,
                                https://empathiccomputing.org/wp-content/uploads/2023/05/Sketch-2-300x155.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/05/Sketch-2-260x135.jpg 260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/05/Sketch-2-50x26.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/05/Sketch-2-145x75.jpg  145w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Tool-based asymmetric interaction for selection in
                            VR.
                          </h5>
                          <small
                            >Qianyuan Zou; Huidong Bai; Gun Lee; Allan Fowler;
                            Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zou, Q., Bai, H., Zhang, Y., Lee, G., Allan, F., &
                            Mark, B. (2021). Tool-based asymmetric interaction
                            for selection in vr. In SIGGRAPH Asia 2021 Technical
                            Communications (pp. 1-4).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @incollection{zou2021tool,<br />
                            title={Tool-based asymmetric interaction for
                            selection in vr},<br />
                            author={Zou, Qianyuan and Bai, Huidong and Zhang,
                            Yuewei and Lee, Gun and Allan, Fowler and Mark,
                            Billinghurst},<br />
                            booktitle={SIGGRAPH Asia 2021 Technical
                            Communications},<br />
                            pages={1--4},<br />
                            year={2021}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3478512.3488615"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3478512.3488615</a
                            ><br />Video:
                            <a
                              href="https://www.youtube.com/watch?v=FVk5lWtntGk"
                              target="_blank"
                              >https://www.youtube.com/watch?v=FVk5lWtntGk</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Mainstream Virtual Reality (VR) devices on the
                            market nowadays mostly use symmetric interaction
                            design for input, yet common practice by artists
                            suggests asymmetric interaction using different
                            input tools in each hand could be a better
                            alternative for 3D modeling tasks in VR. In this
                            paper, we explore the performance and usability of a
                            tool-based asymmetric interaction method for a 3D
                            object selection task in VR and compare it with a
                            symmetric interface. The symmetric VR interface uses
                            two identical handheld controllers to select points
                            on a sphere, while the asymmetric interface uses a
                            handheld controller and a stylus. We conducted a
                            user study to compare these two interfaces and found
                            that the asymmetric system was faster, required less
                            workload, and was rated with better usability. We
                            also discuss the opportunities for tool-based
                            asymmetric input to optimize VR art workflows and
                            future research directions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            haptic HONGI: Reflections on collaboration in the
                            transdisciplinary creation of an AR artwork in
                            Creating Digitally
                          </h5>
                          <small
                            >Gunn, M., Campbell, A., Billinghurst, M.,
                            Sasikumar, P., Lawn, W., Muthukumarana, S</small
                          >
                          <p style="font-style: italic; font-size: 12px"></p>
                          <div class="links-area"></div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Jitsi360: Using 360 images for live tours.
                          </h5>
                          <small
                            >Nassani, A., Bai, H., & Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px"></p>
                          <div class="links-area"></div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Designing, Prototyping and Testing of 360-degree
                            Spatial Audio Conferencing for Virtual Tours.
                          </h5>
                          <small
                            >Nassani, A., Barde, A., Bai, H., Nanayakkara, S., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px"></p>
                          <div class="links-area"></div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Implementation of Attention-Based Spatial Audio for
                            360° Environments.
                          </h5>
                          <small
                            >Nassani, A., Barde, A., Bai, H., Nanayakkara, S., &
                            Billinghurst, M.</small
                          >
                          <p style="font-style: italic; font-size: 12px"></p>
                          <div class="links-area"></div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="210"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="The prototype system"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze.png         929w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze-300x210.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze-768x537.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze-209x146.png 209w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze-50x35.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Sharing-Gaze-107x75.png  107w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The Impact of Sharing Gaze Behaviours in
                            Collaborative Mixed Reality
                          </h5>
                          <small
                            >Allison Jing , Kieran May , Brandon Matthews , Gun
                            Lee , Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Allison Jing, Kieran May, Brandon Matthews, Gun Lee,
                            and Mark Billinghurst. 2022. The Impact of Sharing
                            Gaze Behaviours in Collaborative Mixed Reality.
                            Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article
                            463 (November 2022), 27 pages.
                            https://doi.org/10.1145/3555564
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{10.1145/3555564,<br />
                            author = {Jing, Allison and May, Kieran and
                            Matthews, Brandon and Lee, Gun and Billinghurst,
                            Mark},<br />
                            title = {The Impact of Sharing Gaze Behaviours in
                            Collaborative Mixed Reality},<br />
                            year = {2022},<br />
                            issue_date = {November 2022},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            volume = {6},<br />
                            number = {CSCW2},<br />
                            url = {https://doi.org/10.1145/3555564},<br />
                            doi = {10.1145/3555564},<br />
                            abstract = {In a remote collaboration involving a
                            physical task, visualising gaze behaviours may
                            compensate for other unavailable communication
                            channels. In this paper, we report on a 360°
                            panoramic Mixed Reality (MR) remote collaboration
                            system that shares gaze behaviour visualisations
                            between a local user in Augmented Reality and a
                            remote collaborator in Virtual Reality. We conducted
                            two user studies to evaluate the design of MR gaze
                            interfaces and the effect of gaze behaviour (on/off)
                            and gaze style (bi-/uni-directional). The results
                            indicate that gaze visualisations amplify meaningful
                            joint attention and improve co-presence compared to
                            a no gaze condition. Gaze behaviour visualisations
                            enable communication to be less verbally complex
                            therefore lowering collaborators' cognitive load
                            while improving mutual understanding. Users felt
                            that bi-directional behaviour visualisation, showing
                            both collaborator's gaze state, was the preferred
                            condition since it enabled easy identification of
                            shared interests and task progress.},<br />
                            journal = {Proc. ACM Hum.-Comput. Interact.},<br />
                            month = {nov},<br />
                            articleno = {463},<br />
                            numpages = {27},<br />
                            keywords = {gaze visualization, mixed reality remote
                            collaboration, human-computer interaction}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3555564"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3555564</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In a remote collaboration involving a physical task,
                            visualising gaze behaviours may compensate for other
                            unavailable communication channels. In this paper,
                            we report on a 360° panoramic Mixed Reality (MR)
                            remote collaboration system that shares gaze
                            behaviour visualisations between a local user in
                            Augmented Reality and a remote collaborator in
                            Virtual Reality. We conducted two user studies to
                            evaluate the design of MR gaze interfaces and the
                            effect of gaze behaviour (on/off) and gaze style
                            (bi-/uni-directional). The results indicate that
                            gaze visualisations amplify meaningful joint
                            attention and improve co-presence compared to a no
                            gaze condition. Gaze behaviour visualisations enable
                            communication to be less verbally complex therefore
                            lowering collaborators' cognitive load while
                            improving mutual understanding. Users felt that
                            bi-directional behaviour visualisation, showing both
                            collaborator's gaze state, was the preferred
                            condition since it enabled easy identification of
                            shared interests and task progress.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="148"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="Mixed Reality Remote Collaboration System supporting Near-Gaze Interface"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze.png          1050w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze-300x148.png   300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze-768x380.png   768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze-1024x506.png 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze-260x129.png   260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze-50x25.png      50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Comparing-Gaze-150x75.png    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Comparing Gaze-Supported Modalities with Empathic
                            Mixed Reality Interfaces in Remote Collaboration
                          </h5>
                          <small
                            >Allison Jing; Kunal Gupta; Jeremy McDade; Gun A.
                            Lee; Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            A. Jing, K. Gupta, J. McDade, G. A. Lee and M.
                            Billinghurst, "Comparing Gaze-Supported Modalities
                            with Empathic Mixed Reality Interfaces in Remote
                            Collaboration," 2022 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR), Singapore,
                            Singapore, 2022, pp. 837-846, doi:
                            10.1109/ISMAR55827.2022.00102.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @INPROCEEDINGS{9995367,<br />
                            author={Jing, Allison and Gupta, Kunal and McDade,
                            Jeremy and Lee, Gun A. and Billinghurst, Mark},<br />
                            booktitle={2022 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR)}, <br />
                            title={Comparing Gaze-Supported Modalities with
                            Empathic Mixed Reality Interfaces in Remote
                            Collaboration}, <br />
                            year={2022},<br />
                            volume={},<br />
                            number={},<br />
                            pages={837-846},<br />
                            doi={10.1109/ISMAR55827.2022.00102}}<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/9995367/authors#authors"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/9995367/authors#authors</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we share real-time collaborative gaze
                            behaviours, hand pointing, gesturing, and heart rate
                            visualisations between remote collaborators using a
                            live 360 ° panoramic-video based Mixed Reality (MR)
                            system. We first ran a pilot study to explore visual
                            designs to combine communication cues with
                            biofeedback (heart rate), aiming to understand user
                            perceptions of empathic collaboration. We then
                            conducted a formal study to investigate the effect
                            of modality (Gaze+Hand, Hand-only) and interface
                            (Near-Gaze, Embodied). The results show that the
                            Gaze+Hand modality in a Near-Gaze interface is
                            significantly better at reducing task load,
                            improving co-presence, enhancing understanding and
                            tightening collaborative behaviours compared to the
                            conventional Embodied hand-only experience. Ranked
                            as the most preferred condition, the Gaze+Hand in
                            Near-Gaze condition is perceived to reduce the need
                            for dividing attention to the collaborator’s
                            physical location, although it feels slightly less
                            natural compared to the embodied visualisations. In
                            addition, the Gaze+Hand conditions also led to more
                            joint attention and less hand pointing to align
                            mutual understanding. Lastly, we provide a design
                            guideline to summarize what we have learned from the
                            studies on the representation between modality,
                            interface, and biofeedback.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="164"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="System Overview"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze.png         952w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze-300x164.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze-768x419.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze-260x142.png 260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze-50x27.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Near_Gaze-138x75.png  138w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Near-Gaze Visualisations of Empathic Communication
                            Cues in Mixed Reality Collaboration
                          </h5>
                          <small
                            >Allison Jing; Kunal Gupta; Jeremy McDade; Gun A.
                            Lee; Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Allison Jing, Kunal Gupta, Jeremy McDade, Gun Lee,
                            and Mark Billinghurst. 2022. Near-Gaze
                            Visualisations of Empathic Communication Cues in
                            Mixed Reality Collaboration. In ACM SIGGRAPH 2022
                            Posters (SIGGRAPH '22). Association for Computing
                            Machinery, New York, NY, USA, Article 29, 1–2.
                            https://doi.org/10.1145/3532719.3543213
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3532719.3543213,<br />
                            author = {Jing, Allison and Gupta, Kunal and McDade,
                            Jeremy and Lee, Gun and Billinghurst, Mark},<br />
                            title = {Near-Gaze Visualisations of Empathic
                            Communication Cues in Mixed Reality
                            Collaboration},<br />
                            year = {2022},<br />
                            isbn = {9781450393614},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3532719.3543213},<br />
                            doi = {10.1145/3532719.3543213},<br />
                            abstract = {In this poster, we present a live
                            360°&nbsp;panoramic-video based empathic Mixed
                            Reality (MR) collaboration system that shares
                            various Near-Gaze non-verbal communication cues
                            including gaze, hand pointing, gesturing, and heart
                            rate visualisations in real-time. The preliminary
                            results indicate that the interface with the
                            partner’s communication cues visualised close to the
                            gaze point allows users to focus without dividing
                            attention to the collaborator’s physical body
                            movements yet still effectively communicate. Shared
                            gaze visualisations coupled with deictic languages
                            are primarily used to affirm joint attention and
                            mutual understanding, while hand pointing and
                            gesturing are used as secondary. Our approach
                            provides a new way to help enable effective remote
                            collaboration through varied empathic communication
                            visualisations and modalities which covers different
                            task properties and spatial setups.},<br />
                            booktitle = {ACM SIGGRAPH 2022 Posters},<br />
                            articleno = {29},<br />
                            numpages = {2},<br />
                            location = {Vancouver, BC, Canada},<br />
                            series = {SIGGRAPH '22}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3532719.3543213"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3532719.3543213</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this poster, we present a live 360°
                            panoramic-video based empathic Mixed Reality (MR)
                            collaboration system that shares various Near-Gaze
                            non-verbal communication cues including gaze, hand
                            pointing, gesturing, and heart rate visualisations
                            in real-time. The preliminary results indicate that
                            the interface with the partner’s communication cues
                            visualised close to the gaze point allows users to
                            focus without dividing attention to the
                            collaborator’s physical body movements yet still
                            effectively communicate. Shared gaze visualisations
                            coupled with deictic languages are primarily used to
                            affirm joint attention and mutual understanding,
                            while hand pointing and gesturing are used as
                            secondary. Our approach provides a new way to help
                            enable effective remote collaboration through varied
                            empathic communication visualisations and modalities
                            which covers different task properties and spatial
                            setups.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="225"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="eyemR-Talk system overview: Illustration and demonstration of the system setup, gaze states, and shared gaze indicator interface designs"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk.png         921w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-300x225.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-768x575.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-195x146.png 195w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-50x37.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-Talk-100x75.png  100w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            eyemR-Talk: Using Speech to Visualise Shared MR Gaze
                            Cues
                          </h5>
                          <small
                            >Allison Jing , Brandon Matthews , Kieran May ,
                            Thomas Clarke , Gun Lee , Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Allison Jing, Brandon Matthews, Kieran May, Thomas
                            Clarke, Gun Lee, and Mark Billinghurst. 2021.
                            EyemR-Talk: Using Speech to Visualise Shared MR Gaze
                            Cues. In SIGGRAPH Asia 2021 Posters (SA '21
                            Posters). Association for Computing Machinery, New
                            York, NY, USA, Article 16, 1–2.
                            https://doi.org/10.1145/3476124.3488618
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3476124.3488618,<br />
                            author = {Jing, Allison and Matthews, Brandon and
                            May, Kieran and Clarke, Thomas and Lee, Gun and
                            Billinghurst, Mark},<br />
                            title = {EyemR-Talk: Using Speech to Visualise
                            Shared MR Gaze Cues},<br />
                            year = {2021},<br />
                            isbn = {9781450386876},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3476124.3488618},<br />
                            doi = {10.1145/3476124.3488618},<br />
                            abstract = {In this poster we present eyemR-Talk, a
                            Mixed Reality (MR) collaboration system that uses
                            speech input to trigger shared gaze visualisations
                            between remote users. The system uses 360° panoramic
                            video to support collaboration between a local user
                            in the real world in an Augmented Reality (AR) view
                            and a remote collaborator in Virtual Reality (VR).
                            Using specific speech phrases to turn on virtual
                            gaze visualisations, the system enables contextual
                            speech-gaze interaction between collaborators. The
                            overall benefit is to achieve more natural gaze
                            awareness, leading to better communication and more
                            effective collaboration.},<br />
                            booktitle = {SIGGRAPH Asia 2021 Posters},<br />
                            articleno = {16},<br />
                            numpages = {2},<br />
                            keywords = {Mixed Reality remote collaboration, gaze
                            visualization, speech input},<br />
                            location = {Tokyo, Japan},<br />
                            series = {SA '21 Posters}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3476124.3488618"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3476124.3488618</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this poster we present eyemR-Talk, a Mixed
                            Reality (MR) collaboration system that uses speech
                            input to trigger shared gaze visualisations between
                            remote users. The system uses 360° panoramic video
                            to support collaboration between a local user in the
                            real world in an Augmented Reality (AR) view and a
                            remote collaborator in Virtual Reality (VR). Using
                            specific speech phrases to turn on virtual gaze
                            visualisations, the system enables contextual
                            speech-gaze interaction between collaborators. The
                            overall benefit is to achieve more natural gaze
                            awareness, leading to better communication and more
                            effective collaboration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="96"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="The eyemR-Vis prototype system, showing an AR user (HoloLens2) sharing gaze cues with a VR user (HTC Vive Pro Eye)"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2.png          1432w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2-300x96.png    300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2-768x247.png   768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2-1024x329.png 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2-260x84.png    260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2-50x16.png      50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-2-150x48.png    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            eyemR-Vis: Using Bi-Directional Gaze Behavioural
                            Cues to Improve Mixed Reality Remote Collaboration
                          </h5>
                          <small
                            >Allison Jing , Kieran William May , Mahnoor Naeem ,
                            Gun Lee , Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Allison Jing, Kieran William May, Mahnoor Naeem, Gun
                            Lee, and Mark Billinghurst. 2021. EyemR-Vis: Using
                            Bi-Directional Gaze Behavioural Cues to Improve
                            Mixed Reality Remote Collaboration. In Extended
                            Abstracts of the 2021 CHI Conference on Human
                            Factors in Computing Systems (CHI EA '21).
                            Association for Computing Machinery, New York, NY,
                            USA, Article 283, 1–7.
                            https://doi.org/10.1145/3411763.3451844
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3411763.3451844,<br />
                            author = {Jing, Allison and May, Kieran William and
                            Naeem, Mahnoor and Lee, Gun and Billinghurst,
                            Mark},<br />
                            title = {EyemR-Vis: Using Bi-Directional Gaze
                            Behavioural Cues to Improve Mixed Reality Remote
                            Collaboration},<br />
                            year = {2021},<br />
                            isbn = {9781450380959},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3411763.3451844},<br />
                            doi = {10.1145/3411763.3451844},<br />
                            abstract = {Gaze is one of the most important
                            communication cues in face-to-face collaboration.
                            However, in remote collaboration, sharing dynamic
                            gaze information is more difficult. In this
                            research, we investigate how sharing gaze
                            behavioural cues can improve remote collaboration in
                            a Mixed Reality (MR) environment. To do this, we
                            developed eyemR-Vis, a 360 panoramic Mixed Reality
                            remote collaboration system that shows gaze
                            behavioural cues as bi-directional spatial virtual
                            visualisations shared between a local host and a
                            remote collaborator. Preliminary results from an
                            exploratory study indicate that using virtual cues
                            to visualise gaze behaviour has the potential to
                            increase co-presence, improve gaze awareness,
                            encourage collaboration, and is inclined to be less
                            physically demanding or mentally distracting.},<br />
                            booktitle = {Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            articleno = {283},<br />
                            numpages = {7},<br />
                            keywords = {Human-Computer Interaction, Gaze
                            Visualisation, Mixed Reality Remote Collaboration,
                            CSCW},<br />
                            location = {Yokohama, Japan},<br />
                            series = {CHI EA '21}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3411763.3451844"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3411763.3451844</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Gaze is one of the most important communication cues
                            in face-to-face collaboration. However, in remote
                            collaboration, sharing dynamic gaze information is
                            more difficult. In this research, we investigate how
                            sharing gaze behavioural cues can improve remote
                            collaboration in a Mixed Reality (MR) environment.
                            To do this, we developed eyemR-Vis, a 360 panoramic
                            Mixed Reality remote collaboration system that shows
                            gaze behavioural cues as bi-directional spatial
                            virtual visualisations shared between a local host
                            and a remote collaborator. Preliminary results from
                            an exploratory study indicate that using virtual
                            cues to visualise gaze behaviour has the potential
                            to increase co-presence, improve gaze awareness,
                            encourage collaboration, and is inclined to be less
                            physically demanding or mentally distracting.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="99"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="The eyemR-Vis prototype system, showing an AR user (HoloLens2) sharing gaze cues with a VR user (HTC Vive Pro Eye)"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison.png          1557w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-300x99.png    300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-768x254.png   768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-1024x338.png 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-260x86.png    260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-50x17.png      50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/EyeR-allison-150x50.png    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            eyemR-Vis: A Mixed Reality System to Visualise
                            Bi-Directional Gaze Behavioural Cues Between Remote
                            Collaborators
                          </h5>
                          <small
                            >Allison Jing , Kieran William May , Mahnoor Naeem ,
                            Gun Lee , Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Allison Jing, Kieran William May, Mahnoor Naeem, Gun
                            Lee, and Mark Billinghurst. 2021. EyemR-Vis: A Mixed
                            Reality System to Visualise Bi-Directional Gaze
                            Behavioural Cues Between Remote Collaborators. In
                            Extended Abstracts of the 2021 CHI Conference on
                            Human Factors in Computing Systems (CHI EA '21).
                            Association for Computing Machinery, New York, NY,
                            USA, Article 188, 1–4.
                            https://doi.org/10.1145/3411763.3451545
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3411763.3451545,<br />
                            author = {Jing, Allison and May, Kieran William and
                            Naeem, Mahnoor and Lee, Gun and Billinghurst,
                            Mark},<br />
                            title = {EyemR-Vis: A Mixed Reality System to
                            Visualise Bi-Directional Gaze Behavioural Cues
                            Between Remote Collaborators},<br />
                            year = {2021},<br />
                            isbn = {9781450380959},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3411763.3451545},<br />
                            doi = {10.1145/3411763.3451545},<br />
                            abstract = {This demonstration shows eyemR-Vis, a
                            360 panoramic Mixed Reality collaboration system
                            that translates gaze behavioural cues to
                            bi-directional visualisations between a local host
                            (AR) and a remote collaborator (VR). The system is
                            designed to share dynamic gaze behavioural cues as
                            bi-directional spatial virtual visualisations
                            between a local host and a remote collaborator. This
                            enables richer communication of gaze through four
                            visualisation techniques: browse, focus,
                            mutual-gaze, and fixated circle-map. Additionally,
                            our system supports simple bi-directional avatar
                            interaction as well as panoramic video zoom. This
                            makes interaction in the normally constrained remote
                            task space more flexible and relatively natural. By
                            showing visual communication cues that are
                            physically inaccessible in the remote task space
                            through reallocating and visualising the existing
                            ones, our system aims to provide a more engaging and
                            effective remote collaboration experience.},<br />
                            booktitle = {Extended Abstracts of the 2021 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            articleno = {188},<br />
                            numpages = {4},<br />
                            keywords = {Gaze Visualisation, Human-Computer
                            Interaction, Mixed Reality Remote Collaboration,
                            CSCW},<br />
                            location = {Yokohama, Japan},<br />
                            series = {CHI EA '21}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3411763.3451545"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3411763.3451545</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This demonstration shows eyemR-Vis, a 360 panoramic
                            Mixed Reality collaboration system that translates
                            gaze behavioural cues to bi-directional
                            visualisations between a local host (AR) and a
                            remote collaborator (VR). The system is designed to
                            share dynamic gaze behavioural cues as
                            bi-directional spatial virtual visualisations
                            between a local host and a remote collaborator. This
                            enables richer communication of gaze through four
                            visualisation techniques: browse, focus,
                            mutual-gaze, and fixated circle-map. Additionally,
                            our system supports simple bi-directional avatar
                            interaction as well as panoramic video zoom. This
                            makes interaction in the normally constrained remote
                            task space more flexible and relatively natural. By
                            showing visual communication cues that are
                            physically inaccessible in the remote task space
                            through reallocating and visualising the existing
                            ones, our system aims to provide a more engaging and
                            effective remote collaboration experience.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="81"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="Sankey diagram of EEG analysis pipeline"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee.png          1264w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee-300x81.png    300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee-768x207.png   768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee-1024x275.png 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee-260x70.png    260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee-50x13.png      50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Brain-Avtivity-Enhee-150x40.png    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Brain activity during cybersickness: a scoping
                            review
                          </h5>
                          <small
                            >Eunhee Chang, Mark Billinghurst, Byounghyun
                            Yoo</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Chang, E., Billinghurst, M., & Yoo, B. (2023). Brain
                            activity during cybersickness: a scoping review.
                            Virtual Reality, 1-25.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{chang2023brain,<br />
                            title={Brain activity during cybersickness: a
                            scoping review},<br />
                            author={Chang, Eunhee and Billinghurst, Mark and
                            Yoo, Byounghyun},<br />
                            journal={Virtual Reality},<br />
                            pages={1--25},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-023-00795-y#citeas"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-023-00795-y#citeas</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual reality (VR) experiences can cause a range
                            of negative symptoms such as nausea, disorientation,
                            and oculomotor discomfort, which is collectively
                            called cybersickness. Previous studies have
                            attempted to develop a reliable measure for
                            detecting cybersickness instead of using
                            questionnaires, and electroencephalogram (EEG) has
                            been regarded as one of the possible alternatives.
                            However, despite the increasing interest, little is
                            known about which brain activities are consistently
                            associated with cybersickness and what types of
                            methods should be adopted for measuring discomfort
                            through brain activity. We conducted a scoping
                            review of 33 experimental studies in cybersickness
                            and EEG found through database searches and
                            screening. To understand these studies, we organized
                            the pipeline of EEG analysis into four steps
                            (preprocessing, feature extraction, feature
                            selection, classification) and surveyed the
                            characteristics of each step. The results showed
                            that most studies performed frequency or
                            time-frequency analysis for EEG feature extraction.
                            A part of the studies applied a classification model
                            to predict cybersickness indicating an accuracy
                            between 79 and 100%. These studies tended to use
                            HMD-based VR with a portable EEG headset for
                            measuring brain activity. Most VR content shown was
                            scenic views such as driving or navigating a road,
                            and the age of participants was limited to people in
                            their 20 s. This scoping review contributes to
                            presenting an overview of cybersickness-related EEG
                            research and establishing directions for future
                            work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="213"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/TUI.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="Using an (a) explanation, (b) example, or (c) hint helper block, a brief summary of the code component, examples of its usage, and a guide as to the succeeding component in the statement may respectively be displayed next to the original syntax through 3D text annotation and graphical representation."
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI.png         614w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-300x213.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-206x146.png 206w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-50x36.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-106x75.png  106w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            An AR/TUI-supported Debugging Teaching Environment
                          </h5>
                          <small
                            >Dmitry Resnyansky , Mark Billinghurst , Arindam
                            Dey</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Resnyansky, D., Billinghurst, M., & Dey, A. (2019,
                            December). An AR/TUI-supported debugging teaching
                            environment. In Proceedings of the 31st Australian
                            Conference on Human-Computer-Interaction (pp.
                            590-594).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3369457.3369538,<br />
                            author = {Resnyansky, Dmitry and Billinghurst, Mark
                            and Dey, Arindam},<br />
                            title = {An AR/TUI-Supported Debugging Teaching
                            Environment},<br />
                            year = {2020},<br />
                            isbn = {9781450376969},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3369457.3369538},<br />
                            doi = {10.1145/3369457.3369538},<br />
                            abstract = {This paper presents research on the
                            potential application of Tangible and Augmented
                            Reality (AR) technology to computer science
                            education and the teaching of programming in
                            tertiary settings. An approach to an AR-supported
                            debugging-teaching prototype is outlined, focusing
                            on the design of an AR workspace that uses physical
                            markers to interact with content (code). We describe
                            a prototype which has been designed to actively
                            scaffold the student's development of the two
                            primary abilities necessary for effective debugging:
                            (1) the ability to read not just the code syntax,
                            but to understand the overall program structure
                            behind the code; and (2) the ability to
                            independently recall and apply the new knowledge to
                            produce new, working code structures.},<br />
                            booktitle = {Proceedings of the 31st Australian
                            Conference on Human-Computer-Interaction},<br />
                            pages = {590–594},<br />
                            numpages = {5},<br />
                            keywords = {tangible user interface, tertiary
                            education, debugging, Human-computer interaction,
                            augmented reality},<br />
                            location = {Fremantle, WA, Australia},<br />
                            series = {OzCHI '19}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3369457.3369538"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3369457.3369538</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper presents research on the potential
                            application of Tangible and Augmented Reality (AR)
                            technology to computer science education and the
                            teaching of programming in tertiary settings. An
                            approach to an AR-supported debugging-teaching
                            prototype is outlined, focusing on the design of an
                            AR workspace that uses physical markers to interact
                            with content (code). We describe a prototype which
                            has been designed to actively scaffold the student's
                            development of the two primary abilities necessary
                            for effective debugging: (1) the ability to read not
                            just the code syntax, but to understand the overall
                            program structure behind the code; and (2) the
                            ability to independently recall and apply the new
                            knowledge to produce new, working code structures.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="165"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="TUI/AR-based teaching programming tools"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education.png          1063w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education-300x165.png   300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education-768x421.png   768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education-1024x562.png 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education-260x143.png   260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education-50x27.png      50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/TUI-Education-137x75.png    137w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The potential of augmented reality for computer
                            science education
                          </h5>
                          <small
                            >Dmitry Resnyansky; Emin İbili; Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Resnyansky, D., Ibili, E., & Billinghurst, M. (2018,
                            December). The potential of augmented reality for
                            computer science education. In 2018 IEEE
                            International Conference on Teaching, Assessment,
                            and Learning for Engineering (TALE) (pp. 350-356).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @INPROCEEDINGS{8615331,<br />
                            author={Resnyansky, Dmitry and İbili, Emin and
                            Billinghurst, Mark},<br />
                            booktitle={2018 IEEE International Conference on
                            Teaching, Assessment, and Learning for Engineering
                            (TALE)}, <br />
                            title={The Potential of Augmented Reality for
                            Computer Science Education}, <br />
                            year={2018},<br />
                            volume={},<br />
                            number={},<br />
                            pages={350-356},<br />
                            doi={10.1109/TALE.2018.8615331}}<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/8615331"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/8615331</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Innovative approaches in the teaching of computer
                            science are required to address the needs of diverse
                            target audiences, including groups with minimal
                            mathematical background and insufficient abstract
                            thinking ability. In order to tackle this problem,
                            new pedagogical approaches that make use of
                            technologies such as Virtual and Augmented Reality,
                            Tangible User Interfaces, and 3D graphics are
                            needed. This paper draws upon relevant pedagogical
                            and technological literature to determine how
                            Augmented Reality can be more fully applied to
                            computer science education.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="77"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="Prototype system overview showing a remote expert worker immerse into the local worker’s environment to collaborate"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction.png          1256w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction-300x77.png    300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction-768x196.png   768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction-1024x262.png 1024w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction-260x66.png    260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction-50x13.png      50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/exploring-interaction-150x38.png    150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploring interaction techniques for 360 panoramas
                            inside a 3D reconstructed scene for mixed reality
                            remote collaboration
                          </h5>
                          <small
                            >Theophilus Teo, Mitchell Norman, Gun A. Lee, Mark
                            Billinghurst & Matt Adcock</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            T. Teo, M. Norman, G. A. Lee, M. Billinghurst and M.
                            Adcock. “Exploring interaction techniques for 360
                            panoramas inside a 3D reconstructed scene for mixed
                            reality remote collaboration.” In: J Multimodal User
                            Interfaces. (JMUI), 2020.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{teo2020exploring,<br />
                            title={Exploring interaction techniques for 360
                            panoramas inside a 3D reconstructed scene for mixed
                            reality remote collaboration},<br />
                            author={Teo, Theophilus and Norman, Mitchell and
                            Lee, Gun A and Billinghurst, Mark and Adcock,
                            Matt},<br />
                            journal={Journal on Multimodal User Interfaces},<br />
                            volume={14},<br />
                            pages={373--385},<br />
                            year={2020},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s12193-020-00343-x"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s12193-020-00343-x</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Remote collaboration using mixed reality (MR)
                            enables two separated workers to collaborate by
                            sharing visual cues. A local worker can share
                            his/her environment to the remote worker for a
                            better contextual understanding. However, prior
                            techniques were using either 360 video sharing or a
                            complicated 3D reconstruction configuration. This
                            limits the interactivity and practicality of the
                            system. In this paper we show an interactive and
                            easy-to-configure MR remote collaboration technique
                            enabling a local worker to easily share his/her
                            environment by integrating 360 panorama images into
                            a low-cost 3D reconstructed scene as photo-bubbles
                            and projective textures. This enables the remote
                            worker to visit past scenes on either an immersive
                            360 panoramic scenery, or an interactive 3D
                            environment. We developed a prototype and conducted
                            a user study comparing the two modes of how 360
                            panorama images could be used in a remote
                            collaboration system. Results suggested that both
                            photo-bubbles and projective textures can provide
                            high social presence, co-presence and low cognitive
                            load for solving tasks while each have its advantage
                            and limitations. For example, photo-bubbles are good
                            for a quick navigation inside the 3D environment
                            without depth perception while projective textures
                            are good for spatial understanding but require
                            physical efforts.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="284"
                              height="300"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/omniGlobeVR.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="The OmniGlobeVR enables a VR occupant to communicate and cooperate with multiple designers in the physical world."
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/omniGlobeVR.png         516w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/omniGlobeVR-284x300.png 284w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/omniGlobeVR-138x146.png 138w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/omniGlobeVR-47x50.png    47w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/omniGlobeVR-71x75.png    71w
                              "
                              sizes="(max-width: 284px) 100vw, 284px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            OmniGlobeVR: A Collaborative 360-Degree
                            Communication System for VR
                          </h5>
                          <small
                            >Zhengqing Li , Theophilus Teo , Liwei Chan , Gun
                            Lee , Matt Adcock , Mark Billinghurst , Hideki
                            Koike</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Z. Li, T. Teo, G. Lee, M. Adcock, M. Billinghurst,
                            H. Koike. “A collaborative 360-degree communication
                            system for VR”. In Proceedings of the 2020 Designing
                            Interactive Systems Conference (DIS2020). ACM, 2020.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3357236.3395429,<br />
                            author = {Li, Zhengqing and Teo, Theophilus and
                            Chan, Liwei and Lee, Gun and Adcock, Matt and
                            Billinghurst, Mark and Koike, Hideki},<br />
                            title = {OmniGlobeVR: A Collaborative 360-Degree
                            Communication System for VR},<br />
                            year = {2020},<br />
                            isbn = {9781450369749},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3357236.3395429},<br />
                            doi = {10.1145/3357236.3395429},<br />
                            abstract = {In this paper, we present a novel
                            collaboration tool, OmniGlobeVR, which is an
                            asymmetric system that supports communication and
                            collaboration between a VR user (occupant) and
                            multiple non-VR users (designers) across the virtual
                            and physical platform. OmniGlobeVR allows
                            designer(s) to explore the VR space from any point
                            of view using two view modes: a 360° first-person
                            mode and a third-person mode. In addition, a shared
                            gaze awareness cue is provided to further enhance
                            communication between the occupant and the
                            designer(s). Finally, the system has a face window
                            feature that allows designer(s) to share their
                            facial expressions and upper body view with the
                            occupant for exchanging and expressing information
                            using nonverbal cues. We conducted a user study to
                            evaluate the OmniGlobeVR, comparing three
                            conditions: (1) first-person mode with the face
                            window, (2) first-person mode with a solid window,
                            and (3) third-person mode with the face window. We
                            found that the first-person mode with the face
                            window required significantly less mental effort,
                            and provided better spatial presence, usability, and
                            understanding of the partner's focus. We discuss the
                            design implications of these results and directions
                            for future research.},<br />
                            booktitle = {Proceedings of the 2020 ACM Designing
                            Interactive Systems Conference},<br />
                            pages = {615–625},<br />
                            numpages = {11},<br />
                            keywords = {virtual reality, communication,
                            collaboration, mixed reality, spherical display,
                            360-degree camera},<br />
                            location = {Eindhoven, Netherlands},<br />
                            series = {DIS '20}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/10.1145/3357236.3395429"
                              target="_blank"
                              >https://dl.acm.org/doi/10.1145/3357236.3395429</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we present a novel collaboration
                            tool, OmniGlobeVR, which is an asymmetric system
                            that supports communication and collaboration
                            between a VR user (occupant) and multiple non-VR
                            users (designers) across the virtual and physical
                            platform. OmniGlobeVR allows designer(s) to explore
                            the VR space from any point of view using two view
                            modes: a 360° first-person mode and a third-person
                            mode. In addition, a shared gaze awareness cue is
                            provided to further enhance communication between
                            the occupant and the designer(s). Finally, the
                            system has a face window feature that allows
                            designer(s) to share their facial expressions and
                            upper body view with the occupant for exchanging and
                            expressing information using nonverbal cues. We
                            conducted a user study to evaluate the OmniGlobeVR,
                            comparing three conditions: (1) first-person mode
                            with the face window, (2) first-person mode with a
                            solid window, and (3) third-person mode with the
                            face window. We found that the first-person mode
                            with the face window required significantly less
                            mental effort, and provided better spatial presence,
                            usability, and understanding of the partner's focus.
                            We discuss the design implications of these results
                            and directions for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="115"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/360Drops.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="360Drops System Overview"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/360Drops.png         562w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/360Drops-300x115.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/360Drops-260x100.png 260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/360Drops-50x19.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/360Drops-150x58.png  150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            360Drops: Mixed Reality Remote Collaboration using
                            360 Panoramas within the 3D Scene
                          </h5>
                          <small
                            >Theophilus Teo , Gun A. Lee , Mark Billinghurst ,
                            Matt Adcock</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            T. Teo, G. A. Lee, M. Billinghurst and M. Adcock.
                            “360Drops: Mixed Reality Remove Collaboration using
                            360° Panoramas within the 3D Scene.” In: ACM
                            SIGGRAPH Conference and Exhibition on Computer
                            Graphics & Interactive Technologies in Asia. (SA
                            2019), Brisbane, Australia, 2019.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3355049.3360517,<br />
                            author = {Teo, Theophilus and A. Lee, Gun and
                            Billinghurst, Mark and Adcock, Matt},<br />
                            title = {360Drops: Mixed Reality Remote
                            Collaboration Using 360 Panoramas within the 3D
                            Scene*},<br />
                            year = {2019},<br />
                            isbn = {9781450369428},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3355049.3360517},<br />
                            doi = {10.1145/3355049.3360517},<br />
                            abstract = {Mixed Reality (MR) remote guidance has
                            become a practical solution for collaboration that
                            includes nonverbal communication. This research
                            focuses on integrating different types of MR remote
                            collaboration systems together allowing a new
                            variety for remote collaboration to extend its
                            features and user experience. In this demonstration,
                            we present 360Drops, a MR remote collaboration
                            system that uses 360 panorama images within 3D
                            reconstructed scenes. We introduce a new technique
                            to interact with multiple 360 Panorama Spheres in an
                            immersive 3D reconstructed scene. This allows a
                            remote user to switch between multiple 360 scenes
                            “live/static, past/present,” placed in a 3D
                            reconstructed scene to promote a better
                            understanding of space and interactivity through
                            verbal and nonverbal communication. We present the
                            system features and user experience to the attendees
                            of SIGGRAPH Asia 2019 through a live
                            demonstration.},<br />
                            booktitle = {SIGGRAPH Asia 2019 Emerging
                            Technologies},<br />
                            pages = {1–2},<br />
                            numpages = {2},<br />
                            keywords = {Remote Collaboration, Shared Experience,
                            Mixed Reality},<br />
                            location = {Brisbane, QLD, Australia},<br />
                            series = {SA '19}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3355049.3360517"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3355049.3360517</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Mixed Reality (MR) remote guidance has become a
                            practical solution for collaboration that includes
                            nonverbal communication. This research focuses on
                            integrating different types of MR remote
                            collaboration systems together allowing a new
                            variety for remote collaboration to extend its
                            features and user experience. In this demonstration,
                            we present 360Drops, a MR remote collaboration
                            system that uses 360 panorama images within 3D
                            reconstructed scenes. We introduce a new technique
                            to interact with multiple 360 Panorama Spheres in an
                            immersive 3D reconstructed scene. This allows a
                            remote user to switch between multiple 360 scenes
                            “live/static, past/present,” placed in a 3D
                            reconstructed scene to promote a better
                            understanding of space and interactivity through
                            verbal and nonverbal communication. We present the
                            system features and user experience to the attendees
                            of SIGGRAPH Asia 2019 through a live demonstration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="172"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="Prototype system overview"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration.png         843w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration-300x172.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration-768x441.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration-254x146.png 254w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration-50x29.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Remote-collaboration-131x75.png  131w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Technique for Mixed Reality Remote Collaboration
                            using 360° Panoramas in 3D Reconstructed Scenes
                          </h5>
                          <small
                            >Theophilus Teo , Ashkan F. Hayati , Gun A. Lee ,
                            Mark Billinghurst , Matt Adcock</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            T. Teo, A. F. Hayati, G. A. Lee, M. Billinghurst and
                            M. Adcock. “A Technique for Mixed Reality Remote
                            Collaboration using 360° Panoramas in 3D
                            Reconstructed Scenes.” In: ACM Symposium on Virtual
                            Reality Software and Technology. (VRST), Sydney,
                            Australia, 2019.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{teo2019technique,<br />
                            title={A technique for mixed reality remote
                            collaboration using 360 panoramas in 3d
                            reconstructed scenes},<br />
                            author={Teo, Theophilus and F. Hayati, Ashkan and A.
                            Lee, Gun and Billinghurst, Mark and Adcock,
                            Matt},<br />
                            booktitle={Proceedings of the 25th ACM Symposium on
                            Virtual Reality Software and Technology},<br />
                            pages={1--11},<br />
                            year={2019}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3359996.3364238"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3359996.3364238</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Mixed Reality (MR) remote collaboration provides an
                            enhanced immersive experience where a remote user
                            can provide verbal and nonverbal assistance to a
                            local user to increase the efficiency and
                            performance of the collaboration. This is usually
                            achieved by sharing the local user's environment
                            through live 360 video or a 3D scene, and using
                            visual cues to gesture or point at real objects
                            allowing for better understanding and collaborative
                            task performance. While most of prior work used one
                            of the methods to capture the surrounding
                            environment, there may be situations where users
                            have to choose between using 360 panoramas or 3D
                            scene reconstruction to collaborate, as each have
                            unique benefits and limitations. In this paper we
                            designed a prototype system that combines 360
                            panoramas into a 3D scene to introduce a novel way
                            for users to interact and collaborate with each
                            other. We evaluated the prototype through a user
                            study which compared the usability and performance
                            of our proposed approach to live 360 video
                            collaborative system, and we found that participants
                            enjoyed using different ways to access the local
                            user's environment although it took them longer time
                            to learn to use our system. We also collected
                            subjective feedback for future improvements and
                            provide directions for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="160"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="MR remote collaboration system overview"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360.png         989w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360-300x160.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360-768x408.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360-260x138.png 260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360-50x27.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-with-360-141x75.png  141w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Mixed Reality Remote Collaboration Combining 360
                            Video and 3D Reconstruction
                          </h5>
                          <small
                            >Theophilus Teo , Louise Lawrence , Gun A. Lee ,
                            Mark Billinghurst , Matt Adcock</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            T. Teo, L. Lawrence, G. A. Lee, M. Billinghurst, and
                            M. Adcock. (2019). “Mixed Reality Remote
                            Collaboration Combining 360 Video and 3D
                            Reconstruction”. In Proceedings of the 2019 CHI
                            Conference on Human Factors in Computing Systems
                            (CHI '19). ACM, New York, NY, USA, Paper 201, 14
                            pages.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3290605.3300431,<br />
                            author = {Teo, Theophilus and Lawrence, Louise and
                            Lee, Gun A. and Billinghurst, Mark and Adcock,
                            Matt},<br />
                            title = {Mixed Reality Remote Collaboration
                            Combining 360 Video and 3D Reconstruction},<br />
                            year = {2019},<br />
                            isbn = {9781450359702},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3290605.3300431},<br />
                            doi = {10.1145/3290605.3300431},<br />
                            abstract = {Remote Collaboration using Virtual
                            Reality (VR) and Augmented Reality (AR) has recently
                            become a popular way for people from different
                            places to work together. Local workers can
                            collaborate with remote helpers by sharing
                            360-degree live video or 3D virtual reconstruction
                            of their surroundings. However, each of these
                            techniques has benefits and drawbacks. In this paper
                            we explore mixing 360 video and 3D reconstruction
                            together for remote collaboration, by preserving
                            benefits of both systems while reducing drawbacks of
                            each. We developed a hybrid prototype and conducted
                            user study to compare benefits and problems of using
                            360 or 3D alone to clarify the needs for mixing the
                            two, and also to evaluate the prototype system. We
                            found participants performed significantly better on
                            collaborative search tasks in 360 and felt higher
                            social presence, yet 3D also showed potential to
                            complement. Participant feedback collected after
                            trying our hybrid system provided directions for
                            improvement.},<br />
                            booktitle = {Proceedings of the 2019 CHI Conference
                            on Human Factors in Computing Systems},<br />
                            pages = {1–14},<br />
                            numpages = {14},<br />
                            keywords = {interaction methods, remote
                            collaboration, 3d scene reconstruction, mixed
                            reality, virtual reality, 360 panorama},<br />
                            location = {Glasgow, Scotland Uk},<br />
                            series = {CHI '19}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/10.1145/3290605.3300431"
                              target="_blank"
                              >https://dl.acm.org/doi/10.1145/3290605.3300431</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Remote Collaboration using Virtual Reality (VR) and
                            Augmented Reality (AR) has recently become a popular
                            way for people from different places to work
                            together. Local workers can collaborate with remote
                            helpers by sharing 360-degree live video or 3D
                            virtual reconstruction of their surroundings.
                            However, each of these techniques has benefits and
                            drawbacks. In this paper we explore mixing 360 video
                            and 3D reconstruction together for remote
                            collaboration, by preserving benefits of both
                            systems while reducing drawbacks of each. We
                            developed a hybrid prototype and conducted user
                            study to compare benefits and problems of using 360
                            or 3D alone to clarify the needs for mixing the two,
                            and also to evaluate the prototype system. We found
                            participants performed significantly better on
                            collaborative search tasks in 360 and felt higher
                            social presence, yet 3D also showed potential to
                            complement. Participant feedback collected after
                            trying our hybrid system provided directions for
                            improvement.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="173"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Living-Panorama.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="SharedSphere system overview"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Living-Panorama.png         735w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Living-Panorama-300x173.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Living-Panorama-252x146.png 252w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Living-Panorama-50x29.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Living-Panorama-130x75.png  130w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Mixed reality collaboration through sharing a live
                            panorama
                          </h5>
                          <small
                            >Gun A. Lee , Theophilus Teo , Seungwon Kim , Mark
                            Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            G. A. Lee, T. Teo, S. Kim, and M. Billinghurst.
                            (2017). “Mixed reality collaboration through sharing
                            a live panorama”. In SIGGRAPH Asia 2017 Mobile
                            Graphics & Interactive Applications (SA 2017). ACM,
                            New York, NY, USA, Article 14, 4 pages.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3132787.3139203,<br />
                            author = {Lee, Gun A. and Teo, Theophilus and Kim,
                            Seungwon and Billinghurst, Mark},<br />
                            title = {Mixed Reality Collaboration through Sharing
                            a Live Panorama},<br />
                            year = {2017},<br />
                            isbn = {9781450354103},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url = {https://doi.org/10.1145/3132787.3139203},<br />
                            doi = {10.1145/3132787.3139203},<br />
                            abstract = {One of the popular features on modern
                            social networking platforms is sharing live 360
                            panorama video. This research investigates on how to
                            further improve shared live panorama based
                            collaborative experiences by applying Mixed Reality
                            (MR) technology. Shared-Sphere is a wearable MR
                            remote collaboration system. In addition to sharing
                            a live captured immersive panorama, SharedSphere
                            enriches the collaboration through overlaying MR
                            visualisation of non-verbal communication cues
                            (e.g., view awareness and gestures cues). User
                            feedback collected through a preliminary user study
                            indicated that sharing of live 360 panorama video
                            was beneficial by providing a more immersive
                            experience and supporting view independence. Users
                            also felt that the view awareness cues were helpful
                            for understanding the remote collaborator's
                            focus.},<br />
                            booktitle = {SIGGRAPH Asia 2017 Mobile Graphics \&
                            Interactive Applications},<br />
                            articleno = {14},<br />
                            numpages = {4},<br />
                            keywords = {shared experience, panorama, remote
                            collaboration},<br />
                            location = {Bangkok, Thailand},<br />
                            series = {SA '17}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/10.1145/3132787.3139203"
                              target="_blank"
                              >https://dl.acm.org/doi/10.1145/3132787.3139203</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            One of the popular features on modern social
                            networking platforms is sharing live 360 panorama
                            video. This research investigates on how to further
                            improve shared live panorama based collaborative
                            experiences by applying Mixed Reality (MR)
                            technology. Shared-Sphere is a wearable MR remote
                            collaboration system. In addition to sharing a live
                            captured immersive panorama, SharedSphere enriches
                            the collaboration through overlaying MR
                            visualisation of non-verbal communication cues
                            (e.g., view awareness and gestures cues). User
                            feedback collected through a preliminary user study
                            indicated that sharing of live 360 panorama video
                            was beneficial by providing a more immersive
                            experience and supporting view independence. Users
                            also felt that the view awareness cues were helpful
                            for understanding the remote collaborator's focus.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="77"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="Prototype mixed presence collaborative Mixed Reality System"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence.png         835w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence-300x77.png  300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence-768x198.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence-260x67.png  260w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence-50x13.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Mixed-Presence-150x39.png  150w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Mixed Presence Collaborative Mixed Reality System
                          </h5>
                          <small
                            >Mitchell Norman; Gun Lee; Ross T. Smith; Mark
                            Billinqhurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            M. Norman, G. Lee, R. T. Smith and M. Billinqhurst,
                            "A Mixed Presence Collaborative Mixed Reality
                            System," 2019 IEEE Conference on Virtual Reality and
                            3D User Interfaces (VR), Osaka, Japan, 2019, pp.
                            1106-1107, doi: 10.1109/VR.2019.8797966.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @INPROCEEDINGS{8797966,<br />
                            author={Norman, Mitchell and Lee, Gun and Smith,
                            Ross T. and Billinqhurs, Mark},<br />
                            booktitle={2019 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)}, <br />
                            title={A Mixed Presence Collaborative Mixed Reality
                            System}, <br />
                            year={2019},<br />
                            volume={},<br />
                            number={},<br />
                            pages={1106-1107},<br />
                            doi={10.1109/VR.2019.8797966}}<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/document/8797966/authors#authors"
                              target="_blank"
                              >https://ieeexplore.ieee.org/document/8797966/authors#authors</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Research has shown that Mixed Presence Groupware
                            (MPG) systems are a valuable collaboration tool.
                            However research into MPG systems is limited to a
                            handful of tabletop and Virtual Reality (VR) systems
                            with no exploration of Head-Mounted Display (HMD)
                            based Augmented Reality (AR) solutions. We present a
                            new system with two local users and one remote user
                            using HMD based AR interfaces. Our system provides
                            tools allowing users to layout a room with the help
                            of a remote user. The remote user has access to a
                            marker and pointer tools to assist in directing the
                            local users. Feedback collected from several groups
                            of users showed that our system is easy to learn but
                            could have increased accuracy and consistency.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="249"
                              src="https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim.png"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt="System features (clockwise from top left: 1) gaze reticle, 2) virtual markers, and 3) virtual ray pointer 4) emitting out of a webcam)"
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim.png         822w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim-300x249.png 300w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim-768x636.png 768w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim-176x146.png 176w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim-50x41.png    50w,
                                https://empathiccomputing.org/wp-content/uploads/2023/07/Collaboration-systeim-91x75.png    91w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Mixed Presence Collaborative Mixed Reality System
                          </h5>
                          <small
                            >Mitchell Norman; Gun Lee; Ross T. Smith; Mark
                            Billinqhurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Norman, M., Lee, G., Smith, R. T., & Billinqhurst,
                            M. (2019, March). A mixed presence collaborative
                            mixed reality system. In 2019 IEEE Conference on
                            Virtual Reality and 3D User Interfaces (VR) (pp.
                            1106-1107). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @INPROCEEDINGS{8797966,<br />
                            author={Norman, Mitchell and Lee, Gun and Smith,
                            Ross T. and Billinqhurs, Mark},<br />
                            booktitle={2019 IEEE Conference on Virtual Reality
                            and 3D User Interfaces (VR)}, <br />
                            title={A Mixed Presence Collaborative Mixed Reality
                            System}, <br />
                            year={2019},<br />
                            volume={},<br />
                            number={},<br />
                            pages={1106-1107},<br />
                            doi={10.1109/VR.2019.8797966}}<br />
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/document/8797966"
                              target="_blank"
                              >https://ieeexplore.ieee.org/document/8797966</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Research has shown that Mixed Presence Groupware
                            (MPG) systems are a valuable collaboration tool.
                            However research into MPG systems is limited to a
                            handful of tabletop and Virtual Reality (VR) systems
                            with no exploration of Head-Mounted Display (HMD)
                            based Augmented Reality (AR) solutions. We present a
                            new system with two local users and one remote user
                            using HMD based AR interfaces. Our system provides
                            tools allowing users to layout a room with the help
                            of a remote user. The remote user has access to a
                            marker and pointer tools to assist in directing the
                            local users. Feedback collected from several groups
                            of users showed that our system is easy to learn but
                            could have increased accuracy and consistency.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="291"
                              src="https://empathiccomputing.org/wp-content/uploads/2024/07/LightSense.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2024/07/LightSense.jpg         447w,
                                https://empathiccomputing.org/wp-content/uploads/2024/07/LightSense-300x291.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2024/07/LightSense-150x146.jpg 150w,
                                https://empathiccomputing.org/wp-content/uploads/2024/07/LightSense-50x50.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2024/07/LightSense-77x75.jpg    77w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            LightSense-Long Distance
                          </h5>
                          <small
                            >Uwe Rieger, Yinan Liu, Tharindu Kaluarachchi, Amit
                            Barde, Huidong Bai, Alaeddin Nassani, Suranga
                            Nanayakkara, Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Rieger, U., Liu, Y., Kaluarachchi, T., Barde, A.,
                            Bai, H., Nassani, A., ... & Billinghurst, M. (2023).
                            LightSense-Long Distance. In ACM SIGGRAPH Asia 2023
                            Art Gallery (pp. 1-2).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @incollection{rieger2023lightsense,<br />
                            title={LightSense-Long Distance},<br />
                            author={Rieger, Uwe and Liu, Yinan and Kaluarachchi,
                            Tharindu and Barde, Amit and Bai, Huidong and
                            Nassani, Alaeddin and Nanayakkara, Suranga and
                            Billinghurst, Mark},<br />
                            booktitle={ACM SIGGRAPH Asia 2023 Art Gallery},<br />
                            pages={1--2},<br />
                            year={2023}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3610537.3622963"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3610537.3622963</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            <div role="paragraph">
                              'LightSense - Long Distance' explores remote
                              interaction with architectural space. It is a
                              virtual extension of the project 'LightSense,'
                              which is currently presented at the exhibition
                              'Cyber Physical: Architecture in Real Time' at
                              EPFL Pavilions in Switzerland. Using numerous VR
                              headsets, the setup at the Art Gallery at SIGGRAPH
                              Asia establishes a direct connection between both
                              exhibition sites in Sydney and Lausanne.
                            </div>
                            <div role="paragraph"></div>
                            <div role="paragraph">
                              'LightSense' at EPFL Pavilions is an immersive
                              installation that allows the audience to engage in
                              intimate interaction with a living architectural
                              body. It consists of a 12-meter-long construction
                              that combines a lightweight structure with
                              projected 3D holographic animations. At its core
                              sits a neural network, which has been trained on
                              sixty thousand poems. This allows the structure to
                              engage, lead, and sustain conversations with the
                              visitor. Its responses are truly associative,
                              unpredictable, meaningful, magical, and deeply
                              emotional. Analysing the emotional tenor of the
                              conversation, 'LightSense' can transform into a
                              series of hybrid architectural volumes, immersing
                              the visitors in Pavilions of Love, Anger,
                              Curiosity, and Joy.
                            </div>
                            <div role="paragraph"></div>
                            <div role="paragraph">
                              'LightSense's' physical construction is linked to
                              a digital twin. Movement, holographic animations,
                              sound, and text responses are controlled by the
                              cloud-based AI system. This combination creates a
                              location-independent cyber-physical system. As
                              such, the 'Long Distance' version, which premiered
                              at SIGGRAPH Asia, enables the visitors in Sydney
                              to directly engage with the physical setup in
                              Lausanne. Using VR headsets with a new 360-degree
                              4K live streaming system, the visitors find
                              themselves teleported to face 'LightSense', able
                              to engage in a direct conversation with the
                              structure on-site.
                            </div>
                            <div role="paragraph"></div>
                            <div role="paragraph">
                              'LightSense - Long Distance' leaves behind the
                              notion of architecture being a place-bound and
                              static environment. Instead, it points toward the
                              next generation of responsive buildings that
                              transcend space, are capable of dynamic behaviour,
                              and able to accompany their visitors as creative
                              partners.
                            </div>
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            haptic HONGI: Reflections on Collaboration in the
                            Transdisciplinary Creation of an AR Artwork
                          </h5>
                          <small
                            >Mairi Gunn, Angus Campbell, Mark Billinghurst,
                            Wendy Lawn, Prasanth Sasikumar, and Sachith
                            Muthukumarana.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Gunn, M., Campbell, A., Billinghurst, M., Lawn, W.,
                            Sasikumar, P., & Muthukumarana, S. (2023). haptic
                            HONGI: Reflections on Collaboration in the
                            Transdisciplinary Creation of an AR Artwork. In
                            Creating Digitally: Shifting Boundaries: Arts and
                            Technologies—Contemporary Applications and Concepts
                            (pp. 301-330). Cham: Springer International
                            Publishing.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @incollection{gunn2023haptic,<br />
                            title={haptic HONGI: Reflections on Collaboration in
                            the Transdisciplinary Creation of an AR Artwork},<br />
                            author={Gunn, Mairi and Campbell, Angus and
                            Billinghurst, Mark and Lawn, Wendy and Sasikumar,
                            Prasanth and Muthukumarana, Sachith},<br />
                            booktitle={Creating Digitally: Shifting Boundaries:
                            Arts and Technologies—Contemporary Applications and
                            Concepts},<br />
                            pages={301--330},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/chapter/10.1007/978-3-031-31360-8_11"
                              target="_blank"
                              >https://link.springer.com/chapter/10.1007/978-3-031-31360-8_11</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We developed the Motion-Simulation Platform, a
                            platform running within a game engine that is able
                            to extract both RGB imagery and the corresponding
                            intrinsic motion data (i.e., motion field). This is
                            useful for motion-related computer vision tasks
                            where large amounts of intrinsic motion data are
                            required to train a model. We describe the
                            implementation and design details of the
                            Motion-Simulation Platform. The platform is
                            extendable, such that any scene developed within the
                            game engine is able to take advantage of the motion
                            data extraction tools. We also provide both user and
                            AI-bot controlled navigation, enabling user-driven
                            input and mass automation of motion data collection.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Motion-Simulation Platform to Generate Synthetic
                            Motion Data for Computer Vision Tasks
                          </h5>
                          <small
                            >Andrew Chalmers, Junhong Zhao, Weng Khuan Hoh,
                            James Drown, Simon Finnie, Richard Yao, James Lin,
                            James Wilmott, Arindam Dey, Mark Billinghurst,
                            Taehyun Rhee</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Chalmers, A., Zhao, J., Khuan Hoh, W., Drown, J.,
                            Finnie, S., Yao, R., ... & Rhee, T. (2023). A
                            Motion-Simulation Platform to Generate Synthetic
                            Motion Data for Computer Vision Tasks. In SIGGRAPH
                            Asia 2023 Technical Communications (pp. 1-4).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{10.1145/3610543.3628795,<br />
                            author = {Chalmers, Andrew and Zhao, Junhong and
                            Khuan Hoh, Weng and Drown, James and Finnie, Simon
                            and Yao, Richard and Lin, James and Wilmott, James
                            and Dey, Arindam and Billinghurst, Mark and Rhee,
                            Taehyun},<br />
                            title = {A Motion-Simulation Platform to Generate
                            Synthetic Motion Data for Computer Vision Tasks},<br />
                            year = {2023},<br />
                            isbn = {9798400703140},<br />
                            publisher = {Association for Computing
                            Machinery},<br />
                            address = {New York, NY, USA},<br />
                            url =
                            {https://doi-org.ezproxy.auckland.ac.nz/10.1145/3610543.3628795},<br />
                            doi = {10.1145/3610543.3628795},<br />
                            abstract = {We developed the Motion-Simulation
                            Platform, a platform running within a game engine
                            that is able to extract both RGB imagery and the
                            corresponding intrinsic motion data (i.e., motion
                            field). This is useful for motion-related computer
                            vision tasks where large amounts of intrinsic motion
                            data are required to train a model. We describe the
                            implementation and design details of the
                            Motion-Simulation Platform. The platform is
                            extendable, such that any scene developed within the
                            game engine is able to take advantage of the motion
                            data extraction tools. We also provide both user and
                            AI-bot controlled navigation, enabling user-driven
                            input and mass automation of motion data
                            collection.},<br />
                            booktitle = {SIGGRAPH Asia 2023 Technical
                            Communications},<br />
                            articleno = {21},<br />
                            numpages = {4},<br />
                            keywords = {user study, simulation, motion, machine
                            learning, data generation},<br />
                            location = {Sydney, NSW, Australia},<br />
                            series = {SA '23}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/10.1145/3610543.3628795"
                              target="_blank"
                              >https://dl.acm.org/doi/10.1145/3610543.3628795</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We developed the Motion-Simulation Platform, a
                            platform running within a game engine that is able
                            to extract both RGB imagery and the corresponding
                            intrinsic motion data (i.e., motion field). This is
                            useful for motion-related computer vision tasks
                            where large amounts of intrinsic motion data are
                            required to train a model. We describe the
                            implementation and design details of the
                            Motion-Simulation Platform. The platform is
                            extendable, such that any scene developed within the
                            game engine is able to take advantage of the motion
                            data extraction tools. We also provide both user and
                            AI-bot controlled navigation, enabling user-driven
                            input and mass automation of motion data collection.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers">
                            <img
                              width="300"
                              height="239"
                              src="https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550.jpg"
                              class="attachment-300x300 size-300x300 wp-post-image"
                              alt=""
                              itemprop="image"
                              srcset="
                                https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550.jpg         886w,
                                https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550-300x239.jpg 300w,
                                https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550-768x611.jpg 768w,
                                https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550-183x146.jpg 183w,
                                https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550-50x40.jpg    50w,
                                https://empathiccomputing.org/wp-content/uploads/2024/08/微信截图_20240819141550-94x75.jpg    94w
                              "
                              sizes="(max-width: 300px) 100vw, 300px"
                            />
                          </div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Stylus and Gesture Asymmetric Interaction for Fast
                            and Precise Sketching in Virtual Reality
                          </h5>
                          <small
                            >Qianyuan Zou, Huidong Bai, Lei Gao, Gun A. Lee,
                            Allan Fowler & Mark Billinghurst</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zou, Q., Bai, H., Gao, L., Lee, G. A., Fowler, A., &
                            Billinghurst, M. (2024). Stylus and Gesture
                            Asymmetric Interaction for Fast and Precise
                            Sketching in Virtual Reality. International Journal
                            of Human–Computer Interaction, 1-18.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{zou2024stylus,<br />
                            title={Stylus and Gesture Asymmetric Interaction for
                            Fast and Precise Sketching in Virtual Reality},<br />
                            author={Zou, Qianyuan and Bai, Huidong and Gao, Lei
                            and Lee, Gun A and Fowler, Allan and Billinghurst,
                            Mark},<br />
                            journal={International Journal of Human--Computer
                            Interaction},<br />
                            pages={1--18},<br />
                            year={2024},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2278294#abstract"
                              target="_blank"
                              >https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2278294#abstract</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This research investigates fast and precise Virtual
                            Reality (VR) sketching methods with different
                            tool-based asymmetric interfaces. In traditional
                            real-world drawing, artists commonly employ an
                            asymmetric interaction system where each hand holds
                            different tools, facilitating diverse and nuanced
                            artistic expressions. However, in virtual reality
                            (VR), users are typically limited to using identical
                            tools in both hands for drawing. To bridge this gap,
                            we aim to introduce specifically designed tools in
                            VR that replicate the varied tool configurations
                            found in the real world. Hence, we developed a VR
                            sketching system supporting three hybrid input
                            techniques using a standard VR controller, a VR
                            stylus, or a data glove. We conducted a formal user
                            study consisting of an internal comparative
                            experiment with four conditions and three tasks to
                            compare three asymmetric input methods with each
                            other and with a traditional symmetric
                            controller-based solution based on questionnaires
                            and performance evaluations. The results showed that
                            in contrast to symmetric dual VR controller
                            interfaces, the asymmetric input with gestures
                            significantly reduced task completion times while
                            maintaining good usability and input accuracy with a
                            low task workload. This shows the value of
                            asymmetric input methods for VR sketching. We also
                            found that the overall user experience could be
                            further improved by optimizing the tracking
                            stability of the data glove and the VR stylus.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            ARCOA: using the ar-assisted cooperative assembly
                            system to visualize key information about the
                            occluded partner
                          </h5>
                          <small
                            >Shuo Feng, Weiping He, Qianrui Zhang, Mark
                            Billinghurst, Lingxiao Yang, Shaohua Zhang, and
                            Xiaotian Zhang</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Feng, S., He, W., Zhang, Q., Billinghurst, M., Yang,
                            L., Zhang, S., & Zhang, X. (2023). ARCoA: Using the
                            AR-assisted cooperative assembly system to visualize
                            key information about the occluded partner.
                            International Journal of Human–Computer Interaction,
                            39(18), 3556-3566.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{feng2023arcoa,<br />
                            title={ARCoA: Using the AR-assisted cooperative
                            assembly system to visualize key information about
                            the occluded partner},<br />
                            author={Feng, Shuo and He, Weiping and Zhang,
                            Qianrui and Billinghurst, Mark and Yang, Lingxiao
                            and Zhang, Shaohua and Zhang, Xiaotian},<br />
                            journal={International Journal of Human--Computer
                            Interaction},<br />
                            volume={39},<br />
                            number={18},<br />
                            pages={3556--3566},<br />
                            year={2023},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2022.2099237"
                              target="_blank"
                              >https://www.tandfonline.com/doi/abs/10.1080/10447318.2022.2099237</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            During component assembly, some operations must be
                            completed by two or more workers due to the size or
                            assembly mode. For instance, in manual riveting, two
                            workers are positioned on either side of a steel
                            plate, which blocks the view. Traditional
                            collaborative approaches limit assembly efficiency
                            and is difficult to ensure accurate and rapid
                            interaction between workers. In this study, we
                            developed an AR-Assisted Cooperative Assembly System
                            (ARCoA) to address the issue. ARCoA allows users to
                            view their partner’s key information that is
                            occluded, including tools, gestures, orientations,
                            and shared markers. Besides, we presented a user
                            experiment that compared this method with the
                            traditional approach. The results indicated that the
                            new system could significantly improve assembly
                            efficiency, system availability, and sense of social
                            presence. Moreover, most users we surveyed preferred
                            ARCoA. In the future, we will incorporate more
                            functions and improve the accuracy of the system to
                            solve complex multi-person collaborative problems.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A Distributed Augmented Reality Training
                            Architecture For Distributed Cognitive Intelligent
                            Tutoring Paradigms
                          </h5>
                          <small
                            >Bradley Herbert, Nilufar Baghaei, Mark
                            Billinghurst, and Grant Wigley.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Herbert, B., Baghaei, N., Billinghurst, M., &
                            Wigley, G. (2023). A Distributed Augmented Reality
                            Training Architecture For Distributed Cognitive
                            Intelligent Tutoring Paradigms. Authorea Preprints.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{herbert2023distributed,<br />
                            title={A Distributed Augmented Reality Training
                            Architecture For Distributed Cognitive Intelligent
                            Tutoring Paradigms},<br />
                            author={Herbert, Bradley and Baghaei, Nilufar and
                            Billinghurst, Mark and Wigley, Grant},<br />
                            journal={Authorea Preprints},<br />
                            year={2023},<br />
                            publisher={Authorea}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.13840151.v1"
                              target="_blank"
                              >https://www.techrxiv.org/doi/full/10.36227/techrxiv.13840151.v1</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Modern training typically incorporates real-world
                            training applications. Augmented Reality (AR)
                            technologies support this by overlaying virtual
                            objects in real-world 3-Dimensional (3D) space.
                            However, integrating instruction into AR is
                            challenging because of technological and educational
                            considerations. One reason is a lack of architecture
                            for supporting Intelligent Tutoring Systems (ITSs)
                            in AR training domains. We present a novel modular
                            agent-based Distributed Augmented Reality Training
                            (DART) architecture for ITSs to address two key AR
                            challenges: (1) a decoupling of the display and
                            tracking components and (2) support for modularity.
                            Modular agents communicate with each other over a
                            network, allowing them to be easily swapped out and
                            replaced to support differing needs. Our motivation
                            is driven by the fact that AR technologies are vary
                            considerably and an ITS architecture would need to
                            be flexible enough to support these requirements.
                            Finally, we believe that our novel architecture will
                            appeal to practical designers of ITSs and to the
                            more theoretical educators who wish to use such
                            systems to simulate and broaden research in the
                            distributed cognitive educational theories.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Virtual Reality for Social-Emotional Learning: A
                            Review
                          </h5>
                          <small
                            >Irna Hamzah, Ely Salwana, Mark Billinghurst,
                            Nilufar Baghaei, Mohammad Nazir Ahmad, Fadhilah
                            Rosdi, and Azhar Arsad.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Hamzah, I., Salwana, E., Billinghurst, M., Baghaei,
                            N., Ahmad, M. N., Rosdi, F., & Arsad, A. (2023,
                            October). Virtual Reality for Social-Emotional
                            Learning: A Review. In International Visual
                            Informatics Conference (pp. 119-130). Singapore:
                            Springer Nature Singapore.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{hamzah2023virtual,<br />
                            title={Virtual Reality for Social-Emotional
                            Learning: A Review},<br />
                            author={Hamzah, Irna and Salwana, Ely and
                            Billinghurst, Mark and Baghaei, Nilufar and Ahmad,
                            Mohammad Nazir and Rosdi, Fadhilah and Arsad,
                            Azhar},<br />
                            booktitle={International Visual Informatics
                            Conference},<br />
                            pages={119--130},<br />
                            year={2023},<br />
                            organization={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/chapter/10.1007/978-981-99-7339-2_11"
                              target="_blank"
                              >https://link.springer.com/chapter/10.1007/978-981-99-7339-2_11</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual reality (VR) is an immersive technology that
                            can simulate different environments and experiences.
                            Social-emotional learning (SEL) is a process through
                            which individuals develop the skills, knowledge, and
                            attitudes to understand and manage their emotions,
                            establish positive relationships, and make
                            responsible decisions. SEL promotes healthy
                            emotional regulation in adolescents. However, VR
                            interventions for adolescent emotion regulation have
                            received less attention. The aim of this research is
                            to identify a VR element that includes knowledge in
                            relation to SEL since 2017 through systematic
                            literature reviews (SLRs). A broad review of the
                            current literature was conducted in three databases,
                            namely Scopus, IEEE, and WOS. Data were extracted,
                            including age ranges, year published, and medical
                            procedures, using a search term. The result suggests
                            a requirement list to design a virtual reality for
                            social-emotional learning that promotes a positive
                            impact on emotion regulation for Malaysian
                            adolescents.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The MagicBook Revisited
                          </h5>
                          <small
                            >Geert Lugtenberg, Kazuma Mori, Yuki Matoba,
                            Theophilus Teo, and Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Lugtenberg, G., Mori, K., Matoba, Y., Teo, T., &
                            Billinghurst, M. (2023, October). The MagicBook
                            Revisited. In 2023 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
                            (pp. 801-806). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{lugtenberg2023magicbook,<br />
                            title={The MagicBook Revisited},<br />
                            author={Lugtenberg, Geert and Mori, Kazuma and
                            Matoba, Yuki and Teo, Theophilus and Billinghurst,
                            Mark},<br />
                            booktitle={2023 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={801--806},<br />
                            year={2023},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10322137/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10322137/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Twenty years ago the MagicBook demonstrated an early
                            collaborative cross-reality system allowing users to
                            transition along the reality-virtuality continuum.
                            This current application reproduces the key elements
                            of the MagicBook using mobile phones rather than
                            high end graphics computers and custom handheld
                            displays. Just like the original, people can use a
                            handheld viewer to see virtual content superimposed
                            over real book pages, and then fly into the scenes
                            and experience them immersively. Multi-scale
                            collaboration is also supported enabling people in
                            the AR view to see the users in VR, or to experience
                            the scenes together at the same scale. We also add
                            new features, such as realistic avatars, tangible
                            interaction, use of a handheld controller, and
                            support for remote participants. Overall, we have
                            created a cross reality platform that fits in a
                            person’s pocket, but enables them to collaborate
                            with dozens of people in AR and VR in a very natural
                            way. This is demonstrated in a command and control
                            use case, showing its application in a fire-fighting
                            scenario. We also report on pilot study results from
                            people that have tried the platform.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Utilizing a Robot to Endow Virtual Objects with
                            Stiffness
                          </h5>
                          <small
                            >Jiepeng Dong, Weiping He, Bokai Zheng, Yizhe Liu,
                            and Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Dong, J., He, W., Zheng, B., Liu, Y., &
                            Billinghurst, M. (2023, October). Utilizing a Robot
                            to Endow Virtual Objects with Stiffness. In 2023
                            IEEE International Symposium on Mixed and Augmented
                            Reality Adjunct (ISMAR-Adjunct) (pp. 496-500). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{dong2023utilizing,<br />
                            title={Utilizing a Robot to Endow Virtual Objects
                            with Stiffness},<br />
                            author={Dong, Jiepeng and He, Weiping and Zheng,
                            Bokai and Liu, Yizhe and Billinghurst, Mark},<br />
                            booktitle={2023 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={496--500},<br />
                            year={2023},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10322189/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10322189/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper proposes a novel approach to provide
                            stiffness feedback in VR utilizing the inherent
                            characteristics of encounter-type haptic devices
                            (ETHDs). The method aims to provide a sense of
                            object’s deformation and enhance users’ perception
                            of stiffness when touching objects. We explored how
                            to minimize the influence of hardness on stiffness
                            perception and selected materials for the soft,
                            medium, and hard hardness mapping groups to adapt to
                            users’ perceptions when touching objects of
                            different hardness levels. Using this system, we
                            compared and evaluated the method with a touch
                            interaction without stiffness perceptions and found
                            a better performance in terms of realism,
                            possibility to act, attractiveness, efficiency.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            An Asynchronous Hybrid Cross Reality Collaborative
                            System
                          </h5>
                          <small
                            >Hyunwoo Cho, Bowen Yuan, Jonathon Derek Hart,
                            Eunhee Chang, Zhuang Chang, Jiashuo Cao, Gun A. Lee,
                            Thammathip Piumsomboon, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Cho, H., Yuan, B., Hart, J. D., Chang, E., Chang,
                            Z., Cao, J., ... & Billinghurst, M. (2023, October).
                            An asynchronous hybrid cross reality collaborative
                            system. In 2023 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
                            (pp. 70-73). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{cho2023asynchronous,<br />
                            title={An asynchronous hybrid cross reality
                            collaborative system},<br />
                            author={Cho, Hyunwoo and Yuan, Bowen and Hart,
                            Jonathon Derek and Chang, Eunhee and Chang, Zhuang
                            and Cao, Jiashuo and Lee, Gun A and Piumsomboon,
                            Thammathip and Billinghurst, Mark},<br />
                            booktitle={2023 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={70--73},<br />
                            year={2023},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://cross-realities.org/2024/proceedings/JWCR23_paper_15.pdf"
                              target="_blank"
                              >https://cross-realities.org/2024/proceedings/JWCR23_paper_15.pdf</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This work presents a Mixed Reality (MR)-based
                            asynchronous hybrid cross reality collaborative
                            system which supports recording and playback of user
                            actions in three-dimensional task space at different
                            periods in time. Using this system, an expert user
                            can record a task process such as virtual object
                            placement or assembly, which can then be viewed by
                            other users in either Augmented Reality (AR) or
                            Virtual Reality (VR) views at later points in time
                            to complete the task. In VR, the pre-scanned 3D
                            workspace can be experienced to enhance the
                            understanding of spatial information. Alternatively,
                            AR can provide real-scale information to help the
                            workers manipulate real world objects, and complete
                            the task assignment. Users can also seamlessly move
                            between AR and VR views as desired. In this way the
                            system can contribute to improving task performance
                            and co-presence during asynchronous collaboration.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Time Travellers: An Asynchronous Cross Reality
                            Collaborative System
                          </h5>
                          <small
                            >Hyunwoo Cho, Bowen Yuan, Jonathon Derek Hart,
                            Zhuang Chang, Jiashuo Cao, Eunhee Chang, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Cho, H., Yuan, B., Hart, J. D., Chang, Z., Cao, J.,
                            Chang, E., & Billinghurst, M. (2023, October). Time
                            Travellers: An Asynchronous Cross Reality
                            Collaborative System. In 2023 IEEE International
                            Symposium on Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct) (pp. 848-853). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{cho2023time,<br />
                            title={Time Travellers: An Asynchronous Cross
                            Reality Collaborative System},<br />
                            author={Cho, Hyunwoo and Yuan, Bowen and Hart,
                            Jonathon Derek and Chang, Zhuang and Cao, Jiashuo
                            and Chang, Eunhee and Billinghurst, Mark},<br />
                            booktitle={2023 IEEE International Symposium on
                            Mixed and Augmented Reality Adjunct
                            (ISMAR-Adjunct)},<br />
                            pages={848--853},<br />
                            year={2023},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10322151/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10322151/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This work presents a Mixed Reality (MR)-based
                            asynchronous hybrid cross reality collaborative
                            system which supports recording and playback of user
                            actions in a large task space at different periods
                            in time. Using this system, an expert can record a
                            task process such as virtual object placement or
                            assembly, which can then be viewed by other users in
                            either Augmented Reality (AR) or Virtual Reality
                            (VR) at later points in time to complete the task.
                            In VR, the pre-scanned 3D workspace can be
                            experienced to enhance the understanding of spatial
                            information. Alternatively, AR can provide
                            real-scale information to help the workers
                            manipulate real-world objects, and complete the
                            assignment. Users can seamlessly switch between AR
                            and VR views as desired. In this way, the system can
                            contribute to improving task performance and
                            co-presence during asynchronous collaboration. The
                            system is demonstrated in a use-case scenario of
                            object assembly using parts that must be retrieved
                            from a storehouse location. A pilot user study found
                            that cross reality asynchronous collaboration system
                            was helpful in providing information about work
                            environments, inducing faster task completion with a
                            lower task load. We provide lessons learned and
                            suggestions for future research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Deep Learning-based Simulator Sickness Estimation
                            from 3D Motion
                          </h5>
                          <small
                            >Junhong Zhao, Kien TP Tran, Andrew Chalmers, Weng
                            Khuan Hoh, Richard Yao, Arindam Dey, James Wilmott,
                            James Lin, Mark Billinghurst, Robert W. Lindeman,
                            Taehyun Rhee</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zhao, J., Tran, K. T., Chalmers, A., Hoh, W. K.,
                            Yao, R., Dey, A., ... & Rhee, T. (2023, October).
                            Deep learning-based simulator sickness estimation
                            from 3D motion. In 2023 IEEE International Symposium
                            on Mixed and Augmented Reality (ISMAR) (pp. 39-48).
                            IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{zhao2023deep,<br />
                            title={Deep learning-based simulator sickness
                            estimation from 3D motion},<br />
                            author={Zhao, Junhong and Tran, Kien TP and
                            Chalmers, Andrew and Hoh, Weng Khuan and Yao,
                            Richard and Dey, Arindam and Wilmott, James and Lin,
                            James and Billinghurst, Mark and Lindeman, Robert W
                            and others},<br />
                            booktitle={2023 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR)},<br />
                            pages={39--48},<br />
                            year={2023},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10316443/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10316443/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper presents a novel solution for estimating
                            simulator sickness in HMDs using machine learning
                            and 3D motion data, informed by user-labeled
                            simulator sickness data and user analysis. We
                            conducted a novel VR user study, which decomposed
                            motion data and used an instant dial-based sickness
                            scoring mechanism. We were able to emulate typical
                            VR usage and collect user simulator sickness scores.
                            Our user analysis shows that translation and
                            rotation differently impact user simulator sickness
                            in HMDs. In addition, users’ demographic information
                            and self-assessed simulator sickness susceptibility
                            data are collected and show some indication of
                            potential simulator sickness. Guided by the findings
                            from the user study, we developed a novel deep
                            learning-based solution to better estimate simulator
                            sickness with decomposed 3D motion features and user
                            profile information. The model was trained and
                            tested using the 3D motion dataset with user-labeled
                            simulator sickness and profiles collected from the
                            user study. The results show higher estimation
                            accuracy when using the 3D motion data compared with
                            methods based on optical flow extracted from the
                            recorded video, as well as improved accuracy when
                            decomposing the motion data and incorporating user
                            profile information.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Cognitive Load Measurement with Physiological
                            Sensors in Virtual Reality during Physical Activity
                          </h5>
                          <small
                            >Mohammad Ahmadi, Samantha W. Michalka, Sabrina
                            Lenzoni, Marzieh Ahmadi Najafabadi, Huidong Bai,
                            Alexander Sumich, Burkhard Wuensche, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Ahmadi, M., Michalka, S. W., Lenzoni, S., Ahmadi
                            Najafabadi, M., Bai, H., Sumich, A., ... &
                            Billinghurst, M. (2023, October). Cognitive Load
                            Measurement with Physiological Sensors in Virtual
                            Reality during Physical Activity. In Proceedings of
                            the 29th ACM Symposium on Virtual Reality Software
                            and Technology (pp. 1-11).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{ahmadi2023cognitive,<br />
                            title={Cognitive Load Measurement with Physiological
                            Sensors in Virtual Reality during Physical
                            Activity},<br />
                            author={Ahmadi, Mohammad and Michalka, Samantha W
                            and Lenzoni, Sabrina and Ahmadi Najafabadi, Marzieh
                            and Bai, Huidong and Sumich, Alexander and Wuensche,
                            Burkhard and Billinghurst, Mark},<br />
                            booktitle={Proceedings of the 29th ACM Symposium on
                            Virtual Reality Software and Technology},<br />
                            pages={1--11},<br />
                            year={2023}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3611659.3615704"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3611659.3615704</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Many Virtual Reality (VR) experiences, such as
                            learning tools, would benefit from utilising mental
                            states such as cognitive load. Increases in
                            cognitive load (CL) are often reflected in the
                            alteration of physiological responses, such as pupil
                            dilation (PD), electrodermal cctivity (EDA), heart
                            rate (HR), and electroencephalography (EEG).
                            However, the relationship between these
                            physiological responses and cognitive load are
                            usually measured while participants sit in front of
                            a computer screen, whereas VR environments often
                            require a high degree of physical movement. This
                            physical activity can affect the measured signals,
                            making it unclear how suitable these measures are
                            for use in interactive Virtual Reality (VR). We
                            investigate the suitability of four physiological
                            measures as correlates of cognitive load in
                            interactive VR. Suitable measures must be robust
                            enough to allow the learner to move within VR and be
                            temporally responsive enough to be a useful metric
                            for adaptation. We recorded PD, EDA, HR, and EEG
                            data from nineteen participants during a sequence
                            memory task at varying levels of cognitive load
                            using VR, while in the standing position and using
                            their dominant arm to play a game. We observed
                            significant linear relationships between cognitive
                            load and PD, EDA, and EEG frequency band power, but
                            not HR. PD showed the most reliable relationship but
                            has a slower response rate than EEG. Our results
                            suggest the potential for use of PD, EDA, and EEG in
                            this type of interactive VR environment, but
                            additional studies will be needed to assess
                            feasibility under conditions of greater movement.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Exploring Real-time Precision Feedback for
                            AR-assisted Manual Adjustment in Mechanical Assembly
                          </h5>
                          <small
                            >Xingyue Tang, Zhuang Chang, Weiping He, Mark
                            Billinghurst, and Xiaotian Zhang.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Tang, X., Chang, Z., He, W., Billinghurst, M., &
                            Zhang, X. (2023, October). Exploring Real-time
                            Precision Feedback for AR-assisted Manual Adjustment
                            in Mechanical Assembly. In Proceedings of the 29th
                            ACM Symposium on Virtual Reality Software and
                            Technology (pp. 1-11).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{tang2023exploring,<br />
                            title={Exploring Real-time Precision Feedback for
                            AR-assisted Manual Adjustment in Mechanical
                            Assembly},<br />
                            author={Tang, Xingyue and Chang, Zhuang and He,
                            Weiping and Billinghurst, Mark and Zhang,
                            Xiaotian},<br />
                            booktitle={Proceedings of the 29th ACM Symposium on
                            Virtual Reality Software and Technology},<br />
                            pages={1--11},<br />
                            year={2023}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3611659.3615712"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3611659.3615712</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented Reality (AR) based manual assembly
                            nowadays enables to guide the process of physical
                            tasks, providing intuitive instructions and detailed
                            information in real-time. However, very limited
                            studies have explored AR manual adjustment tasks
                            with precision requirements. In this paper, we
                            develop an AR-assisted guidance system for manual
                            adjustments with relatively high-precision
                            requirements. We first assessed the accuracy of the
                            special-set OptiTrack system to determine the
                            threshold of precision requirements for our user
                            study. We further evaluated the performance of
                            Number-based and Bar-based precision feedback by
                            comparing orienting assembly errors and task
                            completion time, as well as the usability in the
                            user study. We found that the assembly errors of
                            orientation in the Number-based and Bar-based
                            interfaces were significantly lower than the
                            baseline condition, while there was no significant
                            difference between the Number-based and Bar-based
                            interfaces. Furthermore, the Number-based showed
                            faster task completion time, lower workload, and
                            higher usability than the Bar-based condition.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Wearable Augmented Reality: Research Trends and
                            Future Directions from Three Major Venues
                          </h5>
                          <small
                            >Tram Thi Minh Tran, Shane Brown, Oliver Weidlich,
                            Mark Billinghurst, and Callum Parker.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Tran, T. T. M., Brown, S., Weidlich, O.,
                            Billinghurst, M., & Parker, C. (2023). Wearable
                            augmented reality: research trends and future
                            directions from three major venues. IEEE
                            Transactions on Visualization and Computer Graphics.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{tran2023wearable,<br />
                            title={Wearable augmented reality: research trends
                            and future directions from three major venues},<br />
                            author={Tran, Tram Thi Minh and Brown, Shane and
                            Weidlich, Oliver and Billinghurst, Mark and Parker,
                            Callum},<br />
                            journal={IEEE Transactions on Visualization and
                            Computer Graphics},<br />
                            year={2023},<br />
                            publisher={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10269051/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10269051/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Wearable Augmented Reality (AR) has attracted
                            considerable attention in recent years, as evidenced
                            by the growing number of research publications and
                            industry investments. With swift advancements and a
                            multitude of interdisciplinary research areas within
                            wearable AR, a comprehensive review is crucial for
                            integrating the current state of the field. In this
                            paper, we present a review of 389 research papers on
                            wearable AR, published between 2018 and 2022 in
                            three major venues: ISMAR, TVCG, and CHI. Drawing
                            inspiration from previous works by Zhou et al. and
                            Kim et al., which summarized AR research at ISMAR
                            over the past two decades (1998–2017), we categorize
                            the papers into different topics and identify
                            prevailing trends. One notable finding is that
                            wearable AR research is increasingly geared towards
                            enabling broader consumer adoption. From our
                            analysis, we highlight key observations related to
                            potential future research areas essential for
                            capitalizing on this trend and achieving widespread
                            adoption. These include addressing challenges in
                            Display, Tracking, Interaction, and Applications,
                            and exploring emerging frontiers in Ethics,
                            Accessibility, Avatar and Embodiment, and
                            Intelligent Virtual Agents.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Evaluating visual encoding quality of a mixed
                            reality user interface for human–machine co-assembly
                            in complex operational terrain
                          </h5>
                          <small
                            >Zhuo Wang, Xiangyu Zhang, Liang Li, Yiliang Zhou,
                            Zexin Lu, Yuwei Dai, Chaoqian Liu, Zekun Su,
                            Xiaoliang Bai, and Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, Z., Zhang, X., Li, L., Zhou, Y., Lu, Z., Dai,
                            Y., ... & Billinghurst, M. (2023). Evaluating visual
                            encoding quality of a mixed reality user interface
                            for human–machine co-assembly in complex operational
                            terrain. Advanced Engineering Informatics, 58,
                            102171.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{wang2023evaluating,<br />
                            title={Evaluating visual encoding quality of a mixed
                            reality user interface for human--machine
                            co-assembly in complex operational terrain},<br />
                            author={Wang, Zhuo and Zhang, Xiangyu and Li, Liang
                            and Zhou, Yiliang and Lu, Zexin and Dai, Yuwei and
                            Liu, Chaoqian and Su, Zekun and Bai, Xiaoliang and
                            Billinghurst, Mark},<br />
                            journal={Advanced Engineering Informatics},<br />
                            volume={58},<br />
                            pages={102171},<br />
                            year={2023},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S1474034623002999"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S1474034623002999</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            During human–machine collaboration in manufacturing
                            activities, it is important to provide real-time
                            annotations in the three-dimensional workspace for
                            local workers who may lack relevant experience and
                            knowledge. For example, in MR assembly, workers need
                            to be alerted to avoid entering hazardous areas when
                            manually replacing components. Recently, many
                            researchers have explored various visual cues for
                            expressing physical task progress information in the
                            MR interface of intelligent systems. However, the
                            relationship between the implantation of visual cues
                            and the balance of interface cognition has not been
                            well revealed, especially in tasks that require
                            annotating hazardous areas in complex operational
                            terrains. In this study, we developed a novel MR
                            interface for an intelligent assembly system that
                            supports local scene sharing based on dynamic 3D
                            reconstruction, remote expert behavior intention
                            recognition based on deep learning, and local
                            personnel operational behavior visual feedback based
                            on external bounding box. We compared the encoding
                            results of the proposed MR interface with 3D
                            annotations combined with 3D sketch cues (3DS),
                            which combines 3D spatial cues (3DSC) and 3DS
                            combined with adaptive cues (AVC), through a case
                            study. We found that for physical tasks that require
                            specific area annotations, 3D annotations with
                            context (3DAC) can better improve the quality of
                            manual work and regulate the cognitive load
                            distribution of the MR interface more reasonably.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Older adults’ experiences of social isolation and
                            loneliness: Can virtual touring increase social
                            connectedness? A pilot study
                          </h5>
                          <small
                            >Michelle Leanne Oppert, Melissa Ngo, Gun A. Lee,
                            Mark Billinghurst, Siobhan Banks, and Laura
                            Tolson.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Oppert, M. L., Ngo, M., Lee, G. A., Billinghurst,
                            M., Banks, S., & Tolson, L. (2023). Older adults’
                            experiences of social isolation and loneliness: Can
                            virtual touring increase social connectedness? A
                            pilot study. Geriatric Nursing, 53, 270-279.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{oppert2023older,<br />
                            title={Older adults’ experiences of social isolation
                            and loneliness: Can virtual touring increase social
                            connectedness? A pilot study},<br />
                            author={Oppert, Michelle Leanne and Ngo, Melissa and
                            Lee, Gun A and Billinghurst, Mark and Banks, Siobhan
                            and Tolson, Laura},<br />
                            journal={Geriatric Nursing},<br />
                            volume={53},<br />
                            pages={270--279},<br />
                            year={2023},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S019745722300191X"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S019745722300191X</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The present pilot study explored the research aim of
                            understanding how independent-living older adults
                            experience social isolation and loneliness and
                            whether virtual tour digital technology can increase
                            social connectedness (N = 10). Through triangulation
                            of interviews, experiences, and feedback, this study
                            contributes to the knowledge base on the well-being
                            of our ageing populations and how digital
                            technologies, specifically virtual tourism, can aid
                            in this process. The key findings reveal that the
                            participants in our study were moderately lonely but
                            were open to embracing more digital technology,
                            sharing how it is instrumental in facilitating
                            social connection and life administration.
                            Participating in virtual tour experiences was well
                            accepted as participants expressed enjoyment,
                            nostalgia, and interest in future use. However, its
                            contribution to increasing social connections needs
                            to be clarified and requires further investigation.
                            Several future research and education directions are
                            provided.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A comprehensive survey on AR-enabled local
                            collaboration
                          </h5>
                          <small
                            >Shuo Feng, Weiping He, Xiaotian Zhang, Mark
                            Billinghurst, and Shuxia Wang.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Feng, S., He, W., Zhang, X., Billinghurst, M., &
                            Wang, S. (2023). A comprehensive survey on
                            AR-enabled local collaboration. Virtual Reality,
                            27(4), 2941-2966.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{feng2023comprehensive,<br />
                            title={A comprehensive survey on AR-enabled local
                            collaboration},<br />
                            author={Feng, Shuo and He, Weiping and Zhang,
                            Xiaotian and Billinghurst, Mark and Wang,
                            Shuxia},<br />
                            journal={Virtual Reality},<br />
                            volume={27},<br />
                            number={4},<br />
                            pages={2941--2966},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-023-00848-2"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-023-00848-2</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            With the rapid development of augmented reality (AR)
                            technology and devices, it is widely used in
                            education, design, industry, game, medicine and
                            other fields. It brings new development
                            opportunities for computer-supported cooperative
                            work. In recent years, there has been an increasing
                            number of studies on AR collaboration. Many
                            professional researchers have also summarized and
                            commented on these local and remote applications.
                            However, to the best of our knowledge, there is no
                            comprehensive review specifically on AR-enabled
                            local collaboration (AR-LoCol). Therefore, this
                            paper presents a comprehensive survey of research
                            between 2012 and 2022 in this domain. We surveyed
                            133 papers on AR-LoCol in Web of Science, 75% of
                            which were published between 2018 and 2022. Next, we
                            provide an in-depth review of papers in seven areas,
                            including time (synchronous and asynchronous),
                            device (hand-held display, desktop, spatial AR,
                            head-mounted display), participants (double and
                            multiple), place (standing, indoor and outdoor),
                            content (virtual objects, annotations, awareness
                            cues and multi-perspective views), and area
                            (education, industry, medicine, architecture,
                            exhibition, game, exterior design, visualization,
                            interaction, basic tools). We discuss the
                            characteristics and specific work in each category,
                            especially the advantages and disadvantages of
                            different devices and the necessity for shared
                            contents. Following this, we summarize the current
                            state of development of AR-LoCol and discuss
                            possible future research directions. This work will
                            be useful for current and future researchers
                            interested in AR-LoCol systems.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Investigation of learners’ behavioral intentions to
                            use metaverse learning environment in higher
                            education: a virtual computer laboratory
                          </h5>
                          <small
                            >Emin İbili, Melek Ölmez, Abdullah Cihan, Fırat
                            Bilal, Aysel Burcu İbili, Nurullah Okumus, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            İbili, E., Ölmez, M., Cihan, A., Bilal, F., İbili,
                            A. B., Okumus, N., & Billinghurst, M. (2023).
                            Investigation of learners’ behavioral intentions to
                            use metaverse learning environment in higher
                            education: a virtual computer laboratory.
                            Interactive Learning Environments, 1-26.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{ibili2023investigation,<br />
                            title={Investigation of learners’ behavioral
                            intentions to use metaverse learning environment in
                            higher education: a virtual computer laboratory},<br />
                            author={{\.I}bili, Emin and {\"O}lmez, Melek and
                            Cihan, Abdullah and Bilal, F{\i}rat and {\.I}bili,
                            Aysel Burcu and Okumus, Nurullah and Billinghurst,
                            Mark},<br />
                            journal={Interactive Learning Environments},<br />
                            pages={1--26},<br />
                            year={2023},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/abs/10.1080/10494820.2023.2240860"
                              target="_blank"
                              >https://www.tandfonline.com/doi/abs/10.1080/10494820.2023.2240860</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The aim of this study is to investigate the
                            determinants that affect undergraduate students’
                            behavioral intentions to continue learning computer
                            hardware concepts utilizing a Metaverse-based
                            system. The current study examined the factors
                            influencing students’ adoption of Metaverse
                            technology at the tertiary level using a model based
                            on the Technology Acceptance Model (TAM) and the
                            General Extended Technology Acceptance Model for
                            E-Learning (GETAMEL). The data was collected from
                            210 undergraduate students and Structural Equation
                            Modeling (SEM) was adopted to analyze the responses.
                            The findings show that Perceived Usefulness and
                            Hedonic Motivation have significant positive effect
                            on Behavioral Intention. Additionally, Natural
                            Interaction and Perceived Usefulness significantly
                            affect Hedonic Motivation, while Computer Anxiety
                            negatively affects Hedonic Motivation. Furthermore,
                            Natural Interaction was found to be the strongest
                            predictor of Perceived Usefulness, whereas
                            Experience was the strongest predictor of Perceived
                            Ease of Use. The findings also indicate that
                            Subjective Norms and Self-Efficacy have a
                            significant effect on Experience, while Subjective
                            Norms significantly influence Self-Efficacy. The
                            research results also showed that neither gender nor
                            the department had any effect. The results of this
                            study provide major practical outcomes for higher
                            education institutions and teachers in terms of
                            designing Metaverse-based teaching environments.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Immersive medical virtual reality: still a novelty
                            or already a necessity?
                          </h5>
                          <small
                            >Tobias Loetscher, A. M. Barrett, Mark Billinghurst,
                            and Belinda Lange.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Loetscher, T., Barrett, A. M., Billinghurst, M., &
                            Lange, B. (2023). Immersive medical virtual reality:
                            still a novelty or already a necessity?. Journal of
                            Neurology, Neurosurgery & Psychiatry, 94(7),
                            499-501.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @misc{loetscher2023immersive,<br />
                            title={Immersive medical virtual reality: still a
                            novelty or already a necessity?},<br />
                            author={Loetscher, Tobias and Barrett, AM and
                            Billinghurst, Mark and Lange, Belinda},<br />
                            journal={Journal of Neurology, Neurosurgery \&
                            Psychiatry},<br />
                            volume={94},<br />
                            number={7},<br />
                            pages={499--501},<br />
                            year={2023},<br />
                            publisher={BMJ Publishing Group Ltd}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://jnnp.bmj.com/content/94/7/499.abstract"
                              target="_blank"
                              >https://jnnp.bmj.com/content/94/7/499.abstract</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual reality (VR) technologies have been explored
                            for medical applications for over half a century.
                            With major tech companies such as Meta (formerly
                            Facebook), HTC and Microsoft investing heavily in
                            the development of VR technologies, significant
                            advancements have recently been made in hardware
                            (eg, standalone headsets), ease of use (eg, gesture
                            tracking) and equipment cost. These advancements
                            helped spur research in the medical field, with over
                            2700 VR-related articles indexed in PubMed alone in
                            2022, and the number of VR articles more than
                            tripling in the last 6 years. Recently, the US Food
                            and Drug Administration (FDA) also approved the
                            first VR-based therapy for chronic back pain. 1
                            Whether the technology has reached a tipping point
                            for its use in medicine is debatable, but it seems
                            timely to provide a brief overview of the current
                            state of immersive VR in neurology and related
                            fields. In this editorial, we will discuss the
                            characteristics of VR that make it a potentially
                            transformative tool in healthcare, review some of
                            the most mature VR solutions for medical use and
                            highlight barriers to implementation that must be
                            addressed before the technology can be widely
                            adopted in healthcare. This editorial will focus
                            solely on immersive VR technology and will not delve
                            into the applications and use cases of augmented or
                            mixed reality.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Can you hear it? Stereo sound-assisted guidance in
                            augmented reality assembly
                          </h5>
                          <small
                            >Shuo Feng, Xinjing He, Weiping He, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Feng, S., He, X., He, W., & Billinghurst, M. (2023).
                            Can you hear it? Stereo sound-assisted guidance in
                            augmented reality assembly. Virtual Reality, 27(2),
                            591-601.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{feng2023can,<br />
                            title={Can you hear it? Stereo sound-assisted
                            guidance in augmented reality assembly},<br />
                            author={Feng, Shuo and He, Xinjing and He, Weiping
                            and Billinghurst, Mark},<br />
                            journal={Virtual Reality},<br />
                            volume={27},<br />
                            number={2},<br />
                            pages={591--601},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-022-00680-0"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-022-00680-0</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Most augmented reality (AR) assembly guidance
                            systems only utilize visual information. Regarding
                            the sound, the human binaural effect helps users
                            quickly identify the general direction of sound
                            sources. At the same time, pleasant sounds can give
                            people a sense of pleasure and relaxation. However,
                            the effect on workers is still unknown when stereo
                            sound and visual information are used together for
                            assembly guidance. To assess the combination of
                            sound and vision in AR assembly guidance, we
                            constructed a stereo sound-assisted guidance system
                            (SAG) based on AR. In our SAG system, we used the
                            tone of a soft instrument called the Chinese lute as
                            the sound source. To determine if SAG has an impact
                            on assembly efficiency and user experience, we
                            conducted a usability test to compare SAG with
                            visual information alone. Results showed that the
                            SAG system significantly improves the efficiency of
                            assembly guidance. Moreover, simultaneous visual and
                            auditory information processing does not increase
                            user workload or learning difficulty. Additionally,
                            in a noisy environment, pleasant sounds help to
                            reduce mental strain.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            A hybrid 2D–3D tangible interface combining a
                            smartphone and controller for virtual reality
                          </h5>
                          <small
                            >Li Zhang, Weiping He, Huidong Bai, Qianyuan Zou,
                            Shuxia Wang, and Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zhang, L., He, W., Bai, H., Zou, Q., Wang, S., &
                            Billinghurst, M. (2023). A hybrid 2D–3D tangible
                            interface combining a smartphone and controller for
                            virtual reality. Virtual Reality, 27(2), 1273-1291.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{zhang2023hybrid,<br />
                            title={A hybrid 2D--3D tangible interface combining
                            a smartphone and controller for virtual reality},<br />
                            author={Zhang, Li and He, Weiping and Bai, Huidong
                            and Zou, Qianyuan and Wang, Shuxia and Billinghurst,
                            Mark},<br />
                            journal={Virtual Reality},<br />
                            volume={27},<br />
                            number={2},<br />
                            pages={1273--1291},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-022-00735-2"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-022-00735-2</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual reality (VR) controllers are widely used for
                            3D virtual object selection and manipulation in
                            immersive virtual worlds, while touchscreen-based
                            devices like smartphones or tablets provide precise
                            2D tangible input. However, VR controllers and
                            touchscreens are used separately in most cases. This
                            research physically integrates a VR controller and a
                            smartphone to create a hybrid 2D–3D tangible
                            interface for VR interactions, combining the
                            strength of both devices. The hybrid interface
                            inherits physical buttons, 3D tracking, and spatial
                            input from the VR controller while having tangible
                            feedback, 2D precise input, and content display from
                            the smartphone’s touchscreen. We review the
                            capabilities of VR controllers and smartphones to
                            summarize design principles and then present a
                            design space with nine typical interaction paradigms
                            for the hybrid interface. We developed an
                            interactive prototype and three application modes to
                            demonstrate the combination of individual
                            interaction paradigms in various VR scenarios. We
                            conducted a formal user study through a guided
                            walkthrough to evaluate the usability of the hybrid
                            interface. The results were positive, with
                            participants reporting above-average usability and
                            rating the system as excellent on four out of six
                            user experience questionnaire scales. We also
                            described two use cases to demonstrate the potential
                            of the hybrid interface.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Parallel or Cross? Effects of Two Collaborative
                            Modes on Augmented Reality Co-located Operations
                          </h5>
                          <small
                            >Shuo Feng, Yizhe Liu, Qianrui Zhang, Weiping He,
                            Xiaotian Zhang, Shuxia Wang, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Feng, S., Liu, Y., Zhang, Q., He, W., Zhang, X.,
                            Wang, S., & Billinghurst, M. (2023). Parallel or
                            cross? Effects of two collaborative modes on
                            augmented reality co-located operations.
                            International Journal of Human–Computer Interaction,
                            1-12.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{feng2023parallel,<br />
                            title={Parallel or cross? Effects of two
                            collaborative modes on augmented reality co-located
                            operations},<br />
                            author={Feng, Shuo and Liu, Yizhe and Zhang, Qianrui
                            and He, Weiping and Zhang, Xiaotian and Wang, Shuxia
                            and Billinghurst, Mark},<br />
                            journal={International Journal of Human--Computer
                            Interaction},<br />
                            pages={1--12},<br />
                            year={2023},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2202574"
                              target="_blank"
                              >https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2202574</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Augmented reality (AR) can bring a new interactive
                            experience to the collaboration between users. When
                            users are in the same place, there are two modes of
                            joint operation for the same object: parallel-work
                            (PW) and cross-work (CW). PW means two users perform
                            their tasks, while CW means assisting each other. To
                            investigate the difference that collaboration using
                            PW and CW in an AR environment brings to users, we
                            developed a two-person local collaboration system,
                            LoCol. We designed and conducted user experiments by
                            selecting the tasks of adjusting the virtual model
                            of the assembly and adding missing boundaries in the
                            model. The results showed that CW led to a higher
                            sense of social coexistence while reducing workload.
                            In terms of task completion time and accuracy, CW
                            and PW each had advantages. We found that users
                            generally want to reduce unnecessary repetitive
                            operations and frequent movement by working with
                            others. This is likely an important criterion for
                            determining who is better suited for a particular
                            job in either approach.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Investigating the relationship between
                            three-dimensional perception and presence in virtual
                            reality-reconstructed architecture
                          </h5>
                          <small
                            >Daniel Paes, Javier Irizarry, Mark Billinghurst,
                            and Diego Pujoni</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Paes, D., Irizarry, J., Billinghurst, M., & Pujoni,
                            D. (2023). Investigating the relationship between
                            three-dimensional perception and presence in virtual
                            reality-reconstructed architecture. Applied
                            Ergonomics, 109, 103953.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{paes2023investigating,<br />
                            title={Investigating the relationship between
                            three-dimensional perception and presence in virtual
                            reality-reconstructed architecture},<br />
                            author={Paes, Daniel and Irizarry, Javier and
                            Billinghurst, Mark and Pujoni, Diego},<br />
                            journal={Applied Ergonomics},<br />
                            volume={109},<br />
                            pages={103953},<br />
                            year={2023},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S0003687022002769"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S0003687022002769</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Identifying and characterizing the factors that
                            affect presence in virtual environments has been
                            acknowledged as a critical step to improving Virtual
                            Reality (VR) applications in the built environment
                            domain. In the search to identify those factors, the
                            research objective was to test whether
                            three-dimensional perception affects presence in
                            virtual environments. A controlled within-group
                            experiment utilizing perception and presence
                            questionnaires was conducted, followed by data
                            analysis, to test the hypothesized unidirectional
                            association between three-dimensional perception and
                            presence in two different virtual environments
                            (non-immersive and immersive). Results indicate no
                            association in either of the systems studied,
                            contrary to the assumption of many scholars in the
                            field but in line with recent studies on the topic.
                            Consequently, VR applications in architectural
                            design may not necessarily need to incorporate
                            advanced stereoscopic visualization techniques to
                            deliver highly immersive experiences, which may be
                            achieved by addressing factors other than depth
                            realism. As findings suggest that the levels of
                            presence experienced by users are not subject to the
                            display mode of a 3D model (whether immersive or
                            non-immersive display), it may still be possible for
                            professionals involved in the review of 3D models
                            (e.g., designers, contractors, clients) to
                            experience high levels of presence through
                            non-stereoscopic VR systems provided that other
                            presence-promoting factors are included.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Towards an Inclusive and Accessible Metaverse
                          </h5>
                          <small
                            >Callum Parker, Soojeong Yoo, Youngho Lee, Joel
                            Fredericks, Arindam Dey, Youngjun Cho, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Parker, C., Yoo, S., Lee, Y., Fredericks, J., Dey,
                            A., Cho, Y., & Billinghurst, M. (2023, April).
                            Towards an inclusive and accessible metaverse. In
                            Extended Abstracts of the 2023 CHI Conference on
                            Human Factors in Computing Systems (pp. 1-5).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{parker2023towards,<br />
                            title={Towards an inclusive and accessible
                            metaverse},<br />
                            author={Parker, Callum and Yoo, Soojeong and Lee,
                            Youngho and Fredericks, Joel and Dey, Arindam and
                            Cho, Youngjun and Billinghurst, Mark},<br />
                            booktitle={Extended Abstracts of the 2023 CHI
                            Conference on Human Factors in Computing
                            Systems},<br />
                            pages={1--5},<br />
                            year={2023}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3544549.3573811"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3544549.3573811</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            The push towards a Metaverse is growing, with
                            companies such as Meta developing their own
                            interpretation of what it should look like. The
                            Metaverse at its conceptual core promises to remove
                            boundaries and borders, becoming a decentralised
                            entity for everyone to use - forming a digital
                            virtual layer over our own “real” world. However,
                            creation of a Metaverse or “new world” presents the
                            opportunity to create one which is inclusive and
                            accessible to all. This challenge is explored and
                            discussed in this workshop, with an aim of
                            understanding how to create a Metaverse which is
                            open and inclusive to people with physical and
                            intellectual disabilities, and how interactions can
                            be designed in a way to minimise disadvantage. The
                            key outcomes of this workshop outline new
                            opportunities for improving accessibility in the
                            Metaverse, methodologies for designing and
                            evaluating accessibility, and key considerations for
                            designing accessible Metaverse environments and
                            interactions.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Wish You Were Here: Mental and Physiological Effects
                            of Remote Music Collaboration in Mixed Reality
                          </h5>
                          <small
                            >Ruben Schlagowski, Dariia Nazarenko, Yekta Can,
                            Kunal Gupta, Silvan Mertes, Mark Billinghurst, and
                            Elisabeth André.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Schlagowski, R., Nazarenko, D., Can, Y., Gupta, K.,
                            Mertes, S., Billinghurst, M., & André, E. (2023,
                            April). Wish you were here: Mental and physiological
                            effects of remote music collaboration in mixed
                            reality. In Proceedings of the 2023 CHI conference
                            on human factors in computing systems (pp. 1-16).
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{schlagowski2023wish,<br />
                            title={Wish you were here: Mental and physiological
                            effects of remote music collaboration in mixed
                            reality},<br />
                            author={Schlagowski, Ruben and Nazarenko, Dariia and
                            Can, Yekta and Gupta, Kunal and Mertes, Silvan and
                            Billinghurst, Mark and Andr{\'e}, Elisabeth},<br />
                            booktitle={Proceedings of the 2023 CHI conference on
                            human factors in computing systems},<br />
                            pages={1--16},<br />
                            year={2023}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3544548.3581162"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3544548.3581162</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            With face-to-face music collaboration being severely
                            limited during the recent pandemic, mixed reality
                            technologies and their potential to provide
                            musicians a feeling of "being there" with their
                            musical partner can offer tremendous opportunities.
                            In order to assess this potential, we conducted a
                            laboratory study in which musicians made music
                            together in real-time while simultaneously seeing
                            their jamming partner’s mixed reality point cloud
                            via a head-mounted display and compared mental
                            effects such as flow, affect, and co-presence to an
                            audio-only baseline. In addition, we tracked the
                            musicians’ physiological signals and evaluated their
                            features during times of self-reported flow. For
                            users jamming in mixed reality, we observed a
                            significant increase in co-presence. Regardless of
                            the condition (mixed reality or audio-only), we
                            observed an increase in positive affect after
                            jamming remotely. Furthermore, we identified heart
                            rate and HF/LF as promising features for classifying
                            the flow state musicians experienced while making
                            music together.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Brain activity during cybersickness: a scoping
                            review
                          </h5>
                          <small
                            >Eunhee Chang, Mark Billinghurst, and Byounghyun
                            Yoo.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Chang, E., Billinghurst, M., & Yoo, B. (2023). Brain
                            activity during cybersickness: A scoping review.
                            Virtual reality, 27(3), 2073-2097.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{chang2023brain,<br />
                            title={Brain activity during cybersickness: A
                            scoping review},<br />
                            author={Chang, Eunhee and Billinghurst, Mark and
                            Yoo, Byounghyun},<br />
                            journal={Virtual reality},<br />
                            volume={27},<br />
                            number={3},<br />
                            pages={2073--2097},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-023-00795-y"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-023-00795-y</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual reality (VR) experiences can cause a range
                            of negative symptoms such as nausea, disorientation,
                            and oculomotor discomfort, which is collectively
                            called cybersickness. Previous studies have
                            attempted to develop a reliable measure for
                            detecting cybersickness instead of using
                            questionnaires, and electroencephalogram (EEG) has
                            been regarded as one of the possible alternatives.
                            However, despite the increasing interest, little is
                            known about which brain activities are consistently
                            associated with cybersickness and what types of
                            methods should be adopted for measuring discomfort
                            through brain activity. We conducted a scoping
                            review of 33 experimental studies in cybersickness
                            and EEG found through database searches and
                            screening. To understand these studies, we organized
                            the pipeline of EEG analysis into four steps
                            (preprocessing, feature extraction, feature
                            selection, classification) and surveyed the
                            characteristics of each step. The results showed
                            that most studies performed frequency or
                            time-frequency analysis for EEG feature extraction.
                            A part of the studies applied a classification model
                            to predict cybersickness indicating an accuracy
                            between 79 and 100%. These studies tended to use
                            HMD-based VR with a portable EEG headset for
                            measuring brain activity. Most VR content shown was
                            scenic views such as driving or navigating a road,
                            and the age of participants was limited to people in
                            their 20 s. This scoping review contributes to
                            presenting an overview of cybersickness-related EEG
                            research and establishing directions for future
                            work.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            The impact of virtual agents’ multimodal
                            communication on brain activity and cognitive load
                            in virtual reality
                          </h5>
                          <small
                            >Zhuang Chang, Huidong Bai, Li Zhang, Kunal Gupta,
                            Weiping He, and Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Chang, Z., Bai, H., Zhang, L., Gupta, K., He, W., &
                            Billinghurst, M. (2022). The impact of virtual
                            agents’ multimodal communication on brain activity
                            and cognitive load in virtual reality. Frontiers in
                            Virtual Reality, 3, 995090.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{chang2022impact,<br />
                            title={The impact of virtual agents’ multimodal
                            communication on brain activity and cognitive load
                            in virtual reality},<br />
                            author={Chang, Zhuang and Bai, Huidong and Zhang, Li
                            and Gupta, Kunal and He, Weiping and Billinghurst,
                            Mark},<br />
                            journal={Frontiers in Virtual Reality},<br />
                            volume={3},<br />
                            pages={995090},<br />
                            year={2022},<br />
                            publisher={Frontiers Media SA}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.frontiersin.org/articles/10.3389/frvir.2022.995090/full"
                              target="_blank"
                              >https://www.frontiersin.org/articles/10.3389/frvir.2022.995090/full</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Related research has shown that collaborating with
                            Intelligent Virtual Agents (IVAs) embodied in
                            Augmented Reality (AR) or Virtual Reality (VR) can
                            improve task performance and reduce task load. Human
                            cognition and behaviors are controlled by brain
                            activities, which can be captured and reflected by
                            Electroencephalogram (EEG) signals. However, little
                            research has been done to understand users’
                            cognition and behaviors using EEG while interacting
                            with IVAs embodied in AR and VR environments. In
                            this paper, we investigate the impact of the virtual
                            agent’s multimodal communication in VR on users’ EEG
                            signals as measured by alpha band power. We develop
                            a desert survival game where the participants make
                            decisions collaboratively with the virtual agent in
                            VR. We evaluate three different communication
                            methods based on a within-subject pilot study: 1) a
                            Voice-only Agent, 2) an Embodied Agent with speech
                            and gaze, and 3) a Gestural Agent with a gesture
                            pointing at the object while talking about it. No
                            significant difference was found in the EEG alpha
                            band power. However, the alpha band ERD/ERS
                            calculated around the moment when the virtual agent
                            started speaking indicated providing a virtual body
                            for the sudden speech could avoid the abrupt
                            attentional demand when the agent started speaking.
                            Moreover, a sudden gesture coupled with the speech
                            induced more attentional demands, even though the
                            speech was matched with the virtual body. This work
                            is the first to explore the impact of IVAs’
                            interaction methods in VR on users’ brain activity,
                            and our findings contribute to the IVAs interaction
                            design.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Online platforms for remote immersive Virtual
                            Reality testing: an emerging tool for experimental
                            behavioral research
                          </h5>
                          <small
                            >Tobias Loetscher, Nadia Siena Jurkovic, Stefan
                            Carlo Michalski, Mark Billinghurst, and Gun
                            Lee.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Loetscher, T., Jurkovic, N. S., Michalski, S. C.,
                            Billinghurst, M., & Lee, G. (2023). Online platforms
                            for remote immersive Virtual Reality testing: an
                            emerging tool for experimental behavioral research.
                            Multimodal Technologies and Interaction, 7(3), 32.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{loetscher2023online,<br />
                            title={Online platforms for remote immersive Virtual
                            Reality testing: an emerging tool for experimental
                            behavioral research},<br />
                            author={Loetscher, Tobias and Jurkovic, Nadia Siena
                            and Michalski, Stefan Carlo and Billinghurst, Mark
                            and Lee, Gun},<br />
                            journal={Multimodal Technologies and
                            Interaction},<br />
                            volume={7},<br />
                            number={3},<br />
                            pages={32},<br />
                            year={2023},<br />
                            publisher={MDPI}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.mdpi.com/2414-4088/7/3/32"
                              target="_blank"
                              >https://www.mdpi.com/2414-4088/7/3/32</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Virtual Reality (VR) technology is gaining in
                            popularity as a research tool for studying human
                            behavior. However, the use of VR technology for
                            remote testing is still an emerging field. This
                            study aimed to evaluate the feasibility of
                            conducting remote VR behavioral experiments that
                            require millisecond timing. Participants were
                            recruited via an online crowdsourcing platform and
                            accessed a task on the classic cognitive phenomenon
                            “Inhibition of Return” through a web browser using
                            their own VR headset or desktop computer (68
                            participants in each group). The results confirm
                            previous research that remote participants using
                            desktop computers can be used effectively for
                            conducting time-critical cognitive experiments.
                            However, inhibition of return was only partially
                            replicated for the VR headset group. Exploratory
                            analyses revealed that technical factors, such as
                            headset type, were likely to significantly impact
                            variability and must be mitigated to obtain accurate
                            results. This study demonstrates the potential for
                            remote VR testing to broaden the research scope and
                            reach a larger participant population. Crowdsourcing
                            services appear to be an efficient and effective way
                            to recruit participants for remote behavioral
                            testing using high-end VR headsets.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Using Virtual Replicas to Improve Mixed Reality
                            Remote Collaboration
                          </h5>
                          <small
                            >Huayuan Tian, Gun A. Lee, Huidong Bai, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Tian, H., Lee, G. A., Bai, H., & Billinghurst, M.
                            (2023). Using virtual replicas to improve mixed
                            reality remote collaboration. IEEE Transactions on
                            Visualization and Computer Graphics, 29(5),
                            2785-2795.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{tian2023using,<br />
                            title={Using virtual replicas to improve mixed
                            reality remote collaboration},<br />
                            author={Tian, Huayuan and Lee, Gun A and Bai,
                            Huidong and Billinghurst, Mark},<br />
                            journal={IEEE Transactions on Visualization and
                            Computer Graphics},<br />
                            volume={29},<br />
                            number={5},<br />
                            pages={2785--2795},<br />
                            year={2023},<br />
                            publisher={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10049700/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10049700/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we explore how virtual replicas can
                            enhance Mixed Reality (MR) remote collaboration with
                            a 3D reconstruction of the task space. People in
                            different locations may need to work together
                            remotely on complicated tasks. For example, a local
                            user could follow a remote expert's instructions to
                            complete a physical task. However, it could be
                            challenging for the local user to fully understand
                            the remote expert's intentions without effective
                            spatial referencing and action demonstration. In
                            this research, we investigate how virtual replicas
                            can work as a spatial communication cue to improve
                            MR remote collaboration. This approach segments the
                            foreground manipulable objects in the local
                            environment and creates corresponding virtual
                            replicas of physical task objects. The remote user
                            can then manipulate these virtual replicas to
                            explain the task and guide their partner. This
                            enables the local user to rapidly and accurately
                            understand the remote expert's intentions and
                            instructions. Our user study with an object assembly
                            task found that using virtual replica manipulation
                            was more efficient than using 3D annotation drawing
                            in an MR remote collaboration scenario. We report
                            and discuss the findings and limitations of our
                            system and study, and present directions for future
                            research.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Hapticproxy: Providing positional vibrotactile
                            feedback on a physical proxy for virtual-real
                            interaction in augmented reality
                          </h5>
                          <small
                            >Li Zhang, Weiping He, Zhiwei Cao, Shuxia Wang,
                            Huidong Bai, and Mark Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zhang, L., He, W., Cao, Z., Wang, S., Bai, H., &
                            Billinghurst, M. (2023). Hapticproxy: Providing
                            positional vibrotactile feedback on a physical proxy
                            for virtual-real interaction in augmented reality.
                            International Journal of Human–Computer Interaction,
                            39(3), 449-463.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{zhang2023hapticproxy,<br />
                            title={Hapticproxy: Providing positional
                            vibrotactile feedback on a physical proxy for
                            virtual-real interaction in augmented reality},<br />
                            author={Zhang, Li and He, Weiping and Cao, Zhiwei
                            and Wang, Shuxia and Bai, Huidong and Billinghurst,
                            Mark},<br />
                            journal={International Journal of Human--Computer
                            Interaction},<br />
                            volume={39},<br />
                            number={3},<br />
                            pages={449--463},<br />
                            year={2023},<br />
                            publisher={Taylor \& Francis}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2022.2041895"
                              target="_blank"
                              >https://www.tandfonline.com/doi/abs/10.1080/10447318.2022.2041895</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Consistent visual and haptic feedback is an
                            important way to improve the user experience when
                            interacting with virtual objects. However, the
                            perception provided in Augmented Reality (AR) mainly
                            comes from visual cues and amorphous tactile
                            feedback. This work explores how to simulate
                            positional vibrotactile feedback (PVF) with multiple
                            vibration motors when colliding with virtual objects
                            in AR. By attaching spatially distributed vibration
                            motors on a physical haptic proxy, users can obtain
                            an augmented collision experience with positional
                            vibration sensations from the contact point with
                            virtual objects. We first developed a prototype
                            system and conducted a user study to optimize the
                            design parameters. Then we investigated the effect
                            of PVF on user performance and experience in a
                            virtual and real object alignment task in the AR
                            environment. We found that this approach could
                            significantly reduce the alignment offset between
                            virtual and physical objects with tolerable task
                            completion time increments. With the PVF cue,
                            participants obtained a more comprehensive
                            perception of the offset direction, more useful
                            information, and a more authentic AR experience.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Robot-enabled tangible virtual assembly with
                            coordinated midair object placement
                          </h5>
                          <small
                            >Li Zhang, Yizhe Liu, Huidong Bai, Qianyuan Zou,
                            Zhuang Chang, Weiping He, Shuxia Wang, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Zhang, L., Liu, Y., Bai, H., Zou, Q., Chang, Z., He,
                            W., ... & Billinghurst, M. (2023). Robot-enabled
                            tangible virtual assembly with coordinated midair
                            object placement. Robotics and Computer-Integrated
                            Manufacturing, 79, 102434.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{zhang2023robot,<br />
                            title={Robot-enabled tangible virtual assembly with
                            coordinated midair object placement},<br />
                            author={Zhang, Li and Liu, Yizhe and Bai, Huidong
                            and Zou, Qianyuan and Chang, Zhuang and He, Weiping
                            and Wang, Shuxia and Billinghurst, Mark},<br />
                            journal={Robotics and Computer-Integrated
                            Manufacturing},<br />
                            volume={79},<br />
                            pages={102434},<br />
                            year={2023},<br />
                            publisher={Elsevier}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://www.sciencedirect.com/science/article/pii/S0736584522001181"
                              target="_blank"
                              >https://www.sciencedirect.com/science/article/pii/S0736584522001181</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Li Zhang, Yizhe Liu, Huidong Bai, Qianyuan Zou,
                            Zhuang Chang, Weiping He, Shuxia Wang, and Mark
                            Billinghurst.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            BeHere: a VR/SAR remote collaboration system based
                            on virtual replicas sharing gesture and avatar in a
                            procedural task
                          </h5>
                          <small
                            >Peng Wang, Yue Wang, Mark Billinghurst, Huizhen
                            Yang, Peng Xu, and Yanhong Li.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Wang, P., Wang, Y., Billinghurst, M., Yang, H., Xu,
                            P., & Li, Y. (2023). BeHere: a VR/SAR remote
                            collaboration system based on virtual replicas
                            sharing gesture and avatar in a procedural task.
                            Virtual Reality, 27(2), 1409-1430.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{wang2023behere,<br />
                            title={BeHere: a VR/SAR remote collaboration system
                            based on virtual replicas sharing gesture and avatar
                            in a procedural task},<br />
                            author={Wang, Peng and Wang, Yue and Billinghurst,
                            Mark and Yang, Huizhen and Xu, Peng and Li,
                            Yanhong},<br />
                            journal={Virtual Reality},<br />
                            volume={27},<br />
                            number={2},<br />
                            pages={1409--1430},<br />
                            year={2023},<br />
                            publisher={Springer}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://link.springer.com/article/10.1007/s10055-023-00748-5"
                              target="_blank"
                              >https://link.springer.com/article/10.1007/s10055-023-00748-5</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            In this paper, we focus on the significance of
                            remote collaboration using virtual replicas, avatar,
                            and gesture on a procedural task in industry; thus,
                            we present a Virtual Reality (VR)/Spatial Augmented
                            Reality (SAR) remote collaboration system, BeHere,
                            based on 3D virtual replicas and sharing gestures
                            and avatar. BeHere enables a remote expert in VR to
                            guide a local worker in real-time to complete a
                            procedural task in the real-world. For the remote VR
                            site, we construct a 3D virtual environment using
                            virtual replicas, and the user can manipulate them
                            by using gestures in an intuitive interaction and
                            see their partners’ 3D virtual avatar. For the local
                            site, we use SAR to enable the local worker to see
                            instructions projected onto the real-world based on
                            the shared virtual replicas and gestures. We
                            conducted a formal user study to evaluate the
                            prototype system in terms of performance, social
                            presence, workload, and ranking and user preference.
                            We found that the combination of visual cues of
                            gestures, avatar, and virtual replicas plays a
                            positive role in improving user experience,
                            especially for remote VR users. More significantly,
                            our study provides useful information and important
                            design implications for further research on the use
                            of gesture-, gaze- and avatar-based cues as well as
                            virtual replicas in VR/AR remote collaboration on a
                            procedural task in industry.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            RadarHand: a Wrist-Worn Radar for On-Skin
                            Touch-based Proprioceptive Gestures
                          </h5>
                          <small
                            >Ryo Hajika, Tamil Selvan Gunasekaran, Chloe Dolma
                            Si Ying Haigh, Yun Suen Pai, Eiji Hayashi, Jaime
                            Lien, Danielle Lottridge, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Hajika, R., Gunasekaran, T. S., Haigh, C. D. S. Y.,
                            Pai, Y. S., Hayashi, E., Lien, J., ... &
                            Billinghurst, M. (2024). RadarHand: A Wrist-Worn
                            Radar for On-Skin Touch-Based Proprioceptive
                            Gestures. ACM Transactions on Computer-Human
                            Interaction, 31(2), 1-36.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{hajika2024radarhand,<br />
                            title={RadarHand: A Wrist-Worn Radar for On-Skin
                            Touch-Based Proprioceptive Gestures},<br />
                            author={Hajika, Ryo and Gunasekaran, Tamil Selvan
                            and Haigh, Chloe Dolma Si Ying and Pai, Yun Suen and
                            Hayashi, Eiji and Lien, Jaime and Lottridge,
                            Danielle and Billinghurst, Mark},<br />
                            journal={ACM Transactions on Computer-Human
                            Interaction},<br />
                            volume={31},<br />
                            number={2},<br />
                            pages={1--36},<br />
                            year={2024},<br />
                            publisher={ACM New York, NY, USA}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://dl.acm.org/doi/abs/10.1145/3617365"
                              target="_blank"
                              >https://dl.acm.org/doi/abs/10.1145/3617365</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            We introduce RadarHand, a wrist-worn wearable with
                            millimetre wave radar that detects on-skin
                            touch-based proprioceptive hand gestures. Radars are
                            robust, private, small, penetrate materials, and
                            require low computation costs. We first evaluated
                            the proprioceptive and tactile perception nature of
                            the back of the hand and found that tapping on the
                            thumb is the least proprioceptive error of all the
                            finger joints, followed by the index finger, middle
                            finger, ring finger, and pinky finger in the
                            eyes-free and high cognitive load situation. Next,
                            we trained deep-learning models for gesture
                            classification. We introduce two types of gestures
                            based on the locations of the back of the hand:
                            generic gestures and discrete gestures. Discrete
                            gestures are gestures that start at specific
                            locations and end at specific locations at the back
                            of the hand, in contrast to generic gestures, which
                            can start anywhere and end anywhere on the back of
                            the hand. Out of 27 gesture group possibilities, we
                            achieved 92% accuracy for a set of seven gestures
                            and 93% accuracy for the set of eight discrete
                            gestures. Finally, we evaluated RadarHand’s
                            performance in real-time under two interaction
                            modes: Active interaction and Reactive interaction.
                            Active interaction is where the user initiates input
                            to achieve the desired output, and reactive
                            interaction is where the device initiates
                            interaction and requires the user to react. We
                            obtained an accuracy of 87% and 74% for active
                            generic and discrete gestures, respectively, as well
                            as 91% and 81.7% for reactive generic and discrete
                            gestures, respectively. We discuss the implications
                            of RadarHand for gesture recognition and directions
                            for future works.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            View Types and Visual Communication Cues for Remote
                            Collaboration
                          </h5>
                          <small
                            >Seungwon Kim, Weidong Huang, Chi-Min Oh, Gun Lee,
                            Mark Billinghurst, and Sang-Joon Lee.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Kim, S., Huang, W., Oh, C. M., Lee, G.,
                            Billinghurst, M., & Lee, S. J. (2023). View types
                            and visual communication cues for remote
                            collaboration. Computers, Materials and Continua.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{kim2023view,<br />
                            title={View types and visual communication cues for
                            remote collaboration},<br />
                            author={Kim, Seungwon and Huang, Weidong and Oh,
                            Chi-Min and Lee, Gun and Billinghurst, Mark and Lee,
                            Sang-Joon},<br />
                            journal={Computers, Materials and Continua},<br />
                            year={2023},<br />
                            publisher={Computers, Materials and Continua (Tech
                            Science Press)}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://opus.lib.uts.edu.au/handle/10453/168838"
                              target="_blank"
                              >https://opus.lib.uts.edu.au/handle/10453/168838</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Over the last several years, remote collaboration
                            has been getting more attention in the research
                            community because of the COVID-19 pandemic. In
                            previous studies, researchers have investigated the
                            effect of adding visual communication cues or shared
                            views in collaboration, but there has not been any
                            previous study exploring the influence between them.
                            In this paper, we investigate the influence of view
                            types on the use of visual communication cues. We
                            compared the use of the three visual cues (hand
                            gesture, a pointer with hand gesture, and sketches
                            with hand gesture) across two view types (dependent
                            and independent views), respectively. We conducted a
                            user study, and the results showed that hand gesture
                            and sketches with the hand gesture cues were well
                            matched with the dependent view condition, and using
                            a pointer with the hand gesture cue was suited to
                            the independent view condition. With the dependent
                            view, the hand gesture and sketch cues required less
                            mental effort for collaborative communication, had
                            better usability, provided better message
                            understanding, and increased feeling of co-presence
                            compared to the independent view. Since the
                            dependent view supported the same viewpoint between
                            the remote expert and a local worker, the local
                            worker could easily understand the remote expert’s
                            hand gestures. In contrast, in the independent view
                            case, when they had different viewpoints, it was not
                            easy for the local worker to understand the remote
                            expert’s hand gestures. The sketch cue had a benefit
                            of showing the final position and orientation of the
                            manipulating objects with the dependent view, but
                            this benefit was less obvious in the independent
                            view case (which provided a further view compared to
                            the dependent view) because precise drawing in the
                            sketches was difficult from a distance. On the
                            contrary, a pointer with the hand gesture cue
                            required less mental effort to collaborate, had
                            better usability, provided better message
                            understanding, and an increased feeling of
                            co-presence in the independent view condition than
                            in the dependent view condition. The pointer cue
                            could be used instead of a hand gesture in the
                            independent view condition because the pointer could
                            still show precise pointing information regardless
                            of the view type.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            How Immersive Virtual Reality Safety Training System
                            Features Impact Learning Outcomes: An Experimental
                            Study of Forklift Training
                          </h5>
                          <small
                            >Ali Abbas, JoonOh Seo, Seungjun Ahn, Yanfang Luo,
                            Mitchell J. Wyllie, Gun Lee, and Mark
                            Billinghurst.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Abbas, A., Seo, J., Ahn, S., Luo, Y., Wyllie, M. J.,
                            Lee, G., & Billinghurst, M. (2023). How immersive
                            virtual reality safety training system features
                            impact learning outcomes: An experimental study of
                            forklift training. Journal of Management in
                            Engineering, 39(1), 04022068.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @article{abbas2023immersive,<br />
                            title={How immersive virtual reality safety training
                            system features impact learning outcomes: An
                            experimental study of forklift training},<br />
                            author={Abbas, Ali and Seo, JoonOh and Ahn, Seungjun
                            and Luo, Yanfang and Wyllie, Mitchell J and Lee, Gun
                            and Billinghurst, Mark},<br />
                            journal={Journal of Management in Engineering},<br />
                            volume={39},<br />
                            number={1},<br />
                            pages={04022068},<br />
                            year={2023},<br />
                            publisher={American Society of Civil Engineers}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ascelibrary.org/doi/full/10.1061/(ASCE)ME.1943-5479.0001101"
                              target="_blank"
                              >https://ascelibrary.org/doi/full/10.1061/(ASCE)ME.1943-5479.0001101</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            Immersive virtual reality (VR)–based training has
                            been widely proposed in different firms to improve
                            the hazard recognition skills of their workforce and
                            change their unsafe behavior. However, little is
                            known about the impact of VR-based training on the
                            user’s behavior and learning. With the use of
                            structural equation modeling (SEM), this study
                            investigated the impact of VR-based training on 60
                            participants, and the results supported the
                            mediating effect of VR system features on the users’
                            acquisition of knowledge, behavioral intention, and
                            satisfaction. The results also indicated that the VR
                            system features were a significant antecedent to
                            psychological factors (presence, motivation,
                            enjoyment, and self-efficacy). This suggests that
                            there are two general paths: (1) usability and
                            fidelity (UF)–enjoyment (EJ)–behavioral intention
                            (BI); and (2) UF–EJ–satisfaction (ST), by which
                            VR-based safety training can have a positive impact
                            on the users’ behavior. This study also revealed
                            that the higher level of presence in the VR training
                            environment would not exert a strong influence on
                            users’ behavior. The findings of this study could
                            help to better design VR-based training programs in
                            a cost-effective way and thus could maximize the
                            benefits of VR technology for industry.
                          </div>
                        </div>
                      </li>

                      <li class="portfolio-item grid-item one column">
                        <div class="column one-sixth portfolio-image">
                          <div class="image-wrappers"></div>
                        </div>
                        <div class="five-sixth">
                          <h5 itemprop="headline" class="pub-title">
                            Point & Portal: A New Action at a Distance Technique
                            For Virtual Reality
                          </h5>
                          <small
                            >Daniel Ablett, Andrew Cunningham, Gun A. Lee, and
                            Bruce H. Thomas.</small
                          >
                          <p style="font-style: italic; font-size: 12px">
                            Ablett, D., Cunningham, A., Lee, G. A., & Thomas, B.
                            H. (2023, October). Point & Portal: A New Action at
                            a Distance Technique For Virtual Reality. In 2023
                            IEEE International Symposium on Mixed and Augmented
                            Reality (ISMAR) (pp. 119-128). IEEE.
                          </p>
                          <div class="links-area">
                            <a class="expand bibtex">BibTex</a
                            ><a class="expand pub_links">Links</a
                            ><a class="expand pub_abstract">Abstract</a>
                          </div>
                          <div class="collapsed_bibtex bibtex">
                            @inproceedings{ablett2023point,<br />
                            title={Point \& Portal: A New Action at a Distance
                            Technique For Virtual Reality},<br />
                            author={Ablett, Daniel and Cunningham, Andrew and
                            Lee, Gun A and Thomas, Bruce H},<br />
                            booktitle={2023 IEEE International Symposium on
                            Mixed and Augmented Reality (ISMAR)},<br />
                            pages={119--128},<br />
                            year={2023},<br />
                            organization={IEEE}<br />
                            }
                          </div>
                          <div class="collapsed_links pub_links">
                            View:
                            <a
                              href="https://ieeexplore.ieee.org/abstract/document/10316441/"
                              target="_blank"
                              >https://ieeexplore.ieee.org/abstract/document/10316441/</a
                            ><br />
                          </div>
                          <div class="collapsed_abstract pub_abstract">
                            This paper introduces Point &amp; Portal, a novel
                            Virtual Reality (VR) interaction technique, inspired
                            by Point &amp; Teleport. This new technique enables
                            users to configure portals using pointing actions,
                            and supports seamless action at a distance and
                            navigation without requiring line of sight. By
                            supporting multiple portals, Point &amp;
                            Portalenables users to create dynamic portal
                            configurations to manage multiple remote tasks.
                            Additionally, this paper introduces Relative Portal
                            Positioning for reliable portal interactions, and
                            the concept of maintaining Level Portals. In a
                            comparative user study, Point &amp; Portal
                            demonstrated significant advantages over the
                            traditional Point &amp; Teleport technique to bring
                            interaction devices within-arm’s reach. In the
                            presence of obstacles, Point &amp; Portal exhibited
                            faster speed, lower cognitive load and was preferred
                            by participants. Overall, participants required less
                            physical movement, pointing actions, and reported
                            higher involvement and “good” usability.
                          </div>
                        </div>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
            <script>
              jQuery(".collapsed_bibtex").each(function () {
                var el = jQuery(this);
                var link = el.prev().find("a.bibtex");

                if (link.hasClass("expand")) {
                  link.click(function () {
                    el.toggleClass("collapsed_bibtex expanded_bibtex");
                    link.toggleClass("collapse expand");
                    jQuery("div.pub_links").each(function () {
                      jQuery(this)
                        .removeClass("expanded_links")
                        .addClass("collapsed_links");
                    });
                    jQuery("div.pub_abstract").each(function () {
                      jQuery(this)
                        .removeClass("expanded_abstract")
                        .addClass("collapsed_abstract");
                    });
                    return false;
                  });
                }
              });
              jQuery(".collapsed_links").each(function () {
                var el2 = jQuery(this);
                var link2 = el2.prevAll(".links-area").find("a.pub_links");
                if (link2.hasClass("expand")) {
                  link2.click(function () {
                    el2.toggleClass("collapsed_links expanded_links");
                    link2.toggleClass("collapse expand");
                    jQuery("div.pub_abstract").each(function () {
                      jQuery(this)
                        .removeClass("expanded_abstract")
                        .addClass("collapsed_abstract");
                    });
                    jQuery("div.bibtex").each(function () {
                      jQuery(this)
                        .removeClass("expanded_bibtex")
                        .addClass("collapsed_bibtex");
                    });
                    return false;
                  });
                }
              });
              jQuery(".collapsed_abstract").each(function () {
                var el2 = jQuery(this);
                var link2 = el2.prevAll(".links-area").find("a.pub_abstract");
                if (link2.hasClass("expand")) {
                  link2.click(function () {
                    el2.toggleClass("collapsed_abstract expanded_abstract");
                    link2.toggleClass("collapse expand");
                    jQuery("div.pub_links").each(function () {
                      jQuery(this)
                        .removeClass("expanded_links")
                        .addClass("collapsed_links");
                    });
                    jQuery("div.bibtex").each(function () {
                      jQuery(this)
                        .removeClass("expanded_bibtex")
                        .addClass("collapsed_bibtex");
                    });
                    return false;
                  });
                }
              });
            </script>
          </div>

          <!-- .four-columns - sidebar -->
        </div>
      </div>

      <!-- mfn_hook_content_after --><!-- mfn_hook_content_after -->
      <!-- #Footer -->
      <footer id="Footer" class="clearfix">
        <div class="widgets_wrapper" style="padding: 70px 0 10px">
          <div class="container">
            <div class="column one-third">
              <aside id="recent-posts-3" class="widget widget_recent_entries">
                <h4>What&#8217;s New</h4>
                <ul>
                  <li>
                    <a
                      href="https://empathiccomputing.org/using-mixed-reality-for-remote-collaboration/"
                      >Using Mixed Reality for Remote Collaboration</a
                    >
                    <span class="post-date">September 30, 2020</span>
                  </li>
                  <li>
                    <a
                      href="https://empathiccomputing.org/where-in-the-world-is-ar-research-happening/"
                      >Where in the World is AR Research Happening?</a
                    >
                    <span class="post-date">July 12, 2018</span>
                  </li>
                  <li>
                    <a
                      href="https://empathiccomputing.org/prof-mark-billinghurst-awe2018/"
                      >Prof. Mark Billinghurst @AWE2018</a
                    >
                    <span class="post-date">July 12, 2018</span>
                  </li>
                </ul>
              </aside>
            </div>
            <div class="column one-third">
              <aside id="text-2" class="widget widget_text">
                <h4>Research</h4>
                <div class="textwidget">
                  <ul class="list_check">
                    <li><a href="./publications/">Our Publications</a></li>
                    <li><a href="./projects/">Our Projects</a></li>
                    <li><a href="./videos/">Related Videos</a></li>
                    <li><a href="./image-gallery/">Image Gallery</a></li>
                    <li><a href="./join-us/">Join Our Research Team</a></li>
                  </ul>
                </div>
              </aside>
            </div>
            <div class="column one-third">
              <aside id="text-4" class="widget widget_text">
                <h4>Contact Info</h4>
                <div class="textwidget">
                  <p>
                    <strong>Address</strong><br />
                    School of Information Technology and Mathematical
                    Sciences<br />
                    University of South Australia<br />
                    Mawson Lakes Campus<br />
                    Mawson Lakes, SA 5095, Australia
                  </p>
                  <p>
                    <strong>Email</strong><br />
                    info@empathiccomputing.org
                  </p>
                </div>
              </aside>
            </div>
          </div>
        </div>

        <div class="footer_copy">
          <div class="container">
            <div class="column one">
              <a id="back_to_top" class="button button_js" href=""
                ><i class="icon-up-open-big"></i
              ></a>
              <!-- Copyrights -->
              <div class="copyright">
                &copy; 2024 Empathic Computing Lab. All Rights Reserved.
              </div>

              <ul class="social">
                <li class="facebook">
                  <a
                    target="_blank"
                    href="https://www.facebook.com/empathiccomputing/"
                    title="Facebook"
                    ><i class="icon-facebook"></i
                  ></a>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </footer>
    </div>
    <!-- #Wrapper -->

    <!-- mfn_hook_bottom --><!-- mfn_hook_bottom -->
    <!-- wp_footer() -->
    <script type="text/javascript">
      /* <![CDATA[ */
      var wpcf7 = {
        apiSettings: {
          root: "https:\/\/empathiccomputing.org\/wp-json\/contact-form-7\/v1",
          namespace: "contact-form-7\/v1",
        },
        recaptcha: {
          messages: { empty: "Please verify that you are not a robot." },
        },
      };
      /* ]]> */
    </script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/plugins/contact-form-7/includes/js/scripts.js?ver=5.0.1"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/ui/core.min.js?ver=1.11.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/ui/widget.min.js?ver=1.11.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/ui/mouse.min.js?ver=1.11.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/ui/sortable.min.js?ver=1.11.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/ui/tabs.min.js?ver=1.11.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/jquery/ui/accordion.min.js?ver=1.11.4"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/themes/empathic/js/plugins.js?ver=20.8.9.0"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/themes/empathic/js/menu.js?ver=20.8.9.0"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/themes/empathic/assets/animations/animations.min.js?ver=20.8.9.0"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/themes/empathic/assets/jplayer/jplayer.min.js?ver=20.8.9.0"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/themes/empathic/js/parallax/translate3d.js?ver=20.8.9.0"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-content/themes/empathic/js/scripts.js?ver=20.8.9.0"
    ></script>
    <script
      type="text/javascript"
      src="https://empathiccomputing.org/wp-includes/js/wp-embed.min.js?ver=4.9.26"
    ></script>
  </body>
</html>
